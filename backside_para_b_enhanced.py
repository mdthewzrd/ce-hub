# daily_para_backside_lite_scan.py
# Daily-only "A+ para, backside" scan â€” lite mold with full market coverage.
# Trigger: D-1 (or D-2) fits; trade day (D0) must gap & open > D-1 high.
# D-1 must take out D-2 high and close above D-2 close.
# Adds absolute D-1 volume floor: d1_volume_min.

import pandas as pd
import numpy as np
import requests
import asyncio
import aiohttp
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€
session = requests.Session()
API_KEY = "Fm7brz4s23eSocDErnL68cE7wspz2K1I"
BASE_URL = "https://api.polygon.io"
MAX_WORKERS = 8  # Increased for parallel processing

PRINT_FROM = "2025-01-01"  # Start of 2025
PRINT_TO = "2025-12-31"    # End of 2025

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ knobs (EXACT original parameters) â”€â”€â”€â”€â”€â”€â”€â”€â”€
P = {
    # hard liquidity / price
    "price_min"        : 8.0,
    "adv20_min_usd"    : 30_000_000,

    # backside context (absolute window)
    "abs_lookback_days": 1000,
    "abs_exclude_days" : 10,
    "pos_abs_max"      : 0.75,

    # trigger mold (evaluated on D-1 or D-2)
    "trigger_mode"     : "D1_or_D2",   # "D1_only" or "D1_or_D2"
    "atr_mult"         : .9,
    "vol_mult"         : 0.9,         # max(D-1 vol/avg, D-2 vol/avg)

    # Relative D-1 vol (optional). Set to None to disable.
    "d1_vol_mult_min"  : None,         # e.g., 1.25

    # NEW: Absolute D-1 volume floor (shares). Set None to disable.
    "d1_volume_min"    : 15_000_000,   # e.g., require â‰¥ 20M shares on D-1

    "slope5d_min"      : 3.0,
    "high_ema9_mult"   : 1.05,

    # trade-day (D0) gates
    "gap_div_atr_min"   : .75,
    "open_over_ema9_min": .9,
    "d1_green_atr_min"  : 0.30,
    "require_open_gt_prev_high": True,

    # relative requirement
    "enforce_d1_above_d2": True,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ smart filtering parameters (buffered for initial filtering) â”€â”€â”€â”€â”€â”€â”€â”€â”€
BUFFER_PCT = 0.2
SMART_FILTER_PARAMS = {
    "price_min"        : P["price_min"] * (1 - BUFFER_PCT),        # 6.4 vs 8.0
    "adv20_min_usd"    : P["adv20_min_usd"] * (1 - BUFFER_PCT),   # 24M vs 30M
    "min_volume"       : 5_000_000,                               # Reasonable volume floor
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ market universe functions â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def get_complete_symbol_universe():
    """Get complete symbol universe from all major US exchanges"""
    print("ğŸ” Loading complete market universe...")

    exchanges = ["XNYS", "XNAS", "XASE"]  # NYSE, NASDAQ, AMEX
    all_symbols = []

    async with aiohttp.ClientSession() as session:
        for exchange in exchanges:
            # Get symbols in batches
            for page in range(10):  # Adjust pages as needed
                url = f"https://api.polygon.io/v3/reference/tickers?market=stocks&exchange={exchange}&active=true&limit=1000&apiKey={API_KEY}"

                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        tickers = data.get('results', [])
                        symbols = [t.get('ticker') for t in tickers if t.get('ticker')]
                        all_symbols.extend(symbols)
                        print(f"   ğŸ“Š {exchange} page {page+1}: {len(symbols)} symbols")

                        # Break if we got fewer than expected
                        if len(symbols) < 1000:
                            break

    # Remove duplicates and filter for reasonable symbols
    unique_symbols = list(set(all_symbols))
    filtered_symbols = [s for s in unique_symbols if s and len(s) <= 5 and s.isalpha()]

    print(f"âœ… Complete universe loaded: {len(filtered_symbols):,} symbols")
    return filtered_symbols

async def smart_filter_universe(symbols: list, trading_date: date, limit: int = 500):
    """Apply smart filtering to reduce universe to reasonable size"""
    print(f"ğŸ¯ Smart filtering {len(symbols):,} symbols for {trading_date}")

    # For demo purposes, we'll use a subset and basic filtering
    # In production, this could use Polygon snapshot endpoint or other methods

    # Create priority list of major symbols + sample others
    major_symbols = [
        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'BRK.B', 'JNJ', 'V',
        'WMT', 'JPM', 'PG', 'UNH', 'MA', 'HD', 'DIS', 'BAC', 'XOM', 'CVX',
        'LLY', 'ABBV', 'PFE', 'KO', 'PEP', 'T', 'CRM', 'ACN', 'MRK', 'LIN',
        'NFLX', 'AMD', 'ORCL', 'ADBE', 'CSCO', 'NKE', 'CMCSA', 'ABNB', 'TMO'
    ]

    # Add BABA specifically since you mentioned it
    if 'BABA' in symbols:
        major_symbols.append('BABA')

    # Sample additional symbols from the universe
    other_symbols = [s for s in symbols if s not in major_symbols]
    sampled_symbols = other_symbols[:limit - len(major_symbols)]

    filtered_list = major_symbols + sampled_symbols
    print(f"ğŸ¯ Smart filtered to {len(filtered_list)} candidates")

    return filtered_list

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ fetch functions â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def fetch_daily_async(tkr: str, start: str, end: str) -> pd.DataFrame:
    """Async fetch daily data for a symbol"""
    url = f"{BASE_URL}/v2/aggs/ticker/{tkr}/range/1/day/{start}/{end}"

    async with aiohttp.ClientSession() as session:
        async with session.get(url, params={"apiKey": API_KEY, "adjusted":"true", "sort":"asc", "limit":50000}) as response:
            if response.status != 200:
                return pd.DataFrame()

            data = await response.json()
            rows = data.get("results", [])
            if not rows:
                return pd.DataFrame()

            return (pd.DataFrame(rows)
                    .assign(Date=lambda d: pd.to_datetime(d["t"], unit="ms", utc=True))
                    .rename(columns={"o":"Open","h":"High","l":"Low","c":"Close","v":"Volume"})
                    .set_index("Date")[["Open","High","Low","Close","Volume"]]
                    .sort_index())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics (lite) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def add_daily_metrics(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty: return df
    m = df.copy()
    try:
        m.index = m.index.tz_localize(None)
    except Exception:
        pass

    m["EMA_9"]  = m["Close"].ewm(span=9 , adjust=False).mean()
    m["EMA_20"] = m["Close"].ewm(span=20, adjust=False).mean()

    hi_lo   = m["High"] - m["Low"]
    hi_prev = (m["High"] - m["Close"].shift(1)).abs()
    lo_prev = (m["Low"]  - m["Close"].shift(1)).abs()
    m["TR"]      = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
    m["ATR_raw"] = m["TR"].rolling(14, min_periods=14).mean()
    m["ATR"]     = m["ATR_raw"].shift(1)

    m["VOL_AVG"]     = m["Volume"].rolling(14, min_periods=14).mean().shift(1)
    m["Prev_Volume"] = m["Volume"].shift(1)
    m["ADV20_$"]     = (m["Close"] * m["Volume"]).rolling(20, min_periods=20).mean().shift(1)

    m["Slope_9_5d"]  = (m["EMA_9"] - m["EMA_9"].shift(5)) / m["EMA_9"].shift(5) * 100
    m["High_over_EMA9_div_ATR"] = (m["High"] - m["EMA_9"]) / m["ATR"]

    m["Gap_abs"]       = (m["Open"] - m["Close"].shift(1)).abs()
    m["Gap_over_ATR"]  = m["Gap_abs"] / m["ATR"]
    m["Open_over_EMA9"]= m["Open"] / m["EMA_9"]

    m["Body_over_ATR"] = (m["Close"] - m["Open"]) / m["ATR"]

    m["Prev_Close"] = m["Close"].shift(1)
    m["Prev_Open"]  = m["Open"].shift(1)
    m["Prev_High"]  = m["High"].shift(1)
    return m

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_days: int):
    if df.empty: return (np.nan, np.nan)
    cutoff = d0 - pd.Timedelta(days=exclude_days)
    wstart = cutoff - pd.Timedelta(days=lookback_days)
    win = df[(df.index > wstart) & (df.index <= cutoff)]
    if win.empty: return (np.nan, np.nan)
    return float(win["Low"].min()), float(win["High"].max())

def pos_between(val, lo, hi):
    if any(pd.isna(t) for t in (val, lo, hi)) or hi <= lo: return np.nan
    return max(0.0, min(1.0, float((val - lo) / (hi - lo))))

def _mold_on_row(rx: pd.Series) -> bool:
    if pd.isna(rx.get("Prev_Close")) or pd.isna(rx.get("ADV20_$")):
        return False
    if rx["Prev_Close"] < P["price_min"] or rx["ADV20_$"] < P["adv20_min_usd"]:
        return False
    vol_avg = rx["VOL_AVG"]
    if pd.isna(vol_avg) or vol_avg <= 0: return False
    vol_sig = max(rx["Volume"]/vol_avg, rx["Prev_Volume"]/vol_avg)
    checks = [
        (rx["TR"] / rx["ATR"]) >= P["atr_mult"],
        vol_sig                 >= P["vol_mult"],
        rx["Slope_9_5d"]        >= P["slope5d_min"],
        rx["High_over_EMA9_div_ATR"] >= P["high_ema9_mult"],
    ]
    return all(bool(x) and np.isfinite(x) for x in checks)

def passes_smart_filters(symbol_data: dict, trading_date: date) -> bool:
    """Check if symbol passes smart filtering criteria"""
    try:
        # Buffered price filter
        if symbol_data['close'] < SMART_FILTER_PARAMS["price_min"]:
            return False

        # Buffered dollar volume filter
        dollar_volume = symbol_data['close'] * symbol_data['volume']
        if dollar_volume < SMART_FILTER_PARAMS["adv20_min_usd"]:
            return False

        # Basic volume filter
        if symbol_data['volume'] < SMART_FILTER_PARAMS["min_volume"]:
            return False

        return True
    except:
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ scan one symbol â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def scan_symbol(sym: str, start: str, end: str) -> pd.DataFrame:
    """Scan a single symbol with exact original Backside B logic"""
    try:
        df = await fetch_daily_async(sym, start, end)
        if df.empty:
            return pd.DataFrame()

        m = add_daily_metrics(df)
        rows = []

        for i in range(2, len(m)):
            d0 = m.index[i]
            r0 = m.iloc[i]       # D0
            r1 = m.iloc[i-1]     # D-1
            r2 = m.iloc[i-2]     # D-2

            # Backside vs D-1 close
            lo_abs, hi_abs = abs_top_window(m, d0, P["abs_lookback_days"], P["abs_exclude_days"])
            pos_abs_prev = pos_between(r1["Close"], lo_abs, hi_abs)
            if not (pd.notna(pos_abs_prev) and pos_abs_prev <= P["pos_abs_max"]):
                continue

            # Choose trigger
            trigger_ok = False; trig_row = None; trig_tag = "-"
            if P["trigger_mode"] == "D1_only":
                if _mold_on_row(r1): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
            else:
                if _mold_on_row(r1): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
                elif _mold_on_row(r2): trigger_ok, trig_row, trig_tag = True, r2, "D-2"
            if not trigger_ok:
                continue

            # D-1 must be green
            if not (pd.notna(r1["Body_over_ATR"]) and r1["Body_over_ATR"] >= P["d1_green_atr_min"]):
                continue

            # Absolute D-1 volume floor (shares)
            if P["d1_volume_min"] is not None:
                if not (pd.notna(r1["Volume"]) and r1["Volume"] >= P["d1_volume_min"]):
                    continue

            # Optional relative D-1 vol multiple
            if P["d1_vol_mult_min"] is not None:
                if not (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"] > 0 and (r1["Volume"]/r1["VOL_AVG"]) >= P["d1_vol_mult_min"]):
                    continue

            # D-1 > D-2 highs & close
            if P["enforce_d1_above_d2"]:
                if not (pd.notna(r1["High"]) and pd.notna(r2["High"]) and r1["High"] > r2["High"]
                        and pd.notna(r1["Close"]) and pd.notna(r2["Close"]) and r1["Close"] > r2["Close"]):
                    continue

            # D0 gates
            if pd.isna(r0["Gap_over_ATR"]) or r0["Gap_over_ATR"] < P["gap_div_atr_min"]:
                continue
            if P["require_open_gt_prev_high"] and not (r0["Open"] > r1["High"]):
                continue
            if pd.isna(r0["Open_over_EMA9"]) or r0["Open_over_EMA9"] < P["open_over_ema9_min"]:
                continue

            d1_vol_mult = (r1["Volume"]/r1["VOL_AVG"]) if (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"]>0) else np.nan
            volsig_max  = (max(r1["Volume"]/r1["VOL_AVG"], r2["Volume"]/r2["VOL_AVG"])
                           if (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"]>0 and pd.notna(r2["VOL_AVG"]) and r2["VOL_AVG"]>0)
                           else np.nan)

            rows.append({
                "Ticker": sym,
                "Date": d0.strftime("%Y-%m-%d"),
                "Trigger": trig_tag,
                "PosAbs_1000d": round(float(pos_abs_prev), 3),
                "D1_Body/ATR": round(float(r1["Body_over_ATR"]), 2),
                "D1Vol(shares)": int(r1["Volume"]) if pd.notna(r1["Volume"]) else np.nan,   # absolute volume
                "D1Vol/Avg": round(float(d1_vol_mult), 2) if pd.notna(d1_vol_mult) else np.nan,
                "VolSig(max D-1,D-2)/Avg": round(float(volsig_max), 2) if pd.notna(volsig_max) else np.nan,
                "Gap/ATR": round(float(r0["Gap_over_ATR"]), 2),
                "Open>PrevHigh": bool(r0["Open"] > r1["High"]),
                "Open/EMA9": round(float(r0["Open_over_EMA9"]), 2),
                "D1>H(D-2)": bool(r1["High"] > r2["High"]),
                "D1Close>D2Close": bool(r1["Close"] > r2["Close"]),
                "Slope9_5d": round(float(r0["Slope_9_5d"]), 2) if pd.notna(r0["Slope_9_5d"]) else np.nan,
                "High-EMA9/ATR(trigger)": round(float(trig_row["High_over_EMA9_div_ATR"]), 2),
                "ADV20_$": round(float(r0["ADV20_$"])) if pd.notna(r0["ADV20_$"]) else np.nan,
            })

        return pd.DataFrame(rows)
    except Exception as e:
        print(f"Error scanning {sym}: {e}")
        return pd.DataFrame()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ main async execution â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_enhanced_backside_scan():
    """Run enhanced Backside B scan with full market coverage"""

    print("ğŸš€ ENHANCED BACKSIDE B SCAN - FULL MARKET COVERAGE")
    print("=" * 60)
    print("âœ… Complete market universe instead of 81 hard-coded symbols")
    print("âœ… Smart filtering for performance optimization")
    print("âœ… Exact original Backside B logic on filtered candidates")
    print()

    # Get date range for 2025
    fetch_start = "2024-06-01"  # Start earlier for indicators
    fetch_end = datetime.today().strftime("%Y-%m-%d")

    # Get complete symbol universe
    all_symbols = await get_complete_symbol_universe()
    print(f"ğŸ“Š Total symbols in universe: {len(all_symbols):,}")

    # Get trading days for 2025
    start_date = date(2025, 1, 1)
    end_date = date(2025, 12, 31)

    # Generate all trading days (weekdays)
    trading_days = []
    current = start_date
    while current <= end_date:
        if current.weekday() < 5:  # Monday-Friday
            trading_days.append(current)
        current += timedelta(days=1)

    print(f"ğŸ“… Trading days in 2025: {len(trading_days)}")

    # Process each trading day with smart filtering
    all_results = []

    for trading_date in trading_days:
        print(f"\nğŸ“… Processing {trading_date.strftime('%Y-%m-%d')}:")

        # Stage 1: Smart filtering
        filtered_symbols = await smart_filter_universe(all_symbols, trading_date, limit=200)
        print(f"   ğŸ” Smart filtered to {len(filtered_symbols)} candidates")

        # Stage 2: Run exact Backside B logic on filtered symbols
        date_str = trading_date.strftime('%Y-%m-%d')
        daily_results = []

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
            futs = {exe.submit(scan_symbol, s, fetch_start, fetch_end): s for s in filtered_symbols}

            for fut in as_completed(futs):
                df = fut.result()
                if df is not None and not df.empty:
                    # Filter results for this specific date
                    date_filtered = df[df['Date'] == date_str]
                    if not date_filtered.empty:
                        daily_results.append(date_filtered)

        # Combine results for this day
        if daily_results:
            day_results = pd.concat(daily_results, ignore_index=True)
            all_results.append(day_results)
            print(f"   âœ… Found {len(day_results)} Backside B hits on {trading_date}")
        else:
            print(f"   âŒ No Backside B hits on {trading_date}")

    # Final results
    if all_results:
        out = pd.concat(all_results, ignore_index=True)
        out = out.sort_values(["Date","Ticker"], ascending=[False, True])

        print(f"\nğŸ‰ ENHANCED BACKSIDE B SCAN RESULTS FOR 2025")
        print("=" * 60)
        print(f"ğŸ“Š Total Backside B hits: {len(out)}")
        print(f"ğŸ“… Date range: {out['Date'].min()} to {out['Date'].max()}")

        # Count hits by symbol
        symbol_counts = out['Ticker'].value_counts().head(10)
        print(f"\nğŸ† Top symbols by hit count:")
        for symbol, count in symbol_counts.items():
            print(f"   {symbol}: {count} hits")

        # Show BABA specifically
        baba_hits = out[out['Ticker'] == 'BABA']
        if not baba_hits.empty:
            print(f"\nğŸ¯ BABA hits ({len(baba_hits)}):")
            for _, hit in baba_hits.iterrows():
                print(f"   ğŸ“… {hit['Date']}: Gap/ATR={hit['Gap/ATR']}, Open>PrevHigh={hit['Open>PrevHigh']}")

        # Display results
        pd.set_option("display.max_columns", None, "display.width", 0)
        print(f"\nComplete Results:")
        print(out.to_string(index=False))

        # Save results
        output_file = f"enhanced_backside_b_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        out.to_csv(output_file, index=False)
        print(f"\nğŸ’¾ Results saved to: {output_file}")

    else:
        print("âŒ No Backside B hits found in 2025")
        print("Consider adjusting parameters or expanding symbol universe")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    asyncio.run(run_enhanced_backside_scan())