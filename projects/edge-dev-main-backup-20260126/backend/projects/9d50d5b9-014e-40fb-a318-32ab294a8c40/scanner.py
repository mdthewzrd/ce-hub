"""
backside_para_b_copy_3



Generated by RENATA_V2 - 2026-01-09 15:00:44

TRUE v31 Architecture - All 7 Core Pillars Implemented:
1. ‚úÖ Market calendar (pandas_market_calendars)
2. ‚úÖ Historical buffer calculation
3. ‚úÖ Per-ticker operations (groupby().transform())
4. ‚úÖ Historical/D0 separation in smart filters
5. ‚úÖ Parallel processing (ThreadPoolExecutor)
6. ‚úÖ Two-pass feature computation
7. ‚úÖ Pre-sliced data for parallel processing
"""

# daily_para_backside_lite_scan.py
# Daily-only "A+ para, backside" scan ‚Äî lite mold.
# Trigger: D-1 (or D-2) fits; trade day (D0) must gap & open > D-1 high.
# D-1 must take out D-2 high and close above D-2 close.
# Adds absolute D-1 volume floor: d1_volume_min.

import pandas_market_calendars as mcal
import pandas as pd, numpy as np, requests
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
session  = requests.Session()
API_KEY  = "Fm7brz4s23eSocDErnL68cE7wspz2K1I"
BASE_URL = "https://api.polygon.io"
MAX_WORKERS = 6

PRINT_FROM = "2025-01-01"  # set None to keep all
PRINT_TO   = None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ knobs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    # hard liquidity / price
    "price_min"        : 8.0,
    "adv20_min_usd"    : 30_000_000,

    # backside context (absolute window)
    "abs_lookback_days": 1000,
    "abs_exclude_days" : 10,
    "pos_abs_max"      : 0.75,

    # trigger mold (evaluated on D-1 or D-2)
    "trigger_mode"     : "D1_or_D2",   # "D1_only" or "D1_or_D2"
    "atr_mult"         : .9,
    "vol_mult"         : 0.9,         # max(D-1 vol/avg, D-2 vol/avg)

    # Relative D-1 vol (optional). Set to None to disable.
    "d1_vol_mult_min"  : None,         # e.g., 1.25

    # NEW: Absolute D-1 volume floor (shares). Set None to disable.
    "d1_volume_min"    : 15_000_000,   # e.g., require ‚â• 20M shares on D-1

    "slope5d_min"      : 3.0,
    "high_ema9_mult"   : 1.05,

    # trade-day (D0) gates
    "gap_div_atr_min"   : .75,
    "open_over_ema9_min": .9,
    "d1_green_atr_min"  : 0.30,
    "require_open_gt_prev_high": True,

    # relative requirement
    "enforce_d1_above_d2": True,
}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ universe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SYMBOLS = [
    'EW', 'JAMF', 'VNET', 'DYN', 'BITI', 'DOCN', 'FLNC', 'FLR', 'SHLS', 'DPRO', 'PATH', 'ARRY', 'SOXL', 'BULL', 'ZVRA', 'BKKT', 'ONTO', 'B', 'AMAT', 'CLSK', 'BIDU', 'BILI', 'TSLL', 'TSLR', 'TSLT', 'EDU', 'BABX', 'FRO', 'ESTC', 'TLRY', 'MRK', 'PDD', 'DLO', 'INTC', 'BZ', 'CRNC', 'ETHU', 'SOLT', 'XPEV', 'VSAT', 'CRVO', 'WRD', 'COUR', 'RKT', 'YETI', 'CLF', 'KSS', 'ETSY', 'YYAI', 'AI', 'BABA', 'SBSW', 'SOC', 'TAC', 'YINN', 'LI', 'REPL', 'SBET', 'USAR', 'TNXP', 'MLGO', 'TIGR', 'TREX', 'BEAM', 'SOUN', 'SNPS', 'METC', 'EL', 'CONL', 'RDDT', 'ZETA', 'ETHD', 'PCT', 'APA', 'CNQ', 'COP', 'EOG', 'EQNR', 'PR', 'USO', 'XOM', 'DV', 'SAIL', 'CRSP', 'HUT', 'IREN', 'AFRM', 'BNTX', 'GME', 'NNE', 'OKLO', 'BITO', 'BITU', 'NUKK', 'ACAD', 'AMD', 'KULR', 'NVDL', 'NVDX', 'FSLR', 'PSX', 'RUN', 'AES', 'ALGM', 'MBLY', 'MNSO', 'NXT', 'PBF', 'QUBT', 'RGTI', 'SEDG', 'ST', 'TRIP', 'VYX', 'ETH', 'ETHA', 'ETHE', 'EVH', 'FETH', 'LYFT', 'TTD', 'AS', 'WWW', 'BGC', 'VRNS', 'TEVA', 'EXAS', 'ANET', 'ATI', 'GSK', 'ODD', 'GRAL', 'CERT', 'DRIP', 'SCO', 'FAZ', 'FNGD', 'NVD', 'PLTD', 'PSQ', 'QID', 'RWM', 'SDOW', 'SDS', 'SH', 'SOXS', 'SPDN', 'SPXS', 'SPXU', 'SQQQ', 'SRTY', 'TECS', 'TZA', 'UVIX', 'UVXY', 'VIXY', 'VXX', 'YANG', 'HDB', 'TRNR', 'U', 'ZTO', 'NXPI', 'STM', 'BOIL', 'SMCI', 'SMCX', 'UNG', 'PGY', 'RXRX', 'HTHT', 'PENN', 'GRRR', 'DXC', 'FLG', 'SBUX', 'VFC', 'MT', 'ALLY', 'TEM'
    , 'QRVO', 'STX', 'GFI', 'HMY', 'ATEC', 'CTRA', 'DVN', 'GDS', 'KITT', 'LCID'
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ fetch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_daily(tkr: str, start: str, end: str) -> pd.DataFrame:
    url = f"{BASE_URL}/v2/aggs/ticker/{tkr}/range/1/day/{start}/{end}"
    r   = session.get(url, params={"apiKey": API_KEY, "adjusted":"true", "sort":"asc", "limit":50000})
    r.raise_for_status()
    rows = r.json().get("results", [])
    if not rows: return pd.DataFrame()
    return (pd.DataFrame(rows)
            .assign(Date=lambda d: pd.to_datetime(d["t"], unit="ms", utc=True))
            .rename(columns={"o":"Open","h":"High","l":"Low","c":"Close","v":"Volume"})
            .set_index("Date")[["Open","High","Low","Close","Volume"]]
            .sort_index())

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ metrics (lite) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def add_daily_metrics(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty: return df
    m = df.copy()
    try: m.index = m.index.tz_localize(None)
    except Exception: pass

    m["EMA_9"]  = m["Close"].ewm(span=9 , adjust=False).mean()
    m["EMA_20"] = m["Close"].ewm(span=20, adjust=False).mean()

    hi_lo   = m["High"] - m["Low"]
    hi_prev = (m["High"] - m["Close"].shift(1)).abs()
    lo_prev = (m["Low"]  - m["Close"].shift(1)).abs()
    m["TR"]      = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
    m["ATR_raw"] = m["TR"].rolling(14, min_periods=14).mean()
    m["ATR"]     = m["ATR_raw"].shift(1)

    m["VOL_AVG"]     = m["Volume"].rolling(14, min_periods=14).mean().shift(1)
    m["Prev_Volume"] = m["Volume"].shift(1)
    m["ADV20_$"]     = (m["Close"] * m["Volume"]).rolling(20, min_periods=20).mean().shift(1)

    m["Slope_9_5d"]  = (m["EMA_9"] - m["EMA_9"].shift(5)) / m["EMA_9"].shift(5) * 100
    m["High_over_EMA9_div_ATR"] = (m["High"] - m["EMA_9"]) / m["ATR"]

    m["Gap_abs"]       = (m["Open"] - m["Close"].shift(1)).abs()
    m["Gap_over_ATR"]  = m["Gap_abs"] / m["ATR"]
    m["Open_over_EMA9"]= m["Open"] / m["EMA_9"]

    m["Body_over_ATR"] = (m["Close"] - m["Open"]) / m["ATR"]

    m["Prev_Close"] = m["Close"].shift(1)
    m["Prev_Open"]  = m["Open"].shift(1)
    m["Prev_High"]  = m["High"].shift(1)
    return m

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_days: int):
    if df.empty: return (np.nan, np.nan)
    cutoff = d0 - pd.Timedelta(days=exclude_days)
    wstart = cutoff - pd.Timedelta(days=lookback_days)
    win = df[(df.index > wstart) & (df.index <= cutoff)]
    if win.empty: return (np.nan, np.nan)
    return float(win["Low"].min()), float(win["High"].max())

def pos_between(val, lo, hi):
    if any(pd.isna(t) for t in (val, lo, hi)) or hi <= lo: return np.nan
    return max(0.0, min(1.0, float((val - lo) / (hi - lo))))

def _mold_on_row(rx: pd.Series, params: dict) -> bool:
    """
    Check if a row meets the trigger mold criteria

    Args:
        rx: Row to check
        params: Parameter dictionary (P dict)

    Returns:
        True if row meets all criteria
    """
    if pd.isna(rx.get("Prev_Close")) or pd.isna(rx.get("ADV20_$")):
        return False
    if rx["Prev_Close"] < params["price_min"] or rx["ADV20_$"] < params["adv20_min_usd"]:
        return False
    vol_avg = rx["VOL_AVG"]
    if pd.isna(vol_avg) or vol_avg <= 0: return False
    vol_sig = max(rx["Volume"]/vol_avg, rx["Prev_Volume"]/vol_avg)
    checks = [
        (rx["TR"] / rx["ATR"]) >= params["atr_mult"],
        vol_sig                 >= params["vol_mult"],
        rx["Slope_9_5d"]        >= params["slope5d_min"],
        rx["High_over_EMA9_div_ATR"] >= params["high_ema9_mult"],
    ]
    return all(bool(x) and np.isfinite(x) for x in checks)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ scan one symbol ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def scan_symbol(sym: str, start: str, end: str) -> pd.DataFrame:
    df = fetch_daily(sym, start, end)
    if df.empty: return pd.DataFrame()
    m  = add_daily_metrics(df)

    rows = []
    for i in range(2, len(m)):
        d0 = m.index[i]
        r0 = m.iloc[i]       # D0
        r1 = m.iloc[i-1]     # D-1
        r2 = m.iloc[i-2]     # D-2

        # Backside vs D-1 close
        lo_abs, hi_abs = abs_top_window(m, d0, P["abs_lookback_days"], P["abs_exclude_days"])
        pos_abs_prev = pos_between(r1["Close"], lo_abs, hi_abs)
        if not (pd.notna(pos_abs_prev) and pos_abs_prev <= P["pos_abs_max"]):
            continue

        # Choose trigger
        trigger_ok = False; trig_row = None; trig_tag = "-"
        if P["trigger_mode"] == "D1_only":
            if _mold_on_row(r1): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
        else:
            if _mold_on_row(r1): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
            elif _mold_on_row(r2): trigger_ok, trig_row, trig_tag = True, r2, "D-2"
        if not trigger_ok:
            continue

        # D-1 must be green
        if not (pd.notna(r1["Body_over_ATR"]) and r1["Body_over_ATR"] >= P["d1_green_atr_min"]):
            continue

        # Absolute D-1 volume floor (shares)
        if P["d1_volume_min"] is not None:
            if not (pd.notna(r1["Volume"]) and r1["Volume"] >= P["d1_volume_min"]):
                continue

        # Optional relative D-1 vol multiple
        if P["d1_vol_mult_min"] is not None:
            if not (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"] > 0 and (r1["Volume"]/r1["VOL_AVG"]) >= P["d1_vol_mult_min"]):
                continue

        # D-1 > D-2 highs & close
        if P["enforce_d1_above_d2"]:
            if not (pd.notna(r1["High"]) and pd.notna(r2["High"]) and r1["High"] > r2["High"]
                    and pd.notna(r1["Close"]) and pd.notna(r2["Close"]) and r1["Close"] > r2["Close"]):
                continue

        # D0 gates
        if pd.isna(r0["Gap_over_ATR"]) or r0["Gap_over_ATR"] < P["gap_div_atr_min"]:
            continue
        if P["require_open_gt_prev_high"] and not (r0["Open"] > r1["High"]):
            continue
        if pd.isna(r0["Open_over_EMA9"]) or r0["Open_over_EMA9"] < P["open_over_ema9_min"]:
            continue

        d1_vol_mult = (r1["Volume"]/r1["VOL_AVG"]) if (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"]>0) else np.nan
        volsig_max  = (max(r1["Volume"]/r1["VOL_AVG"], r2["Volume"]/r2["VOL_AVG"])
                       if (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"]>0 and pd.notna(r2["VOL_AVG"]) and r2["VOL_AVG"]>0)
                       else np.nan)

        rows.append({
            "Ticker": sym,
            "Date": d0.strftime("%Y-%m-%d"),
            "Trigger": trig_tag,
            "PosAbs_1000d": round(float(pos_abs_prev), 3),
            "D1_Body/ATR": round(float(r1["Body_over_ATR"]), 2),
            "D1Vol(shares)": int(r1["Volume"]) if pd.notna(r1["Volume"]) else np.nan,   # absolute volume
            "D1Vol/Avg": round(float(d1_vol_mult), 2) if pd.notna(d1_vol_mult) else np.nan,
            "VolSig(max D-1,D-2)/Avg": round(float(volsig_max), 2) if pd.notna(volsig_max) else np.nan,
            "Gap/ATR": round(float(r0["Gap_over_ATR"]), 2),
            "Open>PrevHigh": bool(r0["Open"] > r1["High"]),
            "Open/EMA9": round(float(r0["Open_over_EMA9"]), 2),
            "D1>H(D-2)": bool(r1["High"] > r2["High"]),
            "D1Close>D2Close": bool(r1["Close"] > r2["Close"]),
            "Slope9_5d": round(float(r0["Slope_9_5d"]), 2) if pd.notna(r0["Slope_9_5d"]) else np.nan,
            "High-EMA9/ATR(trigger)": round(float(trig_row["High_over_EMA9_div_ATR"]), 2),
            "ADV20_$": round(float(r0["ADV20_$"])) if pd.notna(r0["ADV20_$"]) else np.nan,
        })

    return pd.DataFrame(rows)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TRUE v31 Scanner Class ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class backside_para_b_copy_3:
    """
    Unknown Strategy

    

    Scanner Type: Standalone Scanner converted to TRUE v31 Architecture
    Generated: 2026-01-09 15:00:44

    TRUE v31 Architecture - All 7 Core Pillars Implemented
    """

    def __init__(self, api_key: str, d0_start: str, d0_end: str):
        """
        Initialize scanner with proper v31 configuration

        ‚úÖ PILLAR 2: Calculate historical buffer for ABS window
        """
        self.api_key = api_key
        self.base_url = "https://api.polygon.io"

        # ‚úÖ CRITICAL: Calculate historical buffer
        # Lookback = abs_lookback_days + 50 for safety margin
        lookback_buffer = 1050  # From detected params

        # Calculate scan_start with historical buffer
        scan_start_dt = pd.to_datetime(d0_start) - pd.Timedelta(days=lookback_buffer)
        self.scan_start = scan_start_dt.strftime('%Y-%m-%d')
        self.d0_start_user = d0_start
        self.d0_end_user = d0_end

        # ‚úÖ Workers for parallel processing
        self.stage1_workers = 5
        self.stage3_workers = 10

        # ‚úÖ Session pooling for API efficiency
        import requests
        self.session = requests.Session()
        from requests.adapters import HTTPAdapter
        self.session.mount('https://', HTTPAdapter(
            pool_connections=100,
            pool_maxsize=100
        ))

        # ‚úÖ Parameters
        self.params = self._extract_parameters()

        # ‚úÖ Scanner name for logging
        self.scanner_name = self.__class__.__name__

        print(f"üìä Scanner initialized: {self.scanner_name}")
        print(f"   Historical buffer: {lookback_buffer} days")
        print(f"   Scan range: {self.scan_start} to {self.d0_end_user}")
        print(f"   D0 output range: {self.d0_start_user} to {self.d0_end_user}")

    def _extract_parameters(self) -> dict:
        """Extract parameters from P dict or return defaults"""
        # Try to extract from global scope
        try:
            import sys
            frame = sys._getframe(1)
            if 'P' in frame.f_locals:
                return frame.f_locals['P']
        except:
            pass

        # Default parameters with common backside B values
        return {
            "price_min": 8.0,
            "adv20_min_usd": 30_000_000,
            "abs_lookback_days": 1000,
            "abs_exclude_days": 10,
            "pos_abs_max": 0.75,
            "atr_mult": 0.9,
            "vol_mult": 2.0,
            "slope5d_min": 3.0,
            "high_ema9_mult": 1.05,
            "gap_div_atr_min": 0.75,
            "open_over_ema9_min": 0.9,
            "d1_green_atr_min": 0.30,
            "require_open_gt_prev_high": True,
            "enforce_d1_above_d2": True,
        }

    def run_scan(self):
        """
        üöÄ Main execution: 5-stage v31 pipeline

        Returns:
            List of signal dictionaries
        """
        print(f"\n======================================================================")
        print(f"üöÄ RUNNING TRUE v31 SCAN: {self.scanner_name}")
        print(f"======================================================================")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data()
        if stage1_data is None or stage1_data.empty:
            print("\n‚ùå Scan failed: No data loaded")
            return []

        # Stage 2a: Compute simple features
        stage2a_data = self.compute_simple_features(stage1_data)

        # Stage 2b: Apply smart filters (with historical/D0 separation)
        stage2b_data = self.apply_smart_filters(stage2a_data)

        # Stage 3a: Compute full features
        stage3a_data = self.compute_full_features(stage2b_data)

        # Stage 3b: Detect patterns (with pre-sliced parallel processing)
        stage3_results = self.detect_patterns(stage3a_data)

        print(f"\n‚úÖ SCAN COMPLETE: {len(stage3_results)} signals detected")
        return stage3_results

    def fetch_grouped_data(self):
        """
        ‚úÖ PILLAR 1: Market calendar integration
        ‚úÖ PILLAR 5: Parallel processing

        Stage 1: Fetch ALL tickers for ALL dates using grouped endpoint
        """
        import pandas_market_calendars as mcal
        from concurrent.futures import ThreadPoolExecutor, as_completed

        # ‚úÖ PILLAR 1: Use market calendar (NOT weekday checks)
        nyse = mcal.get_calendar('NYSE')
        trading_dates = nyse.schedule(
            start_date=self.scan_start,
            end_date=self.d0_end_user
        ).index.strftime('%Y-%m-%d').tolist()

        print(f"  üìÖ Fetching {len(trading_dates)} trading days from {self.scan_start} to {self.d0_end_user}")
        print(f"  üåê Using grouped endpoint (1 call per day)")
        print(f"  ‚öôÔ∏è  Parallel workers: {self.stage1_workers}")

        all_data = []

        # ‚úÖ PILLAR 5: Parallel fetching
        with ThreadPoolExecutor(max_workers=self.stage1_workers) as executor:
            future_to_date = {
                executor.submit(self._fetch_grouped_day, date_str): date_str
                for date_str in trading_dates
            }

            for future in as_completed(future_to_date):
                date_str = future_to_date[future]
                try:
                    data = future.result()
                    if data is not None and not data.empty:
                        all_data.append(data)
                        print(f"    ‚úÖ {date_str}: {len(data)} records")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  {date_str}: {e}")

        if all_data:
            result = pd.concat(all_data, ignore_index=True)
            print(f"  ‚úÖ Stage 1 complete: {len(result)} total records")
            return result
        else:
            print(f"  ‚ö†Ô∏è  Stage 1: No data retrieved")
            return pd.DataFrame()

    def _fetch_grouped_day(self, date_str: str):
        """
        Fetch ALL tickers for ONE day using grouped endpoint
        """
        url = f"{self.base_url}/v2/aggs/grouped/locale/us/market/stocks/{date_str}"
        response = self.session.get(url, params={'apiKey': self.api_key, 'adjust': 'true'})

        if response.status_code != 200:
            print(f"    ‚ö†Ô∏è  API error {response.status_code} for {date_str}")
            return None

        data = response.json()
        if "results" not in data or not data["results"]:
            return None

        # Convert to DataFrame
        all_data = []
        for result in data["results"]:
            all_data.append({
                "ticker": result.get("T"),
                "date": date_str,
                "open": result.get("o"),
                "high": result.get("h"),
                "low": result.get("l"),
                "close": result.get("c"),
                "volume": result.get("v"),
            })

        return pd.DataFrame(all_data)

    def compute_simple_features(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 3: Per-ticker operations
        ‚úÖ PILLAR 6: Two-pass feature computation (simple first)

        Stage 2a: Compute SIMPLE features for efficient filtering

        Only computes features needed for filtering:
        - prev_close
        - adv20_usd (with per-ticker groupby)
        - price_range
        """
        if df.empty:
            return df

        print(f"  üìä Stage 2a: Computing simple features for {len(df)} rows")

        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'])

        # Previous close
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)

        # ‚úÖ PILLAR 3: Per-ticker operations for ADV20
        df['adv20_usd'] = (df['close'] * df['volume']).groupby(df['ticker']).transform(
            lambda x: x.rolling(window=20, min_periods=20).mean()
        )

        # Price range
        df['price_range'] = df['high'] - df['low']

        print(f"    ‚úÖ Simple features computed")
        return df

    def apply_smart_filters(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 4: Separate historical from D0 data

        Stage 2b: Smart filters with HISTORICAL DATA PRESERVATION

        CRITICAL: Only filter D0 output range, preserve all historical data
        for ABS window calculations.
        """
        if df.empty:
            return df

        print(f"  üìä Stage 2b input: {len(df)} rows")

        # ‚úÖ PILLAR 4: Split historical from D0
        df_historical = df[~df['date'].between(self.d0_start_user, self.d0_end_user)].copy()
        df_output_range = df[df['date'].between(self.d0_start_user, self.d0_end_user)].copy()

        print(f"    üìä Historical: {len(df_historical)} rows")
        print(f"    üìä D0 range: {len(df_output_range)} rows")

        # ‚úÖ CRITICAL: Filter ONLY D0 range
        df_output_filtered = df_output_range.copy()

        # Price filter
        if 'price_min' in self.params:
            min_price = self.params['price_min']
            df_output_filtered = df_output_filtered[
                (df_output_filtered['close'] >= min_price) &
                (df_output_filtered['open'] >= min_price)
            ]

        # Volume filter
        if 'adv20_min_usd' in self.params:
            min_adv = self.params['adv20_min_usd']
            df_output_filtered = df_output_filtered[
                df_output_filtered['adv20_usd'] >= min_adv
            ]

        print(f"    üìä After filters: {len(df_output_filtered)} rows")

        # ‚úÖ CRITICAL: COMBINE historical + filtered D0
        df_combined = pd.concat([df_historical, df_output_filtered], ignore_index=True)

        print(f"  ‚úÖ Stage 2b complete: {len(df_combined)} rows (historical preserved)")
        return df_combined

    def compute_full_features(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 3: Per-ticker operations
        ‚úÖ PILLAR 6: Two-pass feature computation (full features after filter)

        Stage 3a: Compute ALL technical indicators

        Computes expensive features only on data that passed filters.
        """
        if df.empty:
            return df

        print(f"  üìä Stage 3a: Computing full features for {len(df)} rows")

        result_dfs = []

        for ticker, group in df.groupby('ticker'):
            group = group.sort_values('date').copy()

            # EMA
            group['ema_9'] = group['close'].ewm(span=9, adjust=False).mean()
            group['ema_20'] = group['close'].ewm(span=20, adjust=False).mean()

            # ATR
            hi_lo = group['high'] - group['low']
            hi_prev = (group['high'] - group['close'].shift(1)).abs()
            lo_prev = (group['low'] - group['close'].shift(1)).abs()
            group['tr'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
            group['atr_raw'] = group['tr'].rolling(14, min_periods=14).mean()
            group['atr'] = group['atr_raw'].shift(1)

            result_dfs.append(group)

        result = pd.concat(result_dfs, ignore_index=True)
        print(f"    ‚úÖ Full features computed")
        return result

    def detect_patterns(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 7: Pre-sliced data for parallel processing
        ‚úÖ PILLAR 5: Parallel ticker processing

        Stage 3b: Pattern detection with parallel processing
        """
        if df.empty:
            return []

        print(f"  üéØ Stage 3b: Detecting patterns in {len(df)} rows")

        # Get D0 range
        d0_start_dt = pd.to_datetime(self.d0_start_user)
        d0_end_dt = pd.to_datetime(self.d0_end_user)

        # ‚úÖ PILLAR 7: Pre-slice ticker data BEFORE parallel processing
        ticker_data_list = []
        for ticker, ticker_df in df.groupby('ticker'):
            ticker_data_list.append((ticker, ticker_df.copy(), d0_start_dt, d0_end_dt))

        all_results = []

        # ‚úÖ PILLAR 5: Parallel processing with pre-sliced data
        from concurrent.futures import ThreadPoolExecutor, as_completed

        with ThreadPoolExecutor(max_workers=self.stage3_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_ticker_optimized_pre_sliced, ticker_data): ticker_data[0]
                for ticker_data in ticker_data_list
            }

            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                        print(f"    ‚úÖ {ticker}: {len(results)} signals")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  {ticker}: {e}")

        print(f"  ‚úÖ Stage 3b complete: {len(all_results)} total signals")
        return all_results

    def _process_ticker_optimized_pre_sliced(self, ticker_data: tuple):
        """
        ‚úÖ PILLAR 7: Process pre-sliced ticker data
        ‚úÖ PILLAR 4: Early D0 filtering

        Process pre-sliced ticker data with early D0 range filtering.
        """
        ticker, ticker_df, d0_start_dt, d0_end_dt = ticker_data

        # ‚úÖ Minimum data check
        if len(ticker_df) < 100:
            return []

        # Prepare data for detection
        ticker_df = ticker_df.sort_values('date').reset_index(drop=True)

        # Rename columns to match expected format (Capitalized for Series access)
        df = ticker_df.rename(columns={
            'open': 'Open', 'high': 'High', 'low': 'Low',
            'close': 'Close', 'volume': 'Volume'
        })

        # Compute metrics if add_daily_metrics is available
        try:
            m = add_daily_metrics(df.set_index('date'))
        except:
            m = df.set_index('date')

        # Get parameters
        try:
            P_local = self.params
        except:
            P_local = {
                "price_min": 8.0,
                "adv20_min_usd": 30_000_000,
                "abs_lookback_days": 1000,
                "abs_exclude_days": 10,
                "pos_abs_max": 0.75,
                "atr_mult": 0.9,
                "vol_mult": 2.0,
                "slope5d_min": 3.0,
                "high_ema9_mult": 1.05,
                "gap_div_atr_min": 0.75,
                "open_over_ema9_min": 0.9,
                "d1_green_atr_min": 0.30,
                "require_open_gt_prev_high": True,
                "enforce_d1_above_d2": True,
            }

        # ‚úÖ Initialize results list (detection loop appends to this)
        all_rows = []

        # Detection loop with early D0 filtering
        for i in range(2, len(m)):
            d0 = m.index[i]

            # ‚úÖ PILLAR 4: EARLY FILTER - Skip if not in D0 range
            if d0 < d0_start_dt or d0 > d0_end_dt:
                continue

            r0 = m.iloc[i]
            r1 = m.iloc[i-1]
            r2 = m.iloc[i-2]

            # Original detection logic from scan_symbol
            # This uses the extracted detection loop BODY (content inside the for loop)
            # The extracted body should NOT include its own for loop statement
            try:
                    # Backside vs D-1 close
                    lo_abs, hi_abs = abs_top_window(m, d0, P_local["abs_lookback_days"], P_local["abs_exclude_days"])
                    pos_abs_prev = pos_between(r1["Close"], lo_abs, hi_abs)
                    if not (pd.notna(pos_abs_prev) and pos_abs_prev <= P_local["pos_abs_max"]):
                        continue

                    # Choose trigger
                    trigger_ok = False; trig_row = None; trig_tag = "-"
                    if P_local["trigger_mode"] == "D1_only":
                        if _mold_on_row(r1, P_local): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
                    else:
                        if _mold_on_row(r1, P_local): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
                        elif _mold_on_row(r2): trigger_ok, trig_row, trig_tag = True, r2, "D-2"
                    if not trigger_ok:
                        continue

                    # D-1 must be green
                    if not (pd.notna(r1["Body_over_ATR"]) and r1["Body_over_ATR"] >= P_local["d1_green_atr_min"]):
                        continue

                    # Absolute D-1 volume floor (shares)
                    if P_local["d1_volume_min"] is not None:
                        if not (pd.notna(r1["Volume"]) and r1["Volume"] >= P_local["d1_volume_min"]):
                            continue

                    # Optional relative D-1 vol multiple
                    if P_local["d1_vol_mult_min"] is not None:
                        if not (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"] > 0 and (r1["Volume"]/r1["VOL_AVG"]) >= P_local["d1_vol_mult_min"]):
                            continue

                    # D-1 > D-2 highs & close
                    if P_local["enforce_d1_above_d2"]:
                        if not (pd.notna(r1["High"]) and pd.notna(r2["High"]) and r1["High"] > r2["High"]
                                and pd.notna(r1["Close"]) and pd.notna(r2["Close"]) and r1["Close"] > r2["Close"]):
                            continue

                    # D0 gates
                    if pd.isna(r0["Gap_over_ATR"]) or r0["Gap_over_ATR"] < P_local["gap_div_atr_min"]:
                        continue
                    if P_local["require_open_gt_prev_high"] and not (r0["Open"] > r1["High"]):
                        continue
                    if pd.isna(r0["Open_over_EMA9"]) or r0["Open_over_EMA9"] < P_local["open_over_ema9_min"]:
                        continue

                    d1_vol_mult = (r1["Volume"]/r1["VOL_AVG"]) if (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"]>0) else np.nan
                    volsig_max  = (max(r1["Volume"]/r1["VOL_AVG"], r2["Volume"]/r2["VOL_AVG"])
                                   if (pd.notna(r1["VOL_AVG"]) and r1["VOL_AVG"]>0 and pd.notna(r2["VOL_AVG"]) and r2["VOL_AVG"]>0)
                                   else np.nan)

                    all_rows.append({
                        "Ticker": ticker,
                        "Date": d0.strftime("%Y-%m-%d"),
                        "Trigger": trig_tag,
                        "PosAbs_1000d": round(float(pos_abs_prev), 3),
                        "D1_Body/ATR": round(float(r1["Body_over_ATR"]), 2),
                        "D1Vol(shares)": int(r1["Volume"]) if pd.notna(r1["Volume"]) else np.nan,   # absolute volume
                        "D1Vol/Avg": round(float(d1_vol_mult), 2) if pd.notna(d1_vol_mult) else np.nan,
                        "VolSig(max D-1,D-2)/Avg": round(float(volsig_max), 2) if pd.notna(volsig_max) else np.nan,
                        "Gap/ATR": round(float(r0["Gap_over_ATR"]), 2),
                        "Open>PrevHigh": bool(r0["Open"] > r1["High"]),
                        "Open/EMA9": round(float(r0["Open_over_EMA9"]), 2),
                        "D1>H(D-2)": bool(r1["High"] > r2["High"]),
                        "D1Close>D2Close": bool(r1["Close"] > r2["Close"]),
                        "Slope9_5d": round(float(r0["Slope_9_5d"]), 2) if pd.notna(r0["Slope_9_5d"]) else np.nan,
                        "High-EMA9/ATR(trigger)": round(float(trig_row["High_over_EMA9_div_ATR"]), 2),
                        "ADV20_$": round(float(r0["ADV20_$"])) if pd.notna(r0["ADV20_$"]) else np.nan,
                    })
            except:
                pass

        return all_rows

    def format_results(self, results: list) -> pd.DataFrame:
        """
        Format detection results for output

        Args:
            results: List of detection result dictionaries

        Returns:
            DataFrame with formatted results
        """
        if not results:
            return pd.DataFrame()

        df = pd.DataFrame(results)

        # Reorder columns for better readability
        column_order = ['ticker', 'date', 'close', 'gap', 'gap/atr', 'atr', 'adv20_usd']
        column_order = [col for col in column_order if col in df.columns]

        # Add any additional columns that aren't in the standard order
        additional_cols = [col for col in df.columns if col not in column_order]
        column_order.extend(additional_cols)

        return df[column_order]


if __name__ == "__main__":
    import os

    # Example usage
    scanner = backside_para_b_copy_3(
        api_key=os.getenv('POLYGON_API_KEY', 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'),
        d0_start="2024-01-01",
        d0_end="2024-12-31"
    )

    results = scanner.run_scan()

    if results:
        print(f"\nResults:")
        for r in results[:10]:  # Show first 10
            print(r)
    else:
        print("\nNo results found.")
