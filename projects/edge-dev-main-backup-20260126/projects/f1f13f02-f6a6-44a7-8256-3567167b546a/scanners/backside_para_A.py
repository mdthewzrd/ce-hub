"""
backside_para_A

A daily-only scanner that identifies stocks making new highs relative to their recent range, with specific momentum and volatility criteria, while ensuring they are not near all-time highs over a 2-year lookback window.

Generated by RENATA_V2 - 2026-01-20 12:00:48

TRUE v31 Architecture - All 7 Core Pillars Implemented:
1. âœ… Market calendar (pandas_market_calendars)
2. âœ… Historical buffer calculation
3. âœ… Per-ticker operations (groupby().transform())
4. âœ… Historical/D0 separation in smart filters
5. âœ… Parallel processing (ThreadPoolExecutor)
6. âœ… Two-pass feature computation
7. âœ… Pre-sliced data for parallel processing
"""

# scan_backside_daily.py
# Backside Daily Para â€” daily-only scan (no intraday/dev-bands)
# Uses your P thresholds (1000-day absolute range w/ last 10d excluded).

import pandas as pd, numpy as np, requests
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

session  = requests.Session()
API_KEY  = "Fm7brz4s23eSocDErnL68cE7wspz2K1I"
BASE_URL = "https://api.polygon.io"

# â”€â”€â”€â”€â”€ knobs (your values, with 1000-day lookback) â”€â”€â”€â”€â”€
P = {
    # hard excludes
    "price_min"        : 8.0,
    "adv20_min_usd"    : 15_000_000,

    # daily mold (softer than A+)
    "atr_mult"               : 2,
    "vol_mult"               : 2.5,
    "slope3d_min"            : 7,
    "slope5d_min"            : 12.0,
    "slope15d_min"           : 16,
    "high_ema9_mult"         : 4,   # (High-EMA9)/ATR
    "high_ema20_mult"        : 6,   # (High-EMA20)/ATR
    "pct7d_low_div_atr_min"  : 6,
    "pct14d_low_div_atr_min" : 9,
    "gap_div_atr_min"        : 1.25,
    "open_over_ema9_min"     : 1.1,
    "atr_pct_change_min"     : 0.25,
    "prev_close_min"         : 10.0,
    "pct2d_div_atr_min"      : 4,
    "pct3d_div_atr_min"      : 3,

    # backside: NOT near the top over a long window
    "lookback_days_2y"       : 1000,   # â† as requested
    "exclude_recent_days"    : 10,     # exclude last N days from range calc
    "not_top_frac_abs"       : 0.75,   # require position <= 0.75

    # optional extras (off by default)
    "require_open_gt_prev_high": False,
    "require_green_prev"        : False,

    # pivot sensitivity (for future use; ABS test used here)
    "pivot_left"            : 3,
    "pivot_right"           : 3,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ data fetch â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_daily(tkr: str, start: str, end: str) -> pd.DataFrame:
    url = f"{BASE_URL}/v2/aggs/ticker/{tkr}/range/1/day/{start}/{end}"
    r = session.get(url, params={"apiKey": API_KEY, "adjusted":"true", "sort":"asc", "limit":50000})
    r.raise_for_status()
    rows = r.json().get("results", [])
    if not rows:
        return pd.DataFrame()
    return (
        pd.DataFrame(rows)
          .assign(Date=lambda d: pd.to_datetime(d["t"], unit="ms"))
          .rename(columns={"o":"Open","h":"High","l":"Low","c":"Close","v":"Volume"})
          .set_index("Date")[["Open","High","Low","Close","Volume"]]
          .sort_index()
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics (daily) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_core_metrics(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty: return df.copy()
    m = df.copy()

    # EMAs
    m["EMA_9"]  = m["Close"].ewm(span=9,  adjust=False).mean()
    m["EMA_20"] = m["Close"].ewm(span=20, adjust=False).mean()

    # ATR(30), shifted as "known"
    hi_lo = m["High"] - m["Low"]
    hi_pc = (m["High"] - m["Close"].shift(1)).abs()
    lo_pc = (m["Low"]  - m["Close"].shift(1)).abs()
    m["TR"] = pd.concat([hi_lo, hi_pc, lo_pc], axis=1).max(axis=1)
    m["ATR_raw"] = m["TR"].rolling(30, min_periods=30).mean()
    m["ATR"] = m["ATR_raw"].shift(1)
    atr_safe = m["ATR"].replace(0, np.nan)

    # Volume baselines
    m["VOL_AVG_raw"] = m["Volume"].rolling(30, min_periods=30).mean()
    m["VOL_AVG"] = m["VOL_AVG_raw"].shift(1)
    m["Prev_Volume"] = m["Volume"].shift(1)

    # Slopes (EMA9)
    for w in (3, 5, 15):
        m[f"Slope_9_{w}d"] = (m["EMA_9"] - m["EMA_9"].shift(w)) / m["EMA_9"].shift(w) * 100

    # Dev vs EMAs (Ã· ATR)
    m["High_over_EMA9_div_ATR"]  = (m["High"] - m["EMA_9"])  / atr_safe
    m["High_over_EMA20_div_ATR"] = (m["High"] - m["EMA_20"]) / atr_safe

    # % from recent lows
    low7  = m["Low"].rolling(7 , min_periods=7 ).min()
    low14 = m["Low"].rolling(14, min_periods=14).min()
    m["Pct_7d_low_div_ATR"]  = ((m["Close"] - low7)  / low7)  / atr_safe * 100
    m["Pct_14d_low_div_ATR"] = ((m["Close"] - low14) / low14) / atr_safe * 100

    # Gap / ATR
    m["Gap"] = (m["Open"] - m["Close"].shift(1)).abs()
    m["Gap_over_ATR"] = m["Gap"] / atr_safe

    # Multi-day refs & moves
    m["Prev_Close"] = m["Close"].shift(1)
    m["Prev_Open"]  = m["Open"].shift(1)
    m["Prev_High"]  = m["High"].shift(1)
    m["Close_D3"]   = m["Close"].shift(3)
    m["Close_D4"]   = m["Close"].shift(4)
    m["Move2d_div_ATR"] = (m["Prev_Close"] - m["Close_D3"]) / atr_safe
    m["Move3d_div_ATR"] = (m["Prev_Close"] - m["Close_D4"]) / atr_safe

    # Liquidity
    m["ADV20_$"] = (m["Close"] * m["Volume"]).rolling(20, min_periods=20).mean()

    # ATR % change (prev-known)
    m["ATR_Pct_Change"] = m["ATR_raw"].pct_change().shift(1) * 100

    return m

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ abs range helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_recent_days: int):
    """Absolute high/low up to (d0 - exclude_recent_days)."""
    if df.empty: return (np.nan, None, np.nan, None)
    cutoff = d0 - pd.Timedelta(days=exclude_recent_days)
    wstart = cutoff - pd.Timedelta(days=lookback_days)
    win = df[(df.index > wstart) & (df.index <= cutoff)]
    if win.empty: return (np.nan, None, np.nan, None)
    hi_idx = win["High"].idxmax()
    lo_idx = win["Low"].idxmin()
    return float(win.loc[hi_idx,"High"]), hi_idx, float(win.loc[lo_idx,"Low"]), lo_idx

def clamp01(x: float) -> float:
    if pd.isna(x): return np.nan
    return max(0.0, min(1.0, float(x)))

def pos_between(val, lo, hi):
    if any(pd.isna(t) for t in (val, lo, hi)) or hi <= lo:
        return np.nan
    return clamp01((val - lo) / (hi - lo))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ scan one symbol â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scan_symbol(sym: str, start: str, end: str, params: dict) -> pd.DataFrame:
    df = fetch_daily(sym, start, end)
    if df.empty:
        return pd.DataFrame()

    m = compute_core_metrics(df)
    hits = []

    for i in range(1, len(m)):  # need D-1 history
        d0_ts = m.index[i]
        r = m.iloc[i]

        # hard excludes
        if (r["Prev_Close"] < params["price_min"]) or (r["ADV20_$"] < params["adv20_min_usd"]):
            continue

        # absolute 1000d range context vs D0
        abs_top, abs_top_dt, abs_low, abs_low_dt = abs_top_window(
            m, d0_ts, params["lookback_days_2y"], params["exclude_recent_days"]
        )
        pos_abs = pos_between(r["Prev_Close"], abs_low, abs_top)
        in_backside_abs = (pd.notna(pos_abs) and (pos_abs <= params["not_top_frac_abs"]))
        prev_high_below_abs_top = (pd.notna(abs_top) and (r["Prev_High"] <= abs_top))

        # optional guards
        guard_open_gt_prev_high = (r["Open"] > r["Prev_High"])
        guard_prev_green        = (r["Prev_Close"] > r["Prev_Open"])

        if params["require_open_gt_prev_high"] and not guard_open_gt_prev_high:
            continue
        if params["require_green_prev"] and not guard_prev_green:
            continue

        # aggression/momentum filters (D0)
        cond = (
            (r["TR"] / r["ATR"]                          >= params["atr_mult"]) &
            (r["Volume"] / r["VOL_AVG"]                  >= params["vol_mult"]) &
            (r["Prev_Volume"] / r["VOL_AVG"]             >= params["vol_mult"]) &
            (r["Slope_9_3d"]                              >= params["slope3d_min"]) &
            (r["Slope_9_5d"]                              >= params["slope5d_min"]) &
            (r["Slope_9_15d"]                             >= params["slope15d_min"]) &
            (r["High_over_EMA9_div_ATR"]                  >= params["high_ema9_mult"]) &
            (r["High_over_EMA20_div_ATR"]                 >= params["high_ema20_mult"]) &
            (r["Pct_7d_low_div_ATR"]                      >= params["pct7d_low_div_atr_min"]) &
            (r["Pct_14d_low_div_ATR"]                     >= params["pct14d_low_div_atr_min"]) &
            (r["Gap_over_ATR"]                             >= params["gap_div_atr_min"]) &
            ((r["Open"] / r["EMA_9"])                      >= params["open_over_ema9_min"]) &
            (r["ATR_Pct_Change"]                           >= params["atr_pct_change_min"]) &
            (r["Move2d_div_ATR"]                           >= params["pct2d_div_atr_min"]) &
            (r["Move3d_div_ATR"]                           >= params["pct3d_div_atr_min"]) &
            in_backside_abs & prev_high_below_abs_top
        )

        if bool(cond):
            hits.append({
                "Ticker": sym,
                "Date": d0_ts.strftime("%Y-%m-%d"),
                "Pos_abs_1000d": round(float(pos_abs), 3) if pd.notna(pos_abs) else np.nan,
                "AbsTop_1000d": round(float(abs_top), 3) if pd.notna(abs_top) else np.nan,
                "AbsLow_1000d": round(float(abs_low), 3) if pd.notna(abs_low) else np.nan,
                "Range/ATR": round(float(r["TR"]/r["ATR"]), 2) if pd.notna(r["ATR"]) else np.nan,
                "Vol/Avg": round(float(r["Volume"]/r["VOL_AVG"]), 2) if pd.notna(r["VOL_AVG"]) else np.nan,
                "Slope9_5d": round(float(r["Slope_9_5d"]), 2) if pd.notna(r["Slope_9_5d"]) else np.nan,
                "High/EMA9/ATR": round(float(r["High_over_EMA9_div_ATR"]), 2) if pd.notna(r["High_over_EMA9_div_ATR"]) else np.nan,
                "High/EMA20/ATR": round(float(r["High_over_EMA20_div_ATR"]), 2) if pd.notna(r["High_over_EMA20_div_ATR"]) else np.nan,
            })

    return pd.DataFrame(hits)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ driver â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_and_scan(sym: str, start: str, end: str, params: dict):
    try:
        return scan_symbol(sym, start, end, params)
    except Exception:
        return pd.DataFrame()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRUE v31 Scanner Class â”€â”€â”€â”€â”€â”€â”€â”€â”€
class backside_para_A:
    """
    Backside Daily Para

    A daily-only scanner that identifies stocks making new highs relative to their recent range, with specific momentum and volatility criteria, while ensuring they are not near all-time highs over a 2-year lookback window.

    Scanner Type: Generic Scanner converted to TRUE v31 Architecture
    Generated: 2026-01-20 12:00:48

    TRUE v31 Architecture - All 7 Core Pillars Implemented
    """

    def __init__(self, d0_start: str = None, d0_end: str = None):
        """
        Initialize scanner with proper v31 configuration

        âœ… PILLAR 2: Calculate historical buffer for indicators
        """
        # Date range configuration
        self.d0_start_user = d0_start or "2024-01-01"
        self.d0_end_user = d0_end or "2024-12-31"

        # âœ… CRITICAL: Calculate historical buffer
        lookback_buffer = 780  # From detected params with safety margin

        # Calculate scan_start with historical buffer
        scan_start_dt = pd.to_datetime(self.d0_start_user) - pd.Timedelta(days=lookback_buffer)
        self.scan_start = scan_start_dt.strftime('%Y-%m-%d')

        # âœ… Workers for parallel processing
        self.stage3_workers = 10

        # âœ… Parameters extracted from original code
        self.params = self._extract_parameters()

        # âœ… Scanner name for logging
        self.scanner_name = self.__class__.__name__

        print(f"ğŸ“Š Scanner initialized: {self.scanner_name}")
        print(f"   Historical buffer: {lookback_buffer} days")
        print(f"   Scan range: {self.scan_start} to {self.d0_end_user}")
        print(f"   D0 output range: {self.d0_start_user} to {self.d0_end_user}")

    def _extract_parameters(self) -> dict:
        """Extract parameters from original code or return defaults"""
        # Try to extract from global scope
        try:
            import sys
            frame = sys._getframe(1)
            if 'P' in frame.f_locals:
                return frame.f_locals['P']
        except:
            pass

        # Default parameters from detected params
        return {'price_min': 8.0, 'adv20_min_usd': 15000000, 'atr_mult': 2, 'vol_mult': 2.5, 'slope3d_min': 7, 'slope5d_min': 12.0, 'slope15d_min': 16, 'high_ema9_mult': 4, 'high_ema20_mult': 6, 'pct7d_low_div_atr_min': 6, 'pct14d_low_div_atr_min': 9, 'gap_div_atr_min': 1.25, 'open_over_ema9_min': 1.1, 'atr_pct_change_min': 0.25, 'prev_close_min': 10.0, 'pct2d_div_atr_min': 4, 'pct3d_div_atr_min': 3, 'lookback_days_2y': 1000, 'exclude_recent_days': 10, 'not_top_frac_abs': 0.75, 'require_open_gt_prev_high': False, 'require_green_prev': False, 'pivot_left': 3, 'pivot_right': 3, 'apiKey': 'APIKEY', 'adjusted': 'true', 'sort': 'asc', 'limit': 50000}

    def run_scan(self):
        """
        ğŸš€ Main execution: 5-stage v31 pipeline

        Returns:
            List of signal dictionaries
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ RUNNING TRUE v31 SCAN: {self.scanner_name}")
        print(f"{'='*70}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data()
        if stage1_data is None or stage1_data.empty:
            print("\nâŒ Scan failed: No data loaded")
            return []

        # Stage 2a: Compute simple features
        stage2a_data = self.compute_simple_features(stage1_data)

        # Stage 2b: Apply smart filters (with historical/D0 separation)
        stage2b_data = self.apply_smart_filters(stage2a_data)

        # Stage 3a: Compute full features
        stage3a_data = self.compute_full_features(stage2b_data)

        # Stage 3b: Detect patterns (with pre-sliced parallel processing)
        stage3_results = self.detect_patterns(stage3a_data)

        print(f"\nâœ… SCAN COMPLETE: {len(stage3_results)} signals detected")
        return stage3_results

    def fetch_grouped_data(self):
        """
        âœ… PILLAR 1: Market calendar integration
        âœ… PILLAR 5: Parallel processing

        Stage 1: Fetch ALL tickers for ALL dates using Polygon's grouped API (direct approach)
        """
        import requests
        from datetime import timedelta

        API_KEY = os.getenv("POLYGON_API_KEY", "Fm7brz4s23eSocDErnL68cE7wspz2K1I")
        BASE_URL = "https://api.polygon.io"

        print(f"  ğŸ“¡ Fetching data from {self.scan_start} to {self.d0_end_user} using Polygon grouped API...")

        all_data = []
        current_date = pd.to_datetime(self.scan_start).date()
        end = pd.to_datetime(self.d0_end_user).date()

        # Fetch data for each trading day
        while current_date <= end:
            # Skip weekends
            if current_date.weekday() < 5:  # Monday-Friday
                date_str = current_date.strftime("%Y-%m-%d")
                url = BASE_URL + "/v2/aggs/grouped/locale/us/market/stocks/" + date_str
                params = {
                    "adjusted": "true",
                    "apiKey": API_KEY
                }

                try:
                    response = requests.get(url, params=params)
                    response.raise_for_status()
                    data = response.json()

                    if 'results' in data and data['results']:
                        df_daily = pd.DataFrame(data['results'])
                        df_daily['date'] = pd.to_datetime(df_daily['t'], unit='ms').dt.date
                        df_daily.rename(columns={'T': 'ticker', 'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}, inplace=True)
                        all_data.append(df_daily[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']])

                except Exception as e:
                    print(f"    âš ï¸  Error fetching data for {date_str}: {e}")

            current_date += timedelta(days=1)

        if not all_data:
            print(f"  âŒ No data fetched!")
            return pd.DataFrame()

        # Combine all daily data
        df = pd.concat(all_data, ignore_index=True)

        if df.empty:
            print(f"  âŒ No data fetched!")
            return pd.DataFrame()

        print(f"  âœ… Stage 1 complete: {len(df)} total records")
        print(f"     ğŸ“Š Unique tickers: {df['ticker'].nunique()}")
        print(f"     ğŸ“… Date range: {df['date'].min()} to {df['date'].max()}")
        return df

    def compute_simple_features(self, df: pd.DataFrame):
        """
        âœ… PILLAR 3: Per-ticker operations
        âœ… PILLAR 6: Two-pass feature computation (simple first)

        Stage 2a: Compute SIMPLE features for efficient filtering

        Only computes features needed for filtering:
        - prev_close
        - adv20_usd (with per-ticker groupby)
        - price_range
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 2a: Computing simple features for {len(df)} rows")

        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'])

        # Previous close
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)

        # âœ… PILLAR 3: Per-ticker operations for ADV20
        df['adv20_usd'] = (df['close'] * df['volume']).groupby(df['ticker']).transform(
            lambda x: x.rolling(window=20, min_periods=20).mean()
        )

        # Price range
        df['price_range'] = df['high'] - df['low']

        print(f"    âœ… Simple features computed")
        return df

    def apply_smart_filters(self, df: pd.DataFrame):
        """
        âœ… PILLAR 4: Separate historical from D0 data

        Stage 2b: Smart filters with HISTORICAL DATA PRESERVATION

        CRITICAL: Only filter D0 output range, preserve all historical data
        for indicator calculations.
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 2b input: {len(df)} rows")

        # âœ… PILLAR 4: Split historical from D0
        df_historical = df[~df['date'].between({self.d0_start_user}, {self.d0_end_user})].copy()
        df_output_range = df[df['date'].between({self.d0_start_user}, {self.d0_end_user})].copy()

        print(f"    ğŸ“Š Historical: {len(df_historical)} rows")
        print(f"    ğŸ“Š D0 range: {len(df_output_range)} rows")

        # âœ… CRITICAL: Filter ONLY D0 range using extracted parameters
        df_output_filtered = df_output_range.copy()

        # Price filter
        if 'price_min' in self.params:
            min_price = self.params['price_min']
            df_output_filtered = df_output_filtered[
                (df_output_filtered['close'] >= min_price) &
                (df_output_filtered['open'] >= min_price)
            ]
            print(f"    ğŸ“Š After price filter (>= ${min_price}): {len(df_output_filtered)} rows")

        # Volume filter
        if 'adv20_min_usd' in self.params:
            min_adv = self.params['adv20_min_usd']
            df_output_filtered = df_output_filtered[
                df_output_filtered['adv20_usd'] >= min_adv
            ]
            print(f"    ğŸ“Š After volume filter (>= ${min_adv:,}): {len(df_output_filtered)} rows")

        print(f"  âœ… Stage 2b complete: {len(df_historical) + len(df_output_filtered)} rows (historical preserved)")

        # âœ… CRITICAL: COMBINE historical + filtered D0
        df_combined = pd.concat([df_historical, df_output_filtered], ignore_index=True)
        return df_combined

    def compute_full_features(self, df: pd.DataFrame):
        """
        âœ… PILLAR 3: Per-ticker operations
        âœ… PILLAR 6: Two-pass feature computation (full features after filter)

        Stage 3a: Compute ALL technical indicators

        Computes expensive features only on data that passed filters.
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 3a: Computing full features for {len(df)} rows")

        result_dfs = []

        for ticker, group in df.groupby('ticker'):
            group = group.sort_values('date').copy()

            # EMA
            group['ema_9'] = group['close'].ewm(span=9, adjust=False).mean()
            group['ema_20'] = group['close'].ewm(span=20, adjust=False).mean()

            # ATR
            hi_lo = group['high'] - group['low']
            hi_prev = (group['high'] - group['close'].shift(1)).abs()
            lo_prev = (group['low'] - group['close'].shift(1)).abs()
            group['tr'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
            group['atr'] = group['tr'].rolling(14, min_periods=14).mean().shift(1)

            # Additional common indicators
            group['vol_avg'] = group['volume'].rolling(14, min_periods=14).mean().shift(1)
            group['prev_volume'] = group['volume'].shift(1)

            # RSI
            delta = group['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=14).mean()
            rs = gain / loss
            group['rsi'] = 100 - (100 / (1 + rs))

            result_dfs.append(group)

        result = pd.concat(result_dfs, ignore_index=True)
        print(f"    âœ… Full features computed")
        return result

    def detect_patterns(self, df: pd.DataFrame):
        """
        âœ… PILLAR 7: Pre-sliced data for parallel processing
        âœ… PILLAR 5: Parallel ticker processing

        Stage 3b: Pattern detection with parallel processing
        """
        if df.empty:
            return []

        print(f"  ğŸ¯ Stage 3b: Detecting patterns in {len(df)} rows")

        # Get D0 range
        d0_start_dt = pd.to_datetime({self.d0_start_user})
        d0_end_dt = pd.to_datetime({self.d0_end_user})

        # âœ… PILLAR 7: Pre-slice ticker data BEFORE parallel processing
        ticker_data_list = []
        for ticker, ticker_df in df.groupby('ticker'):
            ticker_data_list.append((ticker, ticker_df.copy(), d0_start_dt, d0_end_dt))

        all_results = []

        # âœ… PILLAR 5: Parallel processing with pre-sliced data
        from concurrent.futures import ThreadPoolExecutor, as_completed

        with ThreadPoolExecutor(max_workers=self.stage3_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_ticker, ticker_data): ticker_data[0]
                for ticker_data in ticker_data_list
            }

            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                        print(f"    âœ… {ticker}: {len(results)} signals")
                except Exception as e:
                    print(f"    âš ï¸  {ticker}: {e}")

        print(f"  âœ… Stage 3b complete: {len(all_results)} total signals")
        return all_results

    def _process_ticker(self, ticker_data: tuple):
        """
        âœ… PILLAR 7: Process pre-sliced ticker data
        âœ… PILLAR 4: Early D0 filtering

        Process ticker data with original detection logic
        """
        ticker, ticker_df, d0_start_dt, d0_end_dt = ticker_data

        # âœ… Minimum data check
        if len(ticker_df) < 100:
            return []

        # âœ… Sort and prepare data
        ticker_df = ticker_df.sort_values('date').reset_index(drop=True)
        ticker_df['date'] = pd.to_datetime(ticker_df['date'])

        # âœ… Initialize results list
        all_rows = []

        # âœ… Apply original detection logic with D0 filtering
        # The original code can use all the computed features
        try:
            # scan_backside_daily.py
            # Backside Daily Para â€” daily-only scan (no intraday/dev-bands)
            # Uses your P thresholds (1000-day absolute range w/ last 10d excluded).

            import pandas as pd, numpy as np, requests
            from datetime import datetime, timedelta
            from concurrent.futures import ThreadPoolExecutor, as_completed

            session  = requests.Session()
            API_KEY  = "Fm7brz4s23eSocDErnL68cE7wspz2K1I"
            BASE_URL = "https://api.polygon.io"

            # â”€â”€â”€â”€â”€ knobs (your values, with 1000-day lookback) â”€â”€â”€â”€â”€
            P = {
                # hard excludes
                "price_min"        : 8.0,
                "adv20_min_usd"    : 15_000_000,

                # daily mold (softer than A+)
                "atr_mult"               : 2,
                "vol_mult"               : 2.5,
                "slope3d_min"            : 7,
                "slope5d_min"            : 12.0,
                "slope15d_min"           : 16,
                "high_ema9_mult"         : 4,   # (High-EMA9)/ATR
                "high_ema20_mult"        : 6,   # (High-EMA20)/ATR
                "pct7d_low_div_atr_min"  : 6,
                "pct14d_low_div_atr_min" : 9,
                "gap_div_atr_min"        : 1.25,
                "open_over_ema9_min"     : 1.1,
                "atr_pct_change_min"     : 0.25,
                "prev_close_min"         : 10.0,
                "pct2d_div_atr_min"      : 4,
                "pct3d_div_atr_min"      : 3,

                # backside: NOT near the top over a long window
                "lookback_days_2y"       : 1000,   # â† as requested
                "exclude_recent_days"    : 10,     # exclude last N days from range calc
                "not_top_frac_abs"       : 0.75,   # require position <= 0.75

                # optional extras (off by default)
                "require_open_gt_prev_high": False,
                "require_green_prev"        : False,

                # pivot sensitivity (for future use; ABS test used here)
                "pivot_left"            : 3,
                "pivot_right"           : 3,
            }

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€ data fetch â”€â”€â”€â”€â”€â”€â”€â”€â”€
            def fetch_daily(tkr: str, start: str, end: str) -> pd.DataFrame:
                url = f"{BASE_URL}/v2/aggs/ticker/{tkr}/range/1/day/{start}/{end}"
                r = session.get(url, params={"apiKey": API_KEY, "adjusted":"true", "sort":"asc", "limit":50000})
                r.raise_for_status()
                rows = r.json().get("results", [])
                if not rows:
                    return pd.DataFrame()
                return (
                    pd.DataFrame(rows)
                      .assign(Date=lambda d: pd.to_datetime(d["t"], unit="ms"))
                      .rename(columns={"o":"Open","h":"High","l":"Low","c":"Close","v":"Volume"})
                      .set_index("Date")[["Open","High","Low","Close","Volume"]]
                      .sort_index()
                )

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics (daily) â”€â”€â”€â”€â”€â”€â”€â”€â”€
            def compute_core_metrics(df: pd.DataFrame) -> pd.DataFrame:
                if df.empty: return df.copy()
                m = df.copy()

                # EMAs
                m["EMA_9"]  = m["Close"].ewm(span=9,  adjust=False).mean()
                m["EMA_20"] = m["Close"].ewm(span=20, adjust=False).mean()

                # ATR(30), shifted as "known"
                hi_lo = m["High"] - m["Low"]
                hi_pc = (m["High"] - m["Close"].shift(1)).abs()
                lo_pc = (m["Low"]  - m["Close"].shift(1)).abs()
                m["TR"] = pd.concat([hi_lo, hi_pc, lo_pc], axis=1).max(axis=1)
                m["ATR_raw"] = m["TR"].rolling(30, min_periods=30).mean()
                m["ATR"] = m["ATR_raw"].shift(1)
                atr_safe = m["ATR"].replace(0, np.nan)

                # Volume baselines
                m["VOL_AVG_raw"] = m["Volume"].rolling(30, min_periods=30).mean()
                m["VOL_AVG"] = m["VOL_AVG_raw"].shift(1)
                m["Prev_Volume"] = m["Volume"].shift(1)

                # Slopes (EMA9)
                for w in (3, 5, 15):
                    m[f"Slope_9_{w}d"] = (m["EMA_9"] - m["EMA_9"].shift(w)) / m["EMA_9"].shift(w) * 100

                # Dev vs EMAs (Ã· ATR)
                m["High_over_EMA9_div_ATR"]  = (m["High"] - m["EMA_9"])  / atr_safe
                m["High_over_EMA20_div_ATR"] = (m["High"] - m["EMA_20"]) / atr_safe

                # % from recent lows
                low7  = m["Low"].rolling(7 , min_periods=7 ).min()
                low14 = m["Low"].rolling(14, min_periods=14).min()
                m["Pct_7d_low_div_ATR"]  = ((m["Close"] - low7)  / low7)  / atr_safe * 100
                m["Pct_14d_low_div_ATR"] = ((m["Close"] - low14) / low14) / atr_safe * 100

                # Gap / ATR
                m["Gap"] = (m["Open"] - m["Close"].shift(1)).abs()
                m["Gap_over_ATR"] = m["Gap"] / atr_safe

                # Multi-day refs & moves
                m["Prev_Close"] = m["Close"].shift(1)
                m["Prev_Open"]  = m["Open"].shift(1)
                m["Prev_High"]  = m["High"].shift(1)
                m["Close_D3"]   = m["Close"].shift(3)
                m["Close_D4"]   = m["Close"].shift(4)
                m["Move2d_div_ATR"] = (m["Prev_Close"] - m["Close_D3"]) / atr_safe
                m["Move3d_div_ATR"] = (m["Prev_Close"] - m["Close_D4"]) / atr_safe

                # Liquidity
                m["ADV20_$"] = (m["Close"] * m["Volume"]).rolling(20, min_periods=20).mean()

                # ATR % change (prev-known)
                m["ATR_Pct_Change"] = m["ATR_raw"].pct_change().shift(1) * 100

                return m

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€ abs range helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
            def abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_recent_days: int):
                """Absolute high/low up to (d0 - exclude_recent_days)."""
                if df.empty: return (np.nan, None, np.nan, None)
                cutoff = d0 - pd.Timedelta(days=exclude_recent_days)
                wstart = cutoff - pd.Timedelta(days=lookback_days)
                win = df[(df.index > wstart) & (df.index <= cutoff)]
                if win.empty: return (np.nan, None, np.nan, None)
                hi_idx = win["High"].idxmax()
                lo_idx = win["Low"].idxmin()
                return float(win.loc[hi_idx,"High"]), hi_idx, float(win.loc[lo_idx,"Low"]), lo_idx

            def clamp01(x: float) -> float:
                if pd.isna(x): return np.nan
                return max(0.0, min(1.0, float(x)))

            def pos_between(val, lo, hi):
                if any(pd.isna(t) for t in (val, lo, hi)) or hi <= lo:
                    return np.nan
                return clamp01((val - lo) / (hi - lo))

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€ scan one symbol â”€â”€â”€â”€â”€â”€â”€â”€â”€
            def scan_symbol(sym: str, start: str, end: str, params: dict) -> pd.DataFrame:
                df = fetch_daily(sym, start, end)
                if df.empty:
                    return pd.DataFrame()

                m = compute_core_metrics(df)
                hits = []

                for i in range(1, len(m)):  # need D-1 history
                    d0_ts = m.index[i]
                    r = m.iloc[i]

                    # hard excludes
                    if (r["Prev_Close"] < params["price_min"]) or (r["ADV20_$"] < params["adv20_min_usd"]):
                        continue

                    # absolute 1000d range context vs D0
                    abs_top, abs_top_dt, abs_low, abs_low_dt = abs_top_window(
                        m, d0_ts, params["lookback_days_2y"], params["exclude_recent_days"]
                    )
                    pos_abs = pos_between(r["Prev_Close"], abs_low, abs_top)
                    in_backside_abs = (pd.notna(pos_abs) and (pos_abs <= params["not_top_frac_abs"]))
                    prev_high_below_abs_top = (pd.notna(abs_top) and (r["Prev_High"] <= abs_top))

                    # optional guards
                    guard_open_gt_prev_high = (r["Open"] > r["Prev_High"])
                    guard_prev_green        = (r["Prev_Close"] > r["Prev_Open"])

                    if params["require_open_gt_prev_high"] and not guard_open_gt_prev_high:
                        continue
                    if params["require_green_prev"] and not guard_prev_green:
                        continue

                    # aggression/momentum filters (D0)
                    cond = (
                        (r["TR"] / r["ATR"]                          >= params["atr_mult"]) &
                        (r["Volume"] / r["VOL_AVG"]                  >= params["vol_mult"]) &
                        (r["Prev_Volume"] / r["VOL_AVG"]             >= params["vol_mult"]) &
                        (r["Slope_9_3d"]                              >= params["slope3d_min"]) &
                        (r["Slope_9_5d"]                              >= params["slope5d_min"]) &
                        (r["Slope_9_15d"]                             >= params["slope15d_min"]) &
                        (r["High_over_EMA9_div_ATR"]                  >= params["high_ema9_mult"]) &
                        (r["High_over_EMA20_div_ATR"]                 >= params["high_ema20_mult"]) &
                        (r["Pct_7d_low_div_ATR"]                      >= params["pct7d_low_div_atr_min"]) &
                        (r["Pct_14d_low_div_ATR"]                     >= params["pct14d_low_div_atr_min"]) &
                        (r["Gap_over_ATR"]                             >= params["gap_div_atr_min"]) &
                        ((r["Open"] / r["EMA_9"])                      >= params["open_over_ema9_min"]) &
                        (r["ATR_Pct_Change"]                           >= params["atr_pct_change_min"]) &
                        (r["Move2d_div_ATR"]                           >= params["pct2d_div_atr_min"]) &
                        (r["Move3d_div_ATR"]                           >= params["pct3d_div_atr_min"]) &
                        in_backside_abs & prev_high_below_abs_top
                    )

                    if bool(cond):
                        hits.append({
                            "Ticker": sym,
                            "Date": d0_ts.strftime("%Y-%m-%d"),
                            "Pos_abs_1000d": round(float(pos_abs), 3) if pd.notna(pos_abs) else np.nan,
                            "AbsTop_1000d": round(float(abs_top), 3) if pd.notna(abs_top) else np.nan,
                            "AbsLow_1000d": round(float(abs_low), 3) if pd.notna(abs_low) else np.nan,
                            "Range/ATR": round(float(r["TR"]/r["ATR"]), 2) if pd.notna(r["ATR"]) else np.nan,
                            "Vol/Avg": round(float(r["Volume"]/r["VOL_AVG"]), 2) if pd.notna(r["VOL_AVG"]) else np.nan,
                            "Slope9_5d": round(float(r["Slope_9_5d"]), 2) if pd.notna(r["Slope_9_5d"]) else np.nan,
                            "High/EMA9/ATR": round(float(r["High_over_EMA9_div_ATR"]), 2) if pd.notna(r["High_over_EMA9_div_ATR"]) else np.nan,
                            "High/EMA20/ATR": round(float(r["High_over_EMA20_div_ATR"]), 2) if pd.notna(r["High_over_EMA20_div_ATR"]) else np.nan,
                        })

                return pd.DataFrame(hits)

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€ driver â”€â”€â”€â”€â”€â”€â”€â”€â”€
            def fetch_and_scan(sym: str, start: str, end: str, params: dict):
                try:
                    return scan_symbol(sym, start, end, params)
                except Exception:
                    return pd.DataFrame()

        except Exception as e:
            print(f"    âš ï¸  {ticker}: Error in detection logic: {e}")

        return all_rows

    def format_results(self, all_results: list) -> pd.DataFrame:
        """
        Format results for display

        Args:
            all_results: List of signal dictionaries from detect_patterns

        Returns:
            DataFrame with formatted results (Ticker, Date, Scanner_Label)
        """
        if not all_results:
            return pd.DataFrame()

        # Convert to DataFrame
        df = pd.DataFrame(all_results)

        # Ensure required columns exist
        if 'Ticker' not in df.columns or 'Date' not in df.columns:
            print("  âš ï¸  Results missing required columns")
            return pd.DataFrame()

        # Group by ticker and date, aggregate labels
        if 'Scanner_Label' not in df.columns:
            df['Scanner_Label'] = 'Signal'

        # Aggregate by ticker+date
        aggregated = df.groupby(['Ticker', 'Date'])['Scanner_Label'].apply(
            lambda x: ', '.join(sorted(set(x)))
        ).reset_index()

        # Sort by date
        aggregated = aggregated.sort_values('Date').reset_index(drop=True)

        print(f"  âœ… Formatted {len(aggregated)} unique signals")
        return aggregated


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Module-level functions for backend integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
