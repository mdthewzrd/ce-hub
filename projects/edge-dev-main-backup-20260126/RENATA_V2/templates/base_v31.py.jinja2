{#
  EdgeDev v31 Base Scanner Template

  This template generates a trading scanner following the EdgeDev v31 5-stage architecture:
  1. fetch_grouped_data - Get all tickers that traded each day
  2. apply_smart_filters - Reduce dataset by 99% with price/volume filters
  3. detect_patterns - Check if pattern conditions are met
  4. format_results - Format output for display
  5. run_scan - Orchestrate the entire pipeline

  Variables:
  - scanner_name: Name of the scanner (e.g., "D2_Gap_Scanner")
  - strategy_name: Human-readable strategy name
  - description: Scanner description
  - date_range: Date range for scan (e.g., "2024-01-01 to 2024-12-31")
  - needs_polygon_api: Boolean - whether to use Polygon API
  - pattern_detection_method: Generated detect_patterns code
  - smart_filters: Smart filter configuration
#}
"""
{{ scanner_name }}

{{ description }}

Generated by RENATA_V2 - {{ timestamp }}
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
{% if needs_polygon_api %}
import requests
from io import StringIO
{% endif %}


class {{ scanner_name }}:
    """
    {{ strategy_name }}

    {{ description }}

    Scanner Type: {{ scanner_type }}
    Generated: {{ timestamp }}
    """

    def __init__(self):
        """Initialize scanner"""
        self.scanner_name = "{{ scanner_name }}"
        self.strategy_name = "{{ strategy_name }}"
        self.date_range = "{{ date_range }}"

        # Smart filter configuration
        self.smart_filters = {
            "min_prev_close": {{ smart_filters.min_prev_close | default(0.75) }},
            "max_prev_close": {{ smart_filters.max_prev_close | default(1000) }},
            "min_prev_volume": {{ smart_filters.min_prev_volume | default(500000) }},
            "max_prev_volume": {{ smart_filters.max_prev_volume | default(100000000) }}
        }

        # Results storage
        self.all_results = []
        self.stage1_data = None
        self.stage2_data = None
        self.stage3_results = None

    def fetch_grouped_data(
        self,
        start_date: str,
        end_date: str,
        workers: int = {{ stage1_workers | default(4) }}
    ) -> pd.DataFrame:
        """
        Stage 1: Fetch grouped daily data for all tickers

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            workers: Number of parallel workers

        Returns:
            DataFrame with columns: [date, ticker, open, high, low, close, volume]
        """
        {% if needs_polygon_api %}
        return self._fetch_from_polygon(start_date, end_date)
        {% else %}
        return self._fetch_from_files(start_date, end_date)
        {% endif %}

    {% if needs_polygon_api %}
    def _fetch_from_polygon(
        self,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        Fetch data from Polygon API

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)

        Returns:
            DataFrame with grouped data
        """
        # TODO: Implement Polygon API fetching
        # This is a placeholder - implement based on your API key and needs
        pass
    {% endif %}

    {% if not needs_polygon_api %}
    def _fetch_from_files(
        self,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        Fetch data from local files

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)

        Returns:
            DataFrame with grouped data
        """
        # TODO: Implement file-based data loading
        # This is a placeholder - implement based on your data structure
        pass
    {% endif %}

    def apply_smart_filters(
        self,
        stage1_data: pd.DataFrame,
        workers: int = {{ stage3_workers | default(4) }}
    ) -> pd.DataFrame:
        """
        Stage 2: Apply smart filters to reduce dataset by ~99%

        Args:
            stage1_data: Output from fetch_grouped_data
            workers: Number of parallel workers

        Returns:
            Filtered DataFrame (stage2_data)
        """
        if stage1_data.empty:
            return pd.DataFrame()

        df = stage1_data.copy()

        # Apply price filters
        if self.smart_filters["min_prev_close"]:
            df = df[df['close'] >= self.smart_filters["min_prev_close"]]

        if self.smart_filters["max_prev_close"]:
            df = df[df['close'] <= self.smart_filters["max_prev_close"]]

        # Apply volume filters
        if self.smart_filters["min_prev_volume"]:
            df = df[df['volume'] >= self.smart_filters["min_prev_volume"]]

        if self.smart_filters["max_prev_volume"]:
            df = df[df['volume'] <= self.smart_filters["max_prev_volume"]]

        return df

    def detect_patterns(self, stage2_data: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        Stage 3: Detect {{ strategy_name }} patterns

        Args:
            stage2_data: Filtered DataFrame from apply_smart_filters

        Returns:
            List of dictionaries with pattern results
        """
        if stage2_data.empty:
            return []

        results = []

        {{ pattern_detection_method | indent(8) }}

        return results

    def format_results(
        self,
        stage3_results: List[Dict[str, Any]]
    ) -> pd.DataFrame:
        """
        Stage 4: Format results for display

        Args:
            stage3_results: Output from detect_patterns

        Returns:
            Formatted DataFrame with results
        """
        if not stage3_results:
            return pd.DataFrame()

        df = pd.DataFrame(stage3_results)

        # Reorder columns for display
        column_order = ['date', 'ticker', 'entry_price']
        remaining_cols = [col for col in df.columns if col not in column_order]
        df = df[column_order + remaining_cols]

        return df

    def run_scan(
        self,
        start_date: str,
        end_date: str,
        workers: int = {{ stage1_workers | default(4) }}
    ) -> pd.DataFrame:
        """
        Stage 5: Run complete scan pipeline

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            workers: Number of parallel workers for data fetching

        Returns:
            Formatted results DataFrame
        """
        print(f"Running {self.scanner_name}...")
        print(f"Date range: {start_date} to {end_date}")

        # Stage 1: Fetch data
        print("Stage 1: Fetching grouped data...")
        self.stage1_data = self.fetch_grouped_data(start_date, end_date, workers)
        print(f"  Loaded {len(self.stage1_data)} records")

        # Stage 2: Apply smart filters
        print("Stage 2: Applying smart filters...")
        self.stage2_data = self.apply_smart_filters(self.stage1_data, workers)
        print(f"  Filtered to {len(self.stage2_data)} records")

        # Stage 3: Detect patterns
        print("Stage 3: Detecting patterns...")
        self.stage3_results = self.detect_patterns(self.stage2_data)
        print(f"  Found {len(self.stage3_results)} signals")

        # Stage 4: Format results
        print("Stage 4: Formatting results...")
        formatted_results = self.format_results(self.stage3_results)

        print(f"Scan complete! Found {len(formatted_results)} total results")
        return formatted_results


# ──────────────────────────────────────
# Module-level functions for backend integration
# ──────────────────────────────────────

def run_scan(start_date=None, end_date=None):
    """
    Module-level function for backend integration.
    Returns FULL results DataFrame with all columns (volume, gap%, score, etc.)
    """
    import os

    scanner = {{ scanner_name }}()

    results = scanner.run_scan(
        start_date=start_date or "{{ date_range.split(' to ')[0] }}",
        end_date=end_date or "{{ date_range.split(' to ')[1] }}"
    )

    # Return FULL DataFrame as list of dicts with ALL columns
    if not results.empty:
        return results.to_dict('records')
    else:
        return []


def main():
    """
    Main function for backend integration.
    Returns simple ticker+date results.
    """
    return run_scan()


if __name__ == "__main__":
    # Example usage
    scanner = {{ scanner_name }}()

    results = scanner.run_scan(
        start_date="{{ date_range.split(' to ')[0] }}",
        end_date="{{ date_range.split(' to ')[1] }}"
    )

    if not results.empty:
        print(f"\nResults ({len(results)} signals):")
        for idx, row in results.iterrows():
            ticker = row.get('ticker', row.get('Ticker', row.get('symbol', 'N/A')))
            date = row.get('date', row.get('Date', row.get('timestamp', 'N/A')))
            print(f"{ticker} {date}")
    else:
        print("\nNo results found.")
