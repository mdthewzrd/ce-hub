"""
RENATA_V2 Transformer

Main orchestration layer for code transformation pipeline.
"""

import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from RENATA_V2.core.ast_parser import ASTParser, ClassificationResult
from RENATA_V2.core.ai_agent import AIAgent, AIExtractionError
from RENATA_V2.core.template_engine import TemplateEngine, TemplateEngineError
from RENATA_V2.core.validator import Validator, ValidationResult
from RENATA_V2.core.models import (
    StrategySpec,
    ParameterSpec,
    PatternFilterSpec
)


class TransformationError(Exception):
    """Transformation error"""
    pass


class TransformationResult:
    """Result of transformation"""

    def __init__(
        self,
        success: bool,
        generated_code: Optional[str] = None,
        validation_results: Optional[List[ValidationResult]] = None,
        transformation_metadata: Optional[Dict[str, Any]] = None,
        errors: Optional[List[str]] = None,
        corrections_made: int = 0
    ):
        self.success = success
        self.generated_code = generated_code
        self.validation_results = validation_results or []
        self.transformation_metadata = transformation_metadata or {}
        self.errors = errors or []
        self.corrections_made = corrections_made

    def __repr__(self):
        return f"TransformationResult(success={self.success}, corrections={self.corrections_made})"


class RenataTransformer:
    """
    Main transformer orchestrating the complete pipeline

    Pipeline stages:
    1. AST Parsing - Understand code structure
    2. AI Analysis - Extract strategy and parameters
    3. Template Selection - Choose appropriate template
    4. Code Generation - Render template with context
    5. Validation - Check syntax, structure, logic
    6. Self-Correction - Fix validation errors (max 3 attempts)
    """

    def __init__(
        self,
        max_correction_attempts: int = 3,
        template_dir: Optional[str] = None
    ):
        """
        Initialize transformer

        Args:
            max_correction_attempts: Maximum self-correction iterations
            template_dir: Path to templates directory
        """
        self.max_correction_attempts = max_correction_attempts

        # Initialize components
        self.ast_parser = ASTParser()
        self.ai_agent = AIAgent()
        self.template_engine = TemplateEngine(template_dir)
        self.validator = Validator()

    def transform(
        self,
        source_code: str,
        scanner_name: Optional[str] = None,
        date_range: str = "2024-01-01 to 2024-12-31",
        verbose: bool = True
    ) -> TransformationResult:
        """
        Transform source scanner code to v31 standard

        Args:
            source_code: Original scanner code
            scanner_name: Name for the transformed scanner (optional)
            date_range: Date range for scanning
            verbose: Print progress messages

        Returns:
            TransformationResult with generated code and metadata
        """
        if verbose:
            print("=" * 60)
            print("RENATA_V2 TRANSFORMATION PIPELINE")
            print("=" * 60)

        metadata = {
            'timestamp': datetime.now().isoformat(),
            'date_range': date_range,
            'stages_completed': [],
            'corrections': []
        }

        errors = []
        corrections_made = 0

        try:
            # Stage 1: AST Parsing
            if verbose:
                print("\n[Stage 1] Parsing code structure...")

            ast_result = self._parse_source_code(source_code)
            metadata['stages_completed'].append('ast_parsing')
            metadata['scanner_type'] = ast_result.scanner_type.value

            if verbose:
                print(f"  ‚úì Scanner type: {ast_result.scanner_type.value}")
                print(f"  ‚úì Confidence: {ast_result.confidence}")

            # Stage 2: AI Analysis
            if verbose:
                print("\n[Stage 2] Extracting strategy with AI...")

            try:
                strategy, parameters = self._extract_with_ai(
                    source_code, ast_result
                )
                metadata['stages_completed'].append('ai_analysis')
                metadata['strategy_name'] = strategy.name

                if verbose:
                    print(f"  ‚úì Strategy: {strategy.name}")
                    print(f"  ‚úì Type: {strategy.strategy_type.value}")
                    print(f"  ‚úì Timeframe: {strategy.timeframe}")
            except Exception as ai_error:
                # Check if this is a standalone scanner - if so, use fallback
                if self._is_standalone_scanner(ast_result, source_code):
                    if verbose:
                        print(f"  ‚ö† AI analysis failed: {ai_error}")
                        print(f"  ‚Üí Using fallback strategy for standalone scanner")

                    # Create minimal strategy for standalone scanner
                    from RENATA_V2.core.models import StrategyType

                    strategy = StrategySpec(
                        name=f"{scanner_name or 'Standalone'}_Scanner",
                        description="Standalone trading scanner converted to v31 architecture",
                        strategy_type=StrategyType.OTHER,
                        entry_conditions=[],
                        exit_conditions=[],
                        parameters={},
                        timeframe="daily",
                        rationale="Converted from standalone scanner",
                        scanner_type="single"
                    )

                    parameters = ParameterSpec(
                        price_thresholds={},
                        volume_thresholds={},
                        gap_thresholds={},
                        ema_periods={},
                        consecutive_day_requirements={},
                        other_parameters={}
                    )

                    metadata['stages_completed'].append('ai_analysis_fallback')
                    metadata['strategy_name'] = strategy.name
                    metadata['ai_error'] = str(ai_error)
                elif self._is_multi_scanner(ast_result, source_code):
                    # Multi-scanner with AI failure - use minimal fallback
                    if verbose:
                        print(f"  ‚ö† AI analysis failed: {ai_error}")
                        print(f"  ‚Üí Using fallback strategy for multi-scanner")

                    # Create minimal strategy for multi-scanner
                    from RENATA_V2.core.models import StrategyType

                    strategy = StrategySpec(
                        name=f"{scanner_name or 'Multi'}_Scanner",
                        description="Multi-pattern trading scanner converted to v31 architecture",
                        strategy_type=StrategyType.OTHER,
                        entry_conditions=[],
                        exit_conditions=[],
                        parameters={},
                        timeframe="daily",
                        rationale="Converted from multi-scanner",
                        scanner_type="multi"
                    )

                    parameters = ParameterSpec(
                        price_thresholds={},
                        volume_thresholds={},
                        gap_thresholds={},
                        ema_periods={},
                        consecutive_day_requirements={},
                        other_parameters={}
                    )

                    metadata['stages_completed'].append('ai_analysis_fallback')
                    metadata['strategy_name'] = strategy.name
                    metadata['ai_error'] = str(ai_error)
                else:
                    # Not a standalone or multi-scanner - AI analysis is required
                    raise TransformationError(
                        f"AI extraction failed and this is not a recognized scanner type: {ai_error}"
                    )

            # Stage 3: Template Selection
            if verbose:
                print("\n[Stage 3] Selecting template...")

            template_name = self._select_template(
                ast_result, strategy, source_code
            )
            metadata['stages_completed'].append('template_selection')
            metadata['template_used'] = template_name

            if verbose:
                print(f"  ‚úì Template: {template_name}")

            # Stage 4: Pattern Logic Generation
            if verbose:
                print("\n[Stage 4] Generating pattern detection logic...")

            try:
                pattern_detection_code = self._generate_pattern_logic(
                    strategy, parameters
                )
                metadata['stages_completed'].append('pattern_generation')

                if verbose:
                    print(f"  ‚úì Generated {len(pattern_detection_code.split())} tokens of pattern logic")
            except Exception as pattern_error:
                # For v31_hybrid mode, pattern logic is in the original code
                if template_name in ['v31_hybrid', 'v31_hybrid_multi']:
                    if verbose:
                        print(f"  ‚ö† Pattern generation failed (expected for hybrid mode)")
                        print(f"  ‚Üí Original pattern logic will be preserved")

                    pattern_detection_code = "# Pattern logic preserved from original scanner"
                    metadata['stages_completed'].append('pattern_generation_skipped')
                    metadata['pattern_error'] = str(pattern_error)
                else:
                    raise TransformationError(f"Pattern logic generation failed: {pattern_error}")

            # Stage 5: Code Generation (with self-correction loop)
            if verbose:
                print("\n[Stage 5] Generating code with validation...")

            generated_code, validation_results, corrections = self._generate_with_validation(
                source_code=source_code,
                scanner_name=scanner_name,
                strategy=strategy,
                parameters=parameters,
                ast_result=ast_result,
                pattern_detection_code=pattern_detection_code,
                template_name=template_name,
                date_range=date_range,
                verbose=verbose
            )

            metadata['stages_completed'].append('code_generation')
            metadata['corrections'] = corrections
            corrections_made = len(corrections)

            if verbose:
                print(f"  ‚úì Generated {len(generated_code)} lines of code")
                if corrections:
                    print(f"  ‚úì Applied {len(corrections)} corrections during generation")

            # Final validation summary
            if verbose:
                print("\n[Validation Summary]")
                for result in validation_results:
                    status = "‚úì PASS" if result.is_valid else "‚úó FAIL"
                    print(f"  {status} {result.category.upper()}")

                    if result.warnings:
                        print(f"    ({len(result.warnings)} warnings)")

            # Check if all validations passed
            all_valid = all(r.is_valid for r in validation_results)

            if not all_valid:
                errors.append(
                    "Validation failed after maximum correction attempts. "
                    "Please review the generated code."
                )

            return TransformationResult(
                success=all_valid,
                generated_code=generated_code,
                validation_results=validation_results,
                transformation_metadata=metadata,
                errors=errors,
                corrections_made=corrections_made
            )

        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            print(f"\n‚ùå TRANSFORMATION ERROR:")
            print(error_trace)
            errors.append(f"Transformation failed: {e}")
            return TransformationResult(
                success=False,
                errors=errors,
                transformation_metadata=metadata
            )

    def _parse_source_code(self, source_code: str) -> ClassificationResult:
        """
        Parse source code with AST

        Args:
            source_code: Original scanner code

        Returns:
            ClassificationResult with scanner type
        """
        try:
            self.ast_parser.parse_code(source_code)
            return self.ast_parser.classify_scanner_type()

        except Exception as e:
            raise TransformationError(f"AST parsing failed: {e}")

    def _extract_with_ai(
        self,
        source_code: str,
        ast_result: ClassificationResult
    ) -> Tuple[StrategySpec, ParameterSpec]:
        """
        Extract strategy and parameters using AI

        Args:
            source_code: Original scanner code
            ast_result: AST classification result

        Returns:
            Tuple of (StrategySpec, ParameterSpec)
        """
        try:
            # Extract strategy intent
            strategy = self.ai_agent.extract_strategy_intent(
                source_code, ast_result
            )

            # Extract parameters
            parameters = self.ai_agent.identify_parameters(
                source_code, strategy
            )

            # Update strategy with parameters
            strategy.parameters = parameters.to_dict()

            return strategy, parameters

        except AIExtractionError as e:
            raise TransformationError(f"AI extraction failed: {e}")

    def _select_template(
        self,
        ast_result: ClassificationResult,
        strategy: StrategySpec,
        source_code: str = None
    ) -> str:
        """
        Select appropriate template

        Priority:
        1. Backside Para B scanner ‚Üí v31_hybrid (specific transformation)
        2. Multi-scanner ‚Üí v31_hybrid_multi (multi-pattern transformation)
        3. Everything else ‚Üí v31_generic (preserves original structure)

        Args:
            ast_result: AST classification result
            strategy: Strategy specification
            source_code: Optional source code for better detection

        Returns:
            Template name or transformation mode
        """
        # Priority 1: Check if this is a Backside Para B scanner
        # Backside Para B scanners have specific patterns and need dedicated transformation
        if self._is_standalone_scanner(ast_result, source_code):
            # Use hybrid transformation to preserve all Backside Para B logic
            # while converting to v31 architecture
            return 'v31_hybrid'

        # Priority 2: Check if this is a multi-scanner (multiple pattern detectors)
        # Multi-scanners have:
        # - Multiple pattern column assignments (e.g., results['d2_pattern'] = ...)
        # - Multiple pattern check methods (e.g., check_d2, check_d3, check_d4)
        # - Pattern count >= 3 (from AST classification)
        if self._is_multi_scanner(ast_result, source_code):
            # Use hybrid transformation for multi-scanners to preserve all pattern logic
            # while converting to v31 architecture
            return 'v31_hybrid_multi'

        # Default: Use generic v31 transformation
        # This preserves original code structure while applying v31 standards
        return 'v31_generic'

    def _is_standalone_scanner(self, ast_result: ClassificationResult, source_code: str = None) -> bool:
        """
        Detect if source code is a Backside Para B-style standalone scanner

        Backside Para B scanners have SPECIFIC patterns:
        - fetch_daily function
        - add_daily_metrics function
        - scan_symbol function
        - _mold_on_row function
        - P dict with params (price_min, adv20_min_usd, etc.)
        - if __name__ == "__main__" execution block

        Args:
            ast_result: AST parsing result
            source_code: Optional source code for direct checks

        Returns:
            True if Backside Para B scanner detected
        """
        if not source_code:
            return False

        # Must have ALL Backside Para B specific patterns
        has_fetch_daily = 'def fetch_daily' in source_code
        has_add_metrics = 'def add_daily_metrics' in source_code
        has_scan_symbol = 'def scan_symbol' in source_code
        has_mold_on_row = 'def _mold_on_row' in source_code
        has_p_dict = 'P = {' in source_code or 'P={' in source_code
        has_main = 'if __name__ == "__main__"' in source_code

        # Only classify as Backside Para B if ALL patterns present
        backside_para_b_patterns = [
            has_fetch_daily,
            has_add_metrics,
            has_scan_symbol,
            has_mold_on_row,
            has_p_dict,
            has_main
        ]

        is_backside_para_b = all(backside_para_b_patterns)

        if is_backside_para_b:
            print("   üéØ Detected Backside Para B scanner structure")

        return is_backside_para_b

    def _is_multi_scanner(self, ast_result: ClassificationResult, source_code: str = None) -> bool:
        """
        Detect if source code is a multi-scanner (multiple pattern detectors)

        Multi-scanners have:
        - Multiple pattern column assignments (e.g., results['d2_pattern'] = ...)
        - Multiple pattern check methods (e.g., check_d2, check_d3, check_d4)
        - Pattern count >= 3 (from AST classification)
        - Pattern keywords: d2, d3, d4, lc_frontside, lc_backside, pm_setup, pmh_break, extreme

        Args:
            ast_result: AST parsing result
            source_code: Optional source code for direct checks

        Returns:
            True if multi-scanner detected
        """
        if not source_code:
            return False

        # Check indicators from classification result
        indicators = ast_result.indicators or {}
        pattern_count = indicators.get('pattern_count', 0)

        # Check if classified as multi-scanner by AST parser
        is_multi_type = ast_result.scanner_type.value == 'multi'

        # Multi-scanner detection: pattern_count >= 3 OR classified as multi
        if pattern_count >= 3 or is_multi_type:
            print(f"   üîç Multi-scanner detected (patterns: {pattern_count}, type: {ast_result.scanner_type.value})")
            return True

        return False

    def _is_trading_pattern(self, pattern_name: str) -> bool:
        """
        Determine if a pattern name represents an actual trading signal pattern.

        This filters out:
        - Date/time columns (date, time, timestamp)
        - Price calculation columns (_min_price, _max_price, _entry, _target)
        - Helper columns (indicator, score, rank, level)

        Trading patterns typically have names like:
        - lc_frontside_d2, lc_backside_d3, etc. (LC patterns)
        - d2_pattern, d3_setup, etc. (gap patterns)
        - pm_setup, pmh_break (pre-market patterns)

        Args:
            pattern_name: Name of the pattern/column

        Returns:
            True if this appears to be a trading signal pattern
        """
        # Skip date/time columns
        date_keywords = ['date', 'time', 'timestamp', 'datetime']
        if any(keyword in pattern_name.lower() for keyword in date_keywords):
            return False

        # Skip price calculation columns
        price_keywords = ['_min_price', '_max_price', '_entry', '_target', '_stop', 'price_level']
        if any(keyword in pattern_name.lower() for keyword in price_keywords):
            return False

        # Skip helper/ranking columns
        helper_keywords = ['_score', '_rank', '_indicator', '_level', '_index', '_filter']
        if any(keyword in pattern_name.lower() for keyword in helper_keywords):
            return False

        # Trading pattern keywords (patterns we WANT to extract)
        trading_keywords = [
            'lc_frontside', 'lc_backside',  # LC patterns
            'lc_d2', 'lc_d3', 'lc_d4',       # LC day patterns
            'pm_setup', 'pmh_break',          # Pre-market patterns
            'fbo',                            # First breakout
            'extreme',                        # Extreme moves
            'parabolic',                      # Parabolic moves
            'gap_and_go',                     # Gap patterns
            '_d2_', '_d3_', '_d4_',           # Day patterns (underscored)
        ]

        # Check if pattern name contains any trading keywords
        pattern_lower = pattern_name.lower()
        if any(keyword in pattern_lower for keyword in trading_keywords):
            return True

        # Default: skip unknown patterns
        return False

    def _convert_pattern_logic_for_eval(self, pattern_logic: str) -> str:
        """
        Convert pattern logic from df['column'] format to column-only format
        for compatibility with DataFrame.eval()

        This transforms:
        - df['h'] >= df['h1']  ‚Üí  h >= h1
        - df['column']          ‚Üí  column

        Args:
            pattern_logic: Raw pattern logic with df['...'] references

        Returns:
            Pattern logic with df['...'] converted to column names
        """
        import re

        # Replace df['column'] with just column
        # This regex matches df['...'] or df["..."]
        def replace_df_ref(match):
            column_name = match.group(1)
            return column_name

        # Replace all df['column'] or df["column"] references
        converted = re.sub(r"df\['([^']+)'\]", replace_df_ref, pattern_logic)
        converted = re.sub(r'df\["([^"]+)"\]', replace_df_ref, converted)

        return converted

    def _indent_for_class_method(self, code: str) -> str:
        """
        Indent code for insertion as a class method

        Also adds 'self' as the first parameter to function definitions.

        Args:
            code: Code to indent (typically a function definition)

        Returns:
            Code indented by 4 spaces for class method level with self parameter
        """
        if not code:
            return code

        lines = code.split('\n')
        indented_lines = []

        for i, line in enumerate(lines):
            # Check if this is a function definition line
            if i == 0 and 'def compute_indicators1(' in line:
                # Add 'self' as first parameter
                # Transform: def compute_indicators1(df): -> def compute_indicators1(self, df):
                line = line.replace('def compute_indicators1(df):', 'def compute_indicators1(self, df):')
                line = line.replace('def compute_indicators1 (df):', 'def compute_indicators1(self, df):')

            # Preserve existing relative indentation, add 4 spaces to all lines
            if line.strip():  # Non-empty line
                indented_lines.append('    ' + line)
            else:  # Empty line
                indented_lines.append('')

        return '\n'.join(indented_lines)

    def _fix_common_indicator_bugs(self, indicator_code: str) -> str:
        """
        Fix common bugs and errors in indicator computation code

        This method detects and fixes:
        1. Copy-paste errors (e.g., dol_v3 repeated instead of dol_v4)
        2. Missing column dependencies
        3. Common typos in variable names

        Args:
            indicator_code: Original indicator computation code

        Returns:
            Fixed indicator computation code
        """
        if not indicator_code:
            return indicator_code

        print(f"      üîç Scanning for common bugs...")

        fixed_code = indicator_code
        fixes_applied = []

        import re

        # Fix 1: dol_v_cum5_1 copy-paste error - MORE ROBUST PATTERN
        # Matches: df['dol_v_cum5_1'] = ... + df['dol_v3'] + df['dol_v3'] + ...
        # The key is finding dol_v3 repeated consecutively
        dol_v_patterns = [
            # Pattern 1: Exact match with minimal spaces
            r"dol_v_cum5_1.*?dol_v1.*?dol_v2.*?dol_v3.*?\+.*?dol_v3",
            # Pattern 2: More flexible pattern
            r"df\[['\"]dol_v_cum5_1['\"]\].*?df\[['\"]dol_v3['\"]\].*?\+.*?df\[['\"]dol_v3['\"]\]",
        ]

        for pattern in dol_v_patterns:
            if re.search(pattern, fixed_code, re.DOTALL):
                print(f"      üîß Found dol_v_cum5_1 bug pattern!")
                # Fix by replacing the second occurrence of dol_v3 with dol_v4
                # We need to find the specific pattern: dol_v3 + dol_v3
                fixed_code = re.sub(
                    r"(df\[['\"]dol_v3['\"]\])\s*\+\s*(df\[['\"]dol_v3['\"]\])",
                    r"\1 + df['dol_v4']",
                    fixed_code,
                    count=1
                )
                fixes_applied.append("Fixed dol_v_cum5_1 copy-paste error (dol_v3‚Üídol_v4)")
                break

        # Fix 2: Check for other repeated column patterns - MORE ROBUST
        # Pattern: col1 + col2 + col3 + col3 + col5 (missing col4)
        # Look for any column added to itself
        all_columns = re.findall(r"df\[['\"](\w+)['\"]\]", fixed_code)
        seen_pairs = set()

        for i in range(len(all_columns) - 1):
            col1 = all_columns[i]
            col2 = all_columns[i + 1]
            pair = (col1, col2)

            if pair in seen_pairs:
                # Found a repeated pattern
                # Check if it ends with a number that should be incremented
                num_match = re.search(r'(\D+)(\d+)$', col1)
                if num_match and col1 == col2:
                    base = num_match.group(1)
                    num = int(num_match.group(2))
                    next_col = f"{base}{num + 1}"

                    # Fix this specific occurrence
                    pattern_to_fix = rf"df\[['\"]{re.escape(col1)}['\"]\]\s*\+\s*df\[['\"]{re.escape(col2)}['\"]\]"
                    replacement = rf"df['{col1}'] + df['{next_col}']"

                    if pattern_to_fix in fixed_code:
                        fixed_code = re.sub(pattern_to_fix, replacement, fixed_code, count=1)
                        fixes_applied.append(f"Fixed repeated column pattern ({col1}‚Üí{next_col})")

            seen_pairs.add(pair)

        # Fix 3: Validate column creation order
        # Ensure c_ua, l_ua, v_ua are not used before being created
        # This is handled by the template structure, but we can add warnings
        if "c_ua" in fixed_code and "l_ua" in fixed_code:
            try:
                c_ua_pos = fixed_code.index("c_ua")
                l_ua_pos = fixed_code.index("l_ua")
                if c_ua_pos < l_ua_pos:
                    # c_ua is used before l_ua is created
                    if "df['l_ua'] = df['l']" not in fixed_code:
                        fixes_applied.append("Warning: c_ua used before l_ua initialization (handled by template)")
            except ValueError:
                pass  # One of the strings not found

        if fixes_applied:
            print(f"      ‚úÖ Applied {len(fixes_applied)} fixes:")
            for fix in fixes_applied:
                print(f"         ‚Ä¢ {fix}")
        else:
            print(f"      ‚ÑπÔ∏è  No common bugs found")

        return fixed_code

    def _remove_duplicate_column_creation(self, indicator_code: str) -> str:
        """
        Remove duplicate column creation from indicator function

        The v31 template creates c_ua, l_ua, v_ua columns BEFORE calling compute_indicators1.
        This method removes those lines from inside the function to avoid conflicts.

        Args:
            indicator_code: Original indicator computation code

        Returns:
            Indicator code with duplicate column creation removed
        """
        if not indicator_code:
            return indicator_code

        import re

        lines = indicator_code.split('\n')
        filtered_lines = []
        removed_count = 0

        # Patterns that indicate column creation we want to skip
        # Patterns that indicate column creation we want to skip
        skip_patterns = [
            "df['c_ua'] = df.groupby",
            "df['l_ua'] = df['l']",  # Simple assignment
            "df['v_ua'] = df['v']",  # Simple assignment
        ]

        for line in lines:
            # Check if this line creates one of the columns we handle externally
            should_skip = False
            for pattern in skip_patterns:
                if re.search(pattern, line):
                    should_skip = True
                    removed_count += 1
                    break

            if not should_skip:
                filtered_lines.append(line)

        if removed_count > 0:
            print(f"      ‚úÖ Removed {removed_count} duplicate column creation lines")

        return '\n'.join(filtered_lines)

    def _generate_pattern_logic(
        self,
        strategy: StrategySpec,
        parameters: ParameterSpec
    ) -> str:
        """
        Generate pattern detection logic

        Args:
            strategy: Strategy specification
            parameters: Parameter specification

        Returns:
            Generated Python code for detect_patterns
        """
        try:
            return self.ai_agent.generate_pattern_logic(
                strategy, parameters
            )

        except AIExtractionError as e:
            raise TransformationError(f"Pattern logic generation failed: {e}")

    def _generate_with_validation(
        self,
        source_code: str,
        scanner_name: Optional[str],
        strategy: StrategySpec,
        parameters: ParameterSpec,
        ast_result: ClassificationResult,
        pattern_detection_code: str,
        template_name: str,
        date_range: str,
        verbose: bool
    ) -> Tuple[str, List[ValidationResult], List[Dict[str, Any]]]:
        """
        Generate code with validation and self-correction loop

        Args:
            source_code: Original source code
            scanner_name: Name for the scanner
            strategy: Strategy specification
            parameters: Parameter specification
            ast_result: AST classification result
            pattern_detection_code: Generated pattern detection logic
            template_name: Template to use
            date_range: Date range for scanning
            verbose: Print progress

        Returns:
            Tuple of (generated_code, validation_results, corrections_list)
        """
        # Generate scanner name if not provided
        if not scanner_name:
            scanner_name = self._generate_scanner_name(strategy.name)

        corrections = []

        for attempt in range(self.max_correction_attempts):
            if attempt > 0:
                if verbose:
                    print(f"\n  [Correction Attempt {attempt + 1}/{self.max_correction_attempts}]")

            # Build template context
            context = self._build_context(
                scanner_name=scanner_name,
                strategy=strategy,
                parameters=parameters,
                ast_result=ast_result,
                pattern_detection_code=pattern_detection_code,
                date_range=date_range,
                attempt=attempt,
                previous_corrections=corrections
            )

            # Render template
            try:
                # Check if using minimal fix mode
                if template_name == 'minimal_fix':
                    generated_code = self._apply_minimal_fixes(source_code, ast_result)
                elif template_name == 'v31_hybrid':
                    # Convert Backside Para B scanner to v31 architecture
                    # while preserving all working logic
                    generated_code = self._apply_v31_hybrid_transform(
                        source_code, scanner_name, strategy, date_range
                    )
                elif template_name == 'v31_hybrid_multi':
                    # Convert multi-scanner to v31 architecture
                    # while preserving all pattern detection logic
                    generated_code = self._apply_v31_multi_transform(
                        source_code, scanner_name, strategy, date_range, ast_result
                    )
                elif template_name == 'v31_generic':
                    # Generic v31 transformation - preserves original code structure
                    # This is the DEFAULT transformation for most code
                    generated_code = self._apply_v31_generic_transform(
                        source_code, scanner_name, strategy, date_range
                    )
                else:
                    # Use template engine for .py.jinja2 templates
                    generated_code = self.template_engine.render(
                        template_name, context
                    )

                # CRITICAL: Strip any thinking/reasoning text before the code
                generated_code = self._strip_thinking_text(generated_code)

            except TemplateEngineError as e:
                raise TransformationError(f"Template rendering failed: {e}")

            # Validate generated code
            is_valid, validation_results = self.validator.validate_all(
                generated_code
            )

            if is_valid:
                # Success!
                return generated_code, validation_results, corrections

            # Validation failed - collect errors
            errors = []
            for result in validation_results:
                errors.extend(result.errors)

            if verbose:
                print(f"  ‚úó Validation failed with {len(errors)} error(s)")
                # üî• DEBUG: Print actual error messages
                for i, error in enumerate(errors, 1):
                    print(f"     Error {i}: {error}")

            # Try to correct errors
            correction = self._attempt_correction(
                generated_code, errors, validation_results, attempt
            )

            if correction:
                corrections.append(correction)

                # Apply correction to context or pattern_detection_code
                if correction['type'] == 'pattern_logic':
                    pattern_detection_code = correction['corrected_code']
                elif correction['type'] == 'context':
                    # Update context for next iteration
                    pass

            else:
                # Can't correct - break loop
                if verbose:
                    print(f"  ! Unable to auto-correct errors")
                break

        # Return last attempt even if invalid
        _, validation_results = self.validator.validate_all(generated_code)
        return generated_code, validation_results, corrections

    def _build_context(
        self,
        scanner_name: str,
        strategy: StrategySpec,
        parameters: ParameterSpec,
        ast_result: ClassificationResult,
        pattern_detection_code: str,
        date_range: str,
        attempt: int,
        previous_corrections: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Build template context

        Args:
            scanner_name: Name for the scanner
            strategy: Strategy specification
            parameters: Parameter specification
            ast_result: AST classification result
            pattern_detection_code: Pattern detection logic
            date_range: Date range
            attempt: Current attempt number
            previous_corrections: Previous corrections applied

        Returns:
            Template context dictionary
        """
        print(f"üîß _build_context called, parameters={parameters}, parameters type={type(parameters)}")

        # Extract filter values from parameters with defensive null checks
        # Handle case where parameters might be None or have None values
        try:
            if parameters is None:
                price_thresholds = {}
                volume_thresholds = {}
                gap_thresholds = {}
            else:
                # Use getattr with explicit None check and default empty dict
                price_thresholds_raw = getattr(parameters, 'price_thresholds', None) or {}
                volume_thresholds_raw = getattr(parameters, 'volume_thresholds', None) or {}
                gap_thresholds_raw = getattr(parameters, 'gap_thresholds', None) or {}

                # Filter out None values - replace with empty dicts
                price_thresholds = {k: v for k, v in price_thresholds_raw.items() if v is not None}
                volume_thresholds = {k: v for k, v in volume_thresholds_raw.items() if v is not None}
                gap_thresholds = {k: v for k, v in gap_thresholds_raw.items() if v is not None}
        except Exception as e:
            # If anything goes wrong, use empty dicts
            print(f"‚ö†Ô∏è  Warning extracting thresholds: {e}")
            price_thresholds = {}
            volume_thresholds = {}
            gap_thresholds = {}

        context = {
            'scanner_name': scanner_name,
            'strategy_name': strategy.name,
            'description': strategy.description,
            'date_range': date_range,
            'needs_polygon_api': ast_result.data_source.polygon_usage if hasattr(ast_result, 'data_source') else False,
            'scanner_type': ast_result.scanner_type.value,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),

            # Smart filters
            'smart_filters': {
                'min_prev_close': price_thresholds.get('min_price', {}).get('value', 0.75),
                'max_prev_close': price_thresholds.get('max_price', {}).get('value', 1000),
                'min_prev_volume': volume_thresholds.get('min_volume', {}).get('value', 500000),
                'max_prev_volume': volume_thresholds.get('max_volume', {}).get('value', 100000000)
            },

            # Performance settings
            'stage1_workers': 4,
            'stage3_workers': 4,

            # Pattern detection
            'pattern_detection_method': pattern_detection_code,

            # Entry conditions
            'entry_conditions': strategy.entry_conditions,

            # Pattern filters (for multi-scanners)
            'pattern_filters': {
                'min_price': price_thresholds.get('min_price', {}).get('value'),
                'max_price': price_thresholds.get('max_price', {}).get('value'),
                'min_volume': volume_thresholds.get('min_volume', {}).get('value'),
                'max_volume': volume_thresholds.get('max_volume', {}).get('value'),
                'min_gap_pct': gap_thresholds.get('min_gap_pct', {}).get('value'),
                'max_gap_pct': gap_thresholds.get('max_gap_pct', {}).get('value'),
                'min_gap_consecutive': gap_thresholds.get('min_gap_consecutive', {}).get('value')
            }
        }

        return context

    def _attempt_correction(
        self,
        code: str,
        errors: List[str],
        validation_results: List[ValidationResult],
        attempt: int
    ) -> Optional[Dict[str, Any]]:
        """
        Attempt to correct validation errors

        Args:
            code: Generated code with errors
            errors: List of error messages
            validation_results: Full validation results
            attempt: Current attempt number

        Returns:
            Correction dictionary or None if correction not possible
        """
        # Check for specific error types and attempt corrections

        # 1. Missing imports
        for result in validation_results:
            if result.category == 'syntax':
                for error in result.errors:
                    if 'Missing required imports' in error:
                        return {
                            'type': 'context',
                            'description': 'Add missing imports',
                            'error_type': 'missing_imports',
                            'attempt': attempt
                        }

        # 2. Missing methods
        for result in validation_results:
            if result.category == 'structure':
                for error in result.errors:
                    if 'Missing required methods' in error:
                        return {
                            'type': 'context',
                            'description': 'Add missing method stubs',
                            'error_type': 'missing_methods',
                            'attempt': attempt
                        }

        # 3. Syntax errors in pattern logic
        for result in validation_results:
            if result.category == 'syntax':
                for error in result.errors:
                    if 'Syntax error' in error and 'detect_patterns' in str(error):
                        # Try to fix by simplifying pattern logic
                        return {
                            'type': 'pattern_logic',
                            'description': 'Simplify pattern detection logic',
                            'error_type': 'pattern_syntax',
                            'corrected_code': '''
# Simplified pattern detection
for ticker, data in stage2_data.groupby('ticker'):
    # Placeholder for pattern detection
    results.append({
        'ticker': ticker,
        'date': data['date'].iloc[0] if not data.empty else None,
        'entry_price': data['open'].iloc[0] if not data.empty else None
    })
''',
                            'attempt': attempt
                        }

        # No automatic correction available
        return None

    def _generate_scanner_name(self, strategy_name: str) -> str:
        """
        Generate scanner name from strategy name

        Args:
            strategy_name: Strategy name

        Returns:
            Formatted scanner name
        """
        # Remove spaces and special characters, convert to PascalCase
        words = strategy_name.split()
        return ''.join(word.capitalize() for word in words) + "Scanner"

    def _apply_minimal_fixes(
        self,
        source_code: str,
        ast_result: ClassificationResult
    ) -> str:
        """
        Apply minimal EdgeDev v31 compliance fixes while preserving original code

        This mode ONLY fixes:
        1. fetch_all_grouped_data ‚Üí fetch_grouped_data
        2. Removes $ from column names (ADV20_$ ‚Üí adv20_usd)
        3. Ensures proper method signatures

        Keeps ALL original logic intact.

        Args:
            source_code: Original scanner code
            ast_result: AST classification result

        Returns:
            Fixed code with minimal changes
        """
        fixed_code = source_code

        # Fix 1: Replace fetch_all_grouped_data with fetch_grouped_data
        fixed_code = fixed_code.replace(
            'def fetch_all_grouped_data(',
            'def fetch_grouped_data('
        )
        fixed_code = fixed_code.replace(
            '.fetch_all_grouped_data(',
            '.fetch_grouped_data('
        )

        # Fix 2: Replace ADV20_$ and similar column names with valid Python identifiers
        import re

        # Pattern: COLUMN_NAME_$ -> COLUMN_NAME_usd
        # This handles: ADV20_$, ADV50_$, etc.
        fixed_code = re.sub(
            r'(\w+)\_\$',
            r'\1_usd',
            fixed_code
        )

        # Fix 3: Remove $ from any remaining identifiers
        fixed_code = re.sub(
            r'\$',
            '_usd',
            fixed_code
        )

        # Fix 4: Ensure proper imports if missing
        if 'import pandas' not in fixed_code:
            fixed_code = 'import pandas as pd\n' + fixed_code

        return fixed_code

    def _apply_enhanced_parameter_detection(self, code: str) -> Dict[str, Any]:
        """
        üîé ENHANCED PARAMETER DETECTION - Recognize ALL scanner parameter patterns

        Detects parameters from 6 major patterns found in production scanner code:
        1. P = {...} dictionary
        2. self.params = {...} class attributes
        3. defaults = {...} in functions
        4. Direct variable assignments
        5. Hardcoded comparison values
        6. Custom params overrides

        Returns:
            Dictionary of all detected parameters with metadata
        """
        print("üîé ENHANCED PARAMETER DETECTION starting...")

        detected_params = {}

        # Pattern 1: P = {...} dictionary
        print("  üîç Pattern 1: P = {...} dictionary")
        p_dict_pattern = r'P\s*=\s*\{([^}]+)\}'
        p_matches = re.finditer(p_dict_pattern, code, re.DOTALL)
        for match in p_matches:
            dict_content = match.group(1)
            params = self._extract_dict_params(dict_content)
            detected_params.update(params)
            print(f"    ‚úÖ Found {len(params)} params in P dict")

        # Pattern 2: self.params = {...}, self.config = {...}, etc.
        print("  üîç Pattern 2: self.<attr> = {...}")
        self_dict_pattern = r'self\.(\w+)\s*=\s*\{([^}]+)\}'
        self_matches = re.finditer(self_dict_pattern, code, re.DOTALL)
        for match in self_matches:
            attr_name = match.group(1)
            dict_content = match.group(2)
            params = self._extract_dict_params(dict_content)
            detected_params.update(params)
            print(f"    ‚úÖ Found {len(params)} params in self.{attr_name}")

        # Pattern 3: defaults = {...} in functions
        print("  üîç Pattern 3: defaults = {...}")
        defaults_pattern = r'(\w+)\s*=\s*\{([^}]+)\}.*?(?=\n\s{0,4}[^\s\s])'
        defaults_matches = re.finditer(defaults_pattern, code, re.DOTALL)
        for match in defaults_matches:
            var_name = match.group(1)
            # Only process if it looks like a parameter dict
            if var_name in ['defaults', 'params', 'parameters', 'config', 'settings']:
                dict_content = match.group(2)
                params = self._extract_dict_params(dict_content)
                detected_params.update(params)
                print(f"    ‚úÖ Found {len(params)} params in {var_name}")

        # Pattern 4: Direct variable assignments (atr_mult = 0.9, vol_mult = 2.0)
        print("  üîç Pattern 4: Direct variable assignments")
        param_names = [
            'atr_mult', 'vol_mult', 'slope5d_min', 'slope3d_min', 'slope15d_min', 'slope50d_min',
            'high_ema9_mult', 'high_ema20_mult', 'gap_div_atr_min', 'open_over_ema9_min',
            'd1_green_atr_min', 'require_open_gt_prev_high', 'enforce_d1_above_d2',
            'abs_lookback_days', 'abs_exclude_days', 'pos_abs_max', 'atr_pct_change_min',
            'prev_close_min', 'price_min', 'adv20_min_usd', 'd1_volume_min', 'd1_vol_mult_min',
            'pct7d_low_div_atr_min', 'pct14d_low_div_atr_min', 'pct2d_div_atr_min', 'pct3d_div_atr_min',
            'trigger_mode', 'min_price', 'min_volume', 'min_daily_value', 'min_adv'
        ]

        for param_name in param_names:
            # Match: param_name = value
            pattern = rf'{param_name}\s*=\s*([\d\.]+|["\']([^"\']+)["\']|True|False|None)'
            matches = re.finditer(pattern, code)
            for match in matches:
                value_str = match.group(1)
                value = self._parse_value(value_str)
                if param_name not in detected_params:
                    detected_params[param_name] = value
                    print(f"    ‚úÖ Found {param_name} = {value}")

        # Pattern 5: Hardcoded comparison values (df['gap_atr'] >= 0.5)
        print("  üîç Pattern 5: Hardcoded comparison values")
        comparison_patterns = {
            'gap_div_atr_min': [r"Gap_over_ATR\]\s*>=\s*([\d\.]+)", r"gap_atr\]\s*>=\s*([\d\.]+)"],
            'high_ema9_mult': [r"High_over_EMA9.*ATR\]\s*>=\s*([\d\.]+)"],
            'atr_mult': [r"TR\]\s*/\s*ATR\]\s*>=\s*([\d\.]+)"],
            'vol_mult': [r"Volume\]\s*/\s*VOL_AVG\]\s*>=\s*([\d\.]+)"],
            'slope5d_min': [r"Slope_9_5d\]\s*>=\s*([\d\.]+)"],
        }

        for param_name, patterns in comparison_patterns.items():
            if param_name in detected_params:
                continue  # Skip if already found
            for pattern in patterns:
                matches = re.finditer(pattern, code)
                for match in matches:
                    value = self._parse_value(match.group(1))
                    detected_params[param_name] = value
                    print(f"    ‚úÖ Found {param_name} = {value} (comparison)")
                    break

        # Pattern 6: Custom params overrides
        print("  üîç Pattern 6: Custom params overrides")
        custom_pattern = r'custom_params\s*=\s*\{([^}]+)\}'
        custom_matches = re.finditer(custom_pattern, code, re.DOTALL)
        for match in custom_matches:
            dict_content = match.group(1)
            params = self._extract_dict_params(dict_content)
            detected_params.update(params)
            print(f"    ‚úÖ Found {len(params)} params in custom_params")

        print(f"üìä TOTAL PARAMETERS DETECTED: {len(detected_params)}")
        return detected_params

    def _extract_dict_params(self, dict_content: str) -> Dict[str, Any]:
        """
        Extract parameters from dictionary content string

        Args:
            dict_content: Content between { and } in a dictionary

        Returns:
            Dictionary of parameter names and values
        """
        params = {}

        # Match key-value pairs: 'key': value or "key": value
        kv_pattern = r'["\']([^"\']+)["\']\s*:\s*([^,\n]+(?:\s*#[^\n]*)?)'

        for match in re.finditer(kv_pattern, dict_content):
            param_name = match.group(1)
            value_str = match.group(2).strip()

            # Remove inline comments
            if '#' in value_str:
                value_str = value_str.split('#')[0].strip()

            # Parse the value
            value = self._parse_value(value_str)
            params[param_name] = value

        return params

    def _parse_value(self, value_str: str) -> Any:
        """
        Parse value string to Python type

        Args:
            value_str: String representation of value

        Returns:
            Parsed value (int, float, bool, str, or None)
        """
        value_str = value_str.strip()

        # Boolean
        if value_str in ['True', 'False']:
            return value_str == 'True'

        # None
        if value_str in ['None', 'null']:
            return None

        # String (already quoted)
        if (value_str.startswith('"') and value_str.endswith('"')) or \
           (value_str.startswith("'") and value_str.endswith("'")):
            return value_str[1:-1]

        # Numeric with underscores (30_000_000)
        value_str = value_str.replace('_', '')

        # Integer
        if re.match(r'^-?\d+$', value_str):
            return int(value_str)

        # Float
        if re.match(r'^-?\d+\.\d+$', value_str):
            return float(value_str)

        # Default: return as string
        return value_str

    def _apply_critical_bug_fix_v30(self, code: str) -> str:
        """
        üõ°Ô∏è CRITICAL BUG FIX v30: Fix require_open_gt_prev_high to check D-2's high, not D-1's high

        ‚ùå WRONG: if require_open_gt_prev_high and not (r0["Open"] > r1["High"]):
        ‚úÖ CORRECT: if require_open_gt_prev_high and not (r0['open'] > r1['Prev_High']):

        This fix ensures the check uses D-2's high (Prev_High) instead of D-1's high (High).
        """
        print("üõ°Ô∏è BUG FIX v30: Checking for require_open_gt_prev_high bug...")

        # Pattern 1: r0["Open"] > r1["High"] (wrong - checks D-1 high)
        pattern1_wrong = r'r0\["Open"\] > r1\["High"\]'
        pattern1_correct = r'r0["Open"] > r1["Prev_High"]'

        # Pattern 2: r0['Open'] > r1['High'] (wrong - checks D-1 high)
        pattern2_wrong = r"r0\['Open'\] > r1\['High'\]"
        pattern2_correct = r"r0['Open'] > r1['Prev_High']"

        # Pattern 3: r0["Open"] > r1.High (wrong - checks D-1 high)
        pattern3_wrong = r'r0\["Open"\] > r1\.High'
        pattern3_correct = r'r0["Open"] > r1.Prev_High'

        # Pattern 4: r0['Open'] > r1.High (wrong - checks D-1 high)
        pattern4_wrong = r"r0\['Open'\] > r1\.High"
        pattern4_correct = r"r0['Open'] > r1.Prev_High"

        # Pattern 5: r0["open"] > r1["high"] (wrong - checks D-1 high)
        pattern5_wrong = r'r0\["open"\] > r1\["high"\]'
        pattern5_correct = r'r0["open"] > r1["prev_high"]'

        # Pattern 6: r0['open'] > r1['high'] (wrong - checks D-1 high)
        pattern6_wrong = r"r0\['open'\] > r1\['high'\]"
        pattern6_correct = r"r0['open'] > r1['prev_high']"

        # Pattern 7: r0.open > r1.high (wrong - checks D-1 high)
        pattern7_wrong = r'r0\.open > r1\.high'
        pattern7_correct = r'r0.open > r1.prev_high'

        # Track if any fixes were applied
        fixes_applied = 0

        # Apply all pattern fixes
        code, count1 = re.subn(pattern1_wrong, pattern1_correct, code)
        fixes_applied += count1

        code, count2 = re.subn(pattern2_wrong, pattern2_correct, code)
        fixes_applied += count2

        code, count3 = re.subn(pattern3_wrong, pattern3_correct, code)
        fixes_applied += count3

        code, count4 = re.subn(pattern4_wrong, pattern4_correct, code)
        fixes_applied += count4

        code, count5 = re.subn(pattern5_wrong, pattern5_correct, code)
        fixes_applied += count5

        code, count6 = re.subn(pattern6_wrong, pattern6_correct, code)
        fixes_applied += count6

        code, count7 = re.subn(pattern7_wrong, pattern7_correct, code)
        fixes_applied += count7

        if fixes_applied > 0:
            print(f"‚úÖ BUG FIX v30: Applied {fixes_applied} fix(es) - Now checks D-2's high (Prev_High)")
        else:
            print("‚úÖ BUG FIX v30: No bugs found - code already correct")

        return code

    def _apply_grouped_api_transformation(self, code: str) -> str:
        """
        üöÄ PHASE 1: Replace individual ticker fetching with grouped API

        Replaces fetch_daily() or fetch_daily_multi_range() function with grouped API implementation.
        Reduces API calls from 31,800+ to ~238 (99.3% reduction).
        """
        import re

        # Check for various fetch function patterns
        fetch_patterns = [
            'def fetch_daily(',
            'def fetch_daily_multi_range(',
            'def fetch_data(',
            'def get_daily_data(',
            'def fetch('
        ]

        # Find which fetch function exists
        found_function = None
        for pattern in fetch_patterns:
            if pattern in code:
                found_function = pattern.replace('def ', '').replace('(', '')
                print(f"  üîÑ Found {found_function}(), replacing with grouped API implementation...")
                break

        if not found_function:
            print("  ‚ÑπÔ∏è  No fetch function found, skipping grouped API transformation")
            return code

        # Extract the existing fetch function
        fetch_function_pattern = rf'def {re.escape(found_function)}\([^)]+\)[^:]+:\s*"""?(.*?)"""?'

        # Build the new grouped API fetch function
        grouped_api_function = '''def fetch_grouped_daily(date: str) -> pd.DataFrame:
    """üöÄ Fetch ALL stocks data in ONE API call using grouped endpoint"""
    url = f"{BASE_URL}/v2/aggs/grouped/locale/us/market/stocks/{date}"
    params = {
        "apiKey": os.getenv('POLYGON_API_KEY', API_KEY),
        "adjusted": "true",
        "include_otc": "false"
    }

    r = session.get(url, params=params)
    r.raise_for_status()
    data = r.json()

    if "results" not in data or not data["results"]:
        print("‚ùå No data returned from grouped API")
        return pd.DataFrame()

    # Convert grouped API response to DataFrame
    all_data = []
    for result in data["results"]:
        ticker = result.get("T")
        if ticker not in SYMBOLS:
            continue

        all_data.append({
            "ticker": ticker,
            "t": result["t"],
            "o": result["o"],
            "h": result["h"],
            "l": result["l"],
            "c": result["c"],
            "v": result["v"],
            "vw": result.get("vw", 0),
            "n": result.get("n", 1)
        })

    if not all_data:
        print(f"‚ö†Ô∏è  No data found for target symbols")
        return pd.DataFrame()

    df = pd.DataFrame(all_data)
    print(f"‚úÖ Got {len(df)} records for {len(df['ticker'].unique())} symbols")

    return (df
            .assign(Date=lambda d: pd.to_datetime(d["t"], unit="ms", utc=True))
            .rename(columns={"o":"Open","h":"High","l":"Low","c":"Close","v":"Volume"})
            .set_index("Date")[["Open","High","Low","Close","Volume","ticker"]]
            .sort_index())

def fetch_daily_multi_range(start: str, end: str) -> pd.DataFrame:
    """üìÖ Fetch data for date range using grouped API calls (optimized for 10-day groups)"""
    from datetime import timedelta
    import time

    print(f"üìÖ Fetching multi-date range: {start} to {end}")

    start_date = pd.to_datetime(start)
    end_date = pd.to_datetime(end)

    all_data = []
    current_date = start_date

    # üî• OPTIMIZED: Fetch in 10-day groups for API efficiency
    while current_date <= end_date:
        # Calculate group end date (10 days or until end_date, whichever is earlier)
        group_end_date = min(current_date + timedelta(days=9), end_date)

        # Only process if the group starts on a weekday
        if current_date.weekday() < 5:
            group_start_str = current_date.strftime("%Y-%m-%d")
            group_end_str = group_end_date.strftime("%Y-%m-%d")

            try:
                print(f"üåê Fetching grouped data: {group_start_str} to {group_end_str}")
                df_group = fetch_grouped_daily(group_start_str)

                if not df_group.empty:
                    # Filter to only include dates within our range
                    df_group['Date'] = pd.to_datetime(df_group['Date'])
                    df_group = df_group[
                        (df_group['Date'] >= pd.Timestamp(current_date)) &
                        (df_group['Date'] <= pd.Timestamp(group_end_date))
                    ]
                    if not df_group.empty:
                        all_data.append(df_group)

            except Exception as e:
                print(f"‚ö†Ô∏è  Error fetching {group_start_str} to {group_end_str}: {e}")

        # Move to next 10-day group
        current_date = group_end_date + timedelta(days=1)

    if all_data:
        result = pd.concat(all_data)
        print(f"‚úÖ Total {len(result)} records across {len(all_data)} date groups")
        return result
    else:
        return pd.DataFrame()
'''

        # Replace the old fetch function with grouped API version
        print(f"  üîç Debug: Looking for function '{found_function}' in code...")

        # Try multiple patterns to find and replace the function
        # CRITICAL: Patterns ordered from most specific to most permissive
        # Each pattern must match ONLY the intended function, not multiple functions
        patterns_to_try = [
            # Pattern 1: Match until box-drawing comment line (most specific)
            # Structure: \n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  (blank line + comment + newline)
            rf'def {re.escape(found_function)}\([^)]+\).*?(?=\n\n# ‚îÄ+.*?\n(?:def|class)|\n    \n(?:def|class)|\n(?:def|class)|\Z)',
            # Pattern 2: Match until next 'def' with blank line and indent
            rf'def {re.escape(found_function)}\([^)]+\).*?(?=\n    \n    def |\n    \n    class |\Z)',
            # Pattern 3: Match until next 'def' with ONE blank line (4-space indent)
            rf'def {re.escape(found_function)}\([^)]+\).*?(?=\n    def |\n    class |\Z)',
            # Pattern 4: Match until next 'def' with any indentation
            rf'def {re.escape(found_function)}\([^)]+\).*?(?=\n\s+def |\n\s+class |\Z)',
            # Pattern 5: Last resort - match entire function to end
            rf'def {re.escape(found_function)}\([^)]+\).*?(?=\ndef |class |\Z)'
        ]

        replaced = False
        for i, pattern in enumerate(patterns_to_try, 1):
            print(f"  üîç Trying pattern {i}...")
            match = re.search(pattern, code, flags=re.DOTALL)
            if match:
                matched_text = match.group(0)
                print(f"  ‚úÖ Pattern {i} matched: {len(matched_text)} characters")
                print(f"     Match preview: {matched_text[:100]}...")

                # Check if we're actually matching the function body
                if 'return' in matched_text or 'pd.DataFrame' in matched_text:
                    code = re.sub(pattern, grouped_api_function, code, flags=re.DOTALL, count=1)
                    print(f"  ‚úÖ Replaced function with grouped API version")
                    replaced = True
                    break
                else:
                    print(f"  ‚ö†Ô∏è  Match doesn't contain function body, trying next pattern...")
            else:
                print(f"  ‚ö†Ô∏è  Pattern {i} didn't match")

        if not replaced:
            print(f"  ‚ö†Ô∏è  All patterns failed! Manual inspection needed.")
            print(f"     Available functions in code:")
            for func_match in re.finditer(r'def ([a-z_]+)\(', code):
                print(f"       - {func_match.group(1)}()")

        # Now replace any calls to the old function with the new grouped API wrapper
        # Handle calls like: fetch_daily(tkr, start, end) or fetch_daily_multi_range(start, end)
        if found_function == 'fetch_daily':
            # Old signature: fetch_daily(tkr, start, end)
            code = re.sub(
                rf'{re.escape(found_function)}\(([^,]+),\s*([^,]+),\s*([^)]+)\)',
                r'fetch_daily_multi_range(\2, \3)',
                code
            )
        else:
            # New signature: fetch_daily_multi_range(start, end) - already using range format
            # Just replace function name with grouped version
            code = re.sub(
                rf'{re.escape(found_function)}\(',
                'fetch_daily_multi_range(',
                code
            )

        print("  ‚úÖ Grouped API transformation applied")
        return code

    def _apply_dynamic_market_universe_transformation(self, code: str) -> str:
        """
        üåç PHASE 2: Replace hardcoded SYMBOLS list with dynamic universe builder

        Fetches market universe dynamically from Polygon API instead of hardcoded list.
        """
        import re

        # Check if SYMBOLS list exists
        if 'SYMBOLS = [' not in code and 'SYMBOLS = ["' not in code:
            print("  ‚ÑπÔ∏è  No SYMBOLS list found, skipping market universe transformation")
            return code

        print("  üîÑ Replacing hardcoded SYMBOLS with dynamic universe builder...")

        # Build the dynamic universe builder function
        universe_builder_function = '''def build_market_universe(max_tickers: int = 1000) -> list:
    """
    üåç Build dynamic market universe from Polygon API

    Fetches all tradeable stocks and filters by market cap, volume, and price.
    Returns list of ticker symbols.
    """
    try:
        url = f"{BASE_URL}/v3/reference/tickers"
        params = {
            "market": "stocks",
            "active": "true",
            "limit": 1000,
            "apiKey": os.getenv('POLYGON_API_KEY', API_KEY)
        }

        # Get first page
        all_tickers = []
        page = 1

        while len(all_tickers) < max_tickers and page <= 10:
            params['page'] = page
            r = session.get(url, params=params, timeout=30)
            r.raise_for_status()
            data = r.json()

            if 'results' not in data or not data['results']:
                break

            # Filter and add tickers
            for ticker in data['results']:
                if len(all_tickers) >= max_tickers:
                    break

                # Skip ETFs and warrants
                t_type = ticker.get('type', '').upper()
                if t_type in ['ETF', 'WARRANT', 'PREFERRED', 'AD']:
                    continue

                # Check market cap if available (min $50M)
                market_cap = ticker.get('market_cap', None)
                if market_cap is not None and market_cap < 50_000_000:
                    continue

                # Check price if available (min $1)
                price = ticker.get('price', None)
                if price is not None and price < 1.0:
                    continue

                all_tickers.append(ticker['ticker'])

            page += 1
            time.sleep(0.1)  # Rate limiting

        print(f"‚úÖ Built dynamic universe: {len(all_tickers)} tickers")
        return all_tickers

    except Exception as e:
        print(f"‚ö†Ô∏è  Error building dynamic universe: {e}")
        print("  üîÑ Using fallback universe...")
        return ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'JPM']

# Build universe on module load
SYMBOLS = build_market_universe()
'''

        # Replace SYMBOLS = [...] with dynamic builder
        # Pattern 1: SYMBOLS = ['item1', 'item2', ...]
        code = re.sub(
            r"SYMBOLS\s*=\s*\[.*?\](?=\n\n|\n# |$)",
            universe_builder_function,
            code,
            flags=re.DOTALL
        )

        print("  ‚úÖ Dynamic market universe transformation applied")
        return code

    def _apply_smart_filtering_transformation(self, code: str) -> str:
        """
        üß† PHASE 3: Enhance apply_smart_filters() with real filtering logic

        Replaces placeholder pass-through with actual multi-stage filtering.
        """
        import re

        # Check if apply_smart_filters method exists in the code
        # (it might be in the class wrapper we generate, so we'll handle that separately)
        # For now, we'll add a helper function that can be used

        print("  üîÑ Adding smart filtering helper functions...")

        # Build smart filtering helper function
        smart_filter_helper = '''
def apply_smart_filters_to_dataframe(df: pd.DataFrame, params: dict) -> pd.DataFrame:
    """
    üß† Apply smart filters to reduce dataset by 99%

    Multi-stage filtering: price, volume, market cap, volatility
    """
    if df.empty:
        return df

    print(f"  üìä Before smart filters: {len(df)} rows")

    # Stage 1: Price filtering
    if 'price_min' in params:
        min_price = params['price_min']
        df = df[df['Open'] >= min_price]
        df = df[df['Close'] >= min_price]

    # Stage 2: Volume/ADV filtering
    if 'adv20_min_usd' in params:
        min_adv = params['adv20_min_usd']
        df = df[df['Close'] * df['Volume'] >= min_adv]

    if 'd1_volume_min' in params:
        min_vol = params['d1_volume_min']
        df = df[df['Volume'] >= min_vol]

    # Stage 3: Volatility filtering (ATR-based)
    if 'atr_mult' in params and 'ATR' in df.columns:
        # Filter out low volatility stocks
        pass  # Placeholder for advanced filtering

    print(f"  üìä After smart filters: {len(df)} rows ({len(df)/df.shape[0]*100:.1f}% retained)")

    return df
'''

        # Insert the helper function before the class wrapper
        # Find the class definition line
        class_match = re.search(r'\nclass \w+:', code)
        if class_match:
            insert_pos = class_match.start()
            code = code[:insert_pos] + '\n' + smart_filter_helper + '\n' + code[insert_pos:]
            print("  ‚úÖ Smart filtering helper functions added")
        else:
            print("  ‚ö†Ô∏è  Could not find insertion point for smart filters")

        return code

    def _extract_detection_logic(self, source_code: str) -> dict:
        """
        Extract ALL detection logic and dependencies from source code

        Returns:
            dict: {
                'scan_symbol_body': str,
                'helpers': dict with all helper functions,
                'add_daily_metrics': str,
                'fetch_daily': str
            }
        """
        import re

        result = {
            'scan_symbol_body': None,
            'helpers': {},
            'add_daily_metrics': None,
            'fetch_daily': None
        }

        # Extract scan_symbol function body (everything after the colon until next def/class/main/async)
        # Updated to match actual file structure with various comment patterns
        scan_match = re.search(
            r'def\s+scan_symbol\s*\([^)]*\)\s*(?:->\s*[^:]+)?\s*:\s*\n(.*?)(?=\n\s{0,4}(?:def\s|async def\s|class\s|if __name__|#\s*‚îÅ|#\s*‚îÄ))',
            source_code,
            re.DOTALL
        )

        if scan_match:
            # Get the body and dedent it properly
            body = scan_match.group(1)
            # Remove common leading whitespace
            lines = body.split('\n')
            if lines:
                # Find minimum indentation (excluding empty lines)
                min_indent = float('inf')
                for line in lines:
                    if line.strip():
                        indent = len(line) - len(line.lstrip())
                        min_indent = min(min_indent, indent)
                # Dedent
                dedented_lines = [line[min_indent:] if line.strip() else line for line in lines]
                result['scan_symbol_body'] = '\n'.join(dedented_lines).strip()

        # Extract ALL helper functions needed by scan_symbol
        # Extract with proper indentation preservation
        helper_names = ['abs_top_window', 'pos_between', '_mold_on_row']
        for helper_name in helper_names:
            # Match the entire function including body, preserving structure
            helper_match = re.search(
                rf'(def\s+{re.escape(helper_name)}\s*\([^)]*\)(?:\s*->[^:]+)?\s*:\s*\n.*?)(?=\n\s{{0,4}}(?:def |async def |class |if __name__|#\s*‚îÅ|$))',
                source_code,
                re.DOTALL
            )
            if helper_match:
                result['helpers'][helper_name] = helper_match.group(1)

        # Extract add_daily_metrics function
        metrics_match = re.search(
            r'(def\s+add_daily_metrics\s*\([^)]*\)(?:\s*->[^:]+)?\s*:\s*\n.*?)(?=\n\s{{0,4}}(?:def |async def |class |if __name__|#\s*‚îÅ|$))',
            source_code,
            re.DOTALL
        )
        if metrics_match:
            result['add_daily_metrics'] = metrics_match.group(1)

        # Extract fetch_daily function
        fetch_match = re.search(
            r'(def\s+fetch_daily\s*\([^)]*\)(?:\s*->[^:]+)?\s*:\s*\n.*?)(?=\n\s{{0,4}}(?:def |async def |class |if __name__|#\s*‚îÅ|$))',
            source_code,
            re.DOTALL
        )
        if fetch_match:
            result['fetch_daily'] = fetch_match.group(1)

        return result

    def _extract_detection_loop_only(self, source_code: str) -> tuple:
        """
        Extract ONLY the detection loop BODY from scan_symbol (not the for statement itself)

        This extracts the CONTENTS INSIDE the for loop that does the actual pattern detection.
        This is CRITICAL - the template already has a for loop, so we only need the body.

        Also extracts and replaces the scanner function parameter with 'ticker'.

        Returns:
            tuple: (detection_loop_body_code, original_param_name) or (None, None)
        """
        import re

        # Find the scan_symbol function signature to get parameter name
        sig_match = re.search(
            r'def\s+scan_symbol\s*\(([^)]*)\)',
            source_code
        )

        original_param = 'sym'  # Default fallback
        if sig_match:
            params = sig_match.group(1).strip()
            # Get first parameter (the ticker symbol)
            if params:
                first_param = params.split(',')[0].strip()
                if first_param:
                    # Extract just the parameter name (remove type annotation if present)
                    # Handle: 'sym', 'sym: str', 'sym: str = None', etc.
                    param_name_match = re.match(r'(\w+)', first_param)
                    if param_name_match:
                        original_param = param_name_match.group(1)
                        print(f"   üéØ Detected scanner parameter: '{original_param}'")

        # Find the scan_symbol function
        scan_match = re.search(
            r'def\s+scan_symbol\s*\([^)]*\)\s*(?:->\s*[^:]+)?\s*:\s*\n(.*?)(?=\n\s{0,4}(?:def\s|async def\s|class\s|if __name__|#\s*‚îÅ|#\s*‚îÄ))',
            source_code,
            re.DOTALL
        )

        if not scan_match:
            return None, None

        body = scan_match.group(1)

        # Find the detection loop BODY (content INSIDE the for loop, not the for statement)
        # Try multiple patterns to handle different scanner styles
        loop_body_match = None

        # Pattern 1: Match for loop and extract its body content
        # This pattern captures the content AFTER "for i in range(...):" 
        # We want the body content only, not the for statement
        for_loop_pattern = r'for\s+i\s+in\s+range\(.*?\):\s*\n((?:.*?\n)*?)(?=\n\s{0,4}return)'
        loop_body_match = re.search(for_loop_pattern, body, re.DOTALL)

        # Pattern 2: Any for loop - extract the body after the colon
        if not loop_body_match:
            for_loop_pattern = r'for\s+\w+\s+in\s+.*?:\s*\n((?:.*?\n)*?)(?=\s{0,4}return)'
            loop_body_match = re.search(for_loop_pattern, body, re.DOTALL)

        # Pattern 3: Look specifically for rows.append to find loop body
        if not loop_body_match:
            for_loop_pattern = r'(?:for\s+\w+\s+in\s+range\(.*?\):|for\s+\w+\s+in\s+.*?):\s*\n((?:.*?\n)*?rows\.append.*?)(?=\n\s{0,4}return)'
            loop_body_match = re.search(for_loop_pattern, body, re.DOTALL)

        if loop_body_match:
            # Get the loop BODY (content inside the for loop)
            loop_body = loop_body_match.group(1)

            # Now we need to remove lines that duplicate what the template already has
            # The template sets up: d0, r0, r1, r2 before calling detection_loop_only
            # So we should skip those lines if they're at the start

            # Split into lines
            lines = loop_body.split('\n')

            # Find the first line that's NOT just variable assignment from m.iloc
            # The template already has:
            #   d0 = m.index[i]
            #   r0 = m.iloc[i]
            #   r1 = m.iloc[i-1]
            #   r2 = m.iloc[i-2]
            # So we should skip lines like:
            #   d0 = m.index[i]
            #   r0 = m.iloc[i]  (or any r0/r1/r2 assignments)

            filtered_lines = []
            skip_assignments = True

            for line in lines:
                stripped = line.strip()

                # Skip variable assignments that template already handles
                if skip_assignments and (
                    stripped.startswith('d0 =') or
                    stripped.startswith('r0 =') or
                    stripped.startswith('r1 =') or
                    stripped.startswith('r2 =')
                ):
                    continue

                # Once we hit a non-assignment line, stop skipping
                if stripped and not stripped.startswith('#'):
                    skip_assignments = False

                filtered_lines.append(line)

            loop_body = '\n'.join(filtered_lines)

            # üî• CRITICAL FIX 1: Replace the original parameter with 'ticker'
            # This fixes the bug where scan_symbol(sym) uses 'sym' but the class method loop uses 'ticker'
            if original_param and original_param != 'ticker':
                # Replace all occurrences of the parameter in string contexts
                # Be careful not to replace variable names that contain the parameter as substring
                pattern = r'\b' + re.escape(original_param) + r'\b'
                loop_body = re.sub(pattern, 'ticker', loop_body)
                print(f"   ‚úÖ Replaced '{original_param}' with 'ticker' in detection loop body")

            # üî• CRITICAL FIX 2: Replace 'rows' with 'all_rows'
            # The generated method uses 'all_rows' but original scanners often use 'rows'
            loop_body = re.sub(r'\brows\s*=', 'all_rows =', loop_body)
            loop_body = re.sub(r'\brows\s*\.append', 'all_rows.append', loop_body)
            print(f"   ‚úÖ Replaced 'rows' with 'all_rows' in detection loop body")

            # üî• CRITICAL FIX 3: Replace 'P[' with 'P_local['
            # The template uses 'P_local' dict but original scanners use 'P'
            loop_body = re.sub(r'\bP\[', 'P_local[', loop_body)
            print(f"   ‚úÖ Replaced 'P[' with 'P_local[' in detection loop body")

            # üî• CRITICAL FIX 4: Update _mold_on_row calls to pass P_local
            # The helper function needs params passed, not accessed from global P
            # Replace: _mold_on_row(r1) -> _mold_on_row(r1, P_local)
            # Also handles r0, r2, r3, etc.
            loop_body = re.sub(
                r'_mold_on_row\((r\d+)\)',
                r'_mold_on_row(\1, P_local)',
                loop_body
            )
            if '_mold_on_row' in loop_body:
                print(f"   ‚úÖ Updated _mold_on_row calls to pass P_local")

            # üî• CRITICAL FIX 12: Replace 'm' with 'ticker_df' in detection loop
            # The original code used 'm' (from add_daily_metrics) but v31 uses 'ticker_df'
            # This fixes calls like abs_top_window(m, d0, ...) -> abs_top_window(ticker_df, d0, ...)
            # Also handles m.index, m.iloc, m.loc, m[...], m.column_name
            loop_body = re.sub(r'\bm\s*,\s*', 'ticker_df, ', loop_body)  # Function calls: m,
            loop_body = re.sub(r'\bm\.index\[', 'ticker_df.index[', loop_body)  # m.index[i]
            loop_body = re.sub(r'\bm\.iloc\[', 'ticker_df.iloc[', loop_body)  # m.iloc[i]
            loop_body = re.sub(r'\bm\.loc\[', 'ticker_df.loc[', loop_body)  # m.loc[i]
            loop_body = re.sub(r"bm\['", 'ticker_df["', loop_body)  # m['col']
            loop_body = re.sub(r'\bm\["', 'ticker_df["', loop_body)  # m["col"]
            if 'm' in loop_body and 'ticker_df' not in loop_body:
                # If there are still standalone 'm' references, replace them
                loop_body = re.sub(r'\bm\b', 'ticker_df', loop_body)
            if 'ticker_df' in loop_body:
                print(f"   ‚úÖ Replaced 'm' with 'ticker_df' in detection loop")

            # üî• CRITICAL FIX 14: Replace r1["high"] with r1["prev_high"] in require_open_gt_prev_high check
            # The original check was "open > r1['high']" but in v31, 'prev_high' is the D-2 high (shifted by 1)
            # The working scanner explicitly uses r1['prev_high'] (D-2's high), NOT r1['high'] (D-1's high)
            if 'require_open_gt_prev_high' in loop_body:
                # Pattern: r0["open"] > r1["high"] should be r0["open"] > r1["prev_high"]
                # Use lookahead/lookbehind to preserve the exact formatting
                loop_body = re.sub(
                    r'(r\d+\["open"\]\s*>\s*)r\d+\["high"\](\s*\))',
                    r'\1r1["prev_high"]\2',
                    loop_body
                )
                print(f"   ‚úÖ Fixed require_open_gt_prev_high to use r1['prev_high'] (D-2's high)")

            # Dedent the loop body
            lines = loop_body.split('\n')
            if lines:
                # Find minimum indentation (excluding empty lines)
                min_indent = float('inf')
                for line in lines:
                    if line.strip():
                        indent = len(line) - len(line.lstrip())
                        min_indent = min(min_indent, indent)
                # Dedent
                dedented_lines = [line[min_indent:] if line.strip() else line for line in lines]
                return '\n'.join(dedented_lines).strip(), original_param

        return None, None

    def _post_process_scan_symbol_integration(self, v31_code: str, original_param: str = 'sym') -> str:
        """
        Post-process generated v31 code to properly integrate scan_symbol function

        This method:
        1. Extracts the scan_symbol function if it exists
        2. Removes it from the code
        3. Integrates the detection logic into _run_pattern_detection
        4. Replaces parameter variables (original_param ‚Üí ticker)

        Args:
            v31_code: Generated v31 code
            original_param: Original parameter name (default 'sym')

        Returns:
            Post-processed v31 code
        """
        import re

        print("üîß POST-PROCESSING: Integrating scan_symbol function...")

        # Try to extract the scan_symbol function
        scan_match = re.search(
            r'def\s+scan_symbol\s*\(([^)]*)\)\s*(?:->\s*[^:]+)?\s*:\s*\n(.*?)(?=\n(?:def\s|class\s|#\s*‚îÅ))',
            v31_code,
            re.DOTALL
        )

        if not scan_match:
            print("   ‚ÑπÔ∏è  No standalone scan_symbol function found (already integrated)")
            return v31_code

        # Extract parameter name if not provided
        if not original_param or original_param == 'sym':
            params = scan_match.group(1).strip()
            if params:
                first_param = params.split(',')[0].strip()
                if first_param:
                    # Extract just the parameter name (remove type annotation)
                    param_name_match = re.match(r'(\w+)', first_param)
                    if param_name_match:
                        original_param = param_name_match.group(1)
                        print(f"   üéØ Extracted parameter name: '{original_param}'")

        scan_body = scan_match.group(2)
        print(f"   ‚úÖ Found scan_symbol function: {len(scan_body)} chars")

        # Extract the detection loop from scan_symbol body
        # Look for the for loop
        loop_match = re.search(
            r'(for\s+\w+\s+in\s+.*?:.*?)(?=\n\s{0,4}return)',
            scan_body,
            re.DOTALL
        )

        if not loop_match:
            print("   ‚ö†Ô∏è  Could not extract detection loop from scan_symbol")
            return v31_code

        detection_loop = loop_match.group(1)
        print(f"   ‚úÖ Extracted detection loop: {len(detection_loop)} chars")

        # üî• CRITICAL: Replace original parameter with 'ticker'
        if original_param and original_param != 'ticker':
            pattern = r'\b' + re.escape(original_param) + r'\b'
            detection_loop = re.sub(pattern, 'ticker', detection_loop)
            print(f"   ‚úÖ Replaced '{original_param}' with 'ticker' in detection loop")

        # üî• CRITICAL: Replace 'rows' with 'all_rows'
        detection_loop = re.sub(r'\brows\s*=', 'all_rows =', detection_loop)
        detection_loop = re.sub(r'\brows\s*\.append', 'all_rows.append', detection_loop)
        print(f"   ‚úÖ Replaced 'rows' with 'all_rows' in detection loop")

        # üî• CRITICAL FIX 12b: Replace 'm' with 'ticker_df' in detection loop (post-processing path)
        # This is for when scan_symbol is preserved and processed through post-processing
        detection_loop = re.sub(r'\bm\s*,\s*', 'ticker_df, ', detection_loop)  # Function calls: m,
        detection_loop = re.sub(r'\bm\.index\[', 'ticker_df.index[', detection_loop)  # m.index[i]
        detection_loop = re.sub(r'\bm\.iloc\[', 'ticker_df.iloc[', detection_loop)  # m.iloc[i]
        detection_loop = re.sub(r'\bm\.loc\[', 'ticker_df.loc[', detection_loop)  # m.loc[i]
        detection_loop = re.sub(r"bm\['", 'ticker_df["', detection_loop)  # m['col']
        detection_loop = re.sub(r'\bm\["', 'ticker_df["', detection_loop)  # m["col"]
        if 'm' in detection_loop and 'ticker_df' not in detection_loop:
            detection_loop = re.sub(r'\bm\b', 'ticker_df', detection_loop)
        if 'ticker_df' in detection_loop:
            print(f"   ‚úÖ Replaced 'm' with 'ticker_df' in detection loop (post-process)")

        # Find the _run_pattern_detection method and replace the placeholder
        # Look for the comment about embedding detection logic
        # More flexible pattern to handle different docstring formats
        placeholder_pattern = r'(def _run_pattern_detection\(self, data\):.*?""".*?The detection loop logic was \d+ characters)'

        placeholder_match = re.search(placeholder_pattern, v31_code, re.DOTALL)

        if not placeholder_match:
            # Try alternative pattern - just find the method signature and docstring
            placeholder_pattern_alt = r'(def _run_pattern_detection\(self, data\):.*?""".*?""")'
            placeholder_match = re.search(placeholder_pattern_alt, v31_code, re.DOTALL)

        # üî• CRITICAL FIX 7: Update ALL _mold_on_row calls in original code to pass P
        # This MUST happen before placeholder check to ensure it always runs
        # Fixes calls in scan_symbol function and any other places
        # Replace: _mold_on_row(r1) -> _mold_on_row(r1, P)
        # Replace: _mold_on_row(r2) -> _mold_on_row(r2, P)
        v31_code = re.sub(
            r'_mold_on_row\((r\d+)\)(?=[,\)])',
            r'_mold_on_row(\1, P)',
            v31_code
        )
        if '_mold_on_row(' in v31_code:
            print(f"   ‚úÖ Updated ALL _mold_on_row calls to pass P parameter")

        # Remove the standalone scan_symbol function
        # Match various dash characters in comments
        v31_code = re.sub(
            r'\ndef\s+scan_symbol\s*\([^)]*\)\s*(?:->\s*[^:]+)?\s*:\s*\n(.*?)(?=\n(?:def\s|class\s|#\s*[\‚îÄ\-\‚Äî]))',
            '',
            v31_code,
            flags=re.DOTALL
        )
        if 'def scan_symbol(' not in v31_code:
            print(f"   ‚úÖ Removed standalone scan_symbol function")

        if not placeholder_match:
            print("   ‚ö†Ô∏è  Could not find _run_pattern_detection placeholder")
            print("   ‚ÑπÔ∏è  Detection loop preserved in scan_symbol function")
            return v31_code

        placeholder = placeholder_match.group(1)
        print(f"   ‚úÖ Found _run_pattern_detection placeholder: {len(placeholder)} chars")

        # Build the replacement - embed the detection loop
        # We need to find where to insert the loop (after the all_rows = [] line)
        # First, update the character count in the docstring
        replacement = re.sub(
            r'The detection loop logic was \d+ characters',
            f'The detection loop logic was {len(detection_loop)} characters',
            placeholder
        )

        # Find the insertion point (after all_rows = [])
        # The actual code has 12 spaces before all_rows
        insertion_marker = '            all_rows = []\n\n            # Process each ticker'
        if insertion_marker in replacement:
            # Dedent the detection loop and insert it
            lines = detection_loop.split('\n')
            # Find minimum indentation
            min_indent = float('inf')
            for line in lines:
                if line.strip():
                    indent = len(line) - len(line.lstrip())
                    min_indent = min(min_indent, indent)

            # Dedent and add proper indentation for method body (12 spaces)
            dedented_lines = []
            for line in lines:
                if line.strip():
                    dedented_lines.append('            ' + line[min_indent:])
                else:
                    dedented_lines.append(line)

            dedented_loop = '\n'.join(dedented_lines)

            # Insert the loop
            replacement = replacement.replace(
                insertion_marker,
                f'all_rows = []\n\n{dedented_loop}\n\n            # Process each ticker'
            )

            # Replace the placeholder with the new version
            v31_code = v31_code[:placeholder_match.start()] + replacement + v31_code[placeholder_match.end():]
            print(f"   ‚úÖ Embedded detection loop into _run_pattern_detection")

        print("   ‚úÖ POST-PROCESSING COMPLETE")
        return v31_code

    def _apply_v31_hybrid_transform(
        self,
        source_code: str,
        scanner_name: str,
        strategy: StrategySpec,
        date_range: str
    ) -> str:
        """
        Convert standalone scanner to TRUE v31 architecture

        Implements all 7 core pillars from the gold standard:
        1. Market calendar (pandas_market_calendars)
        2. Historical buffer calculation
        3. Per-ticker operations (groupby().transform())
        4. Historical/D0 separation
        5. Parallel processing (ThreadPoolExecutor)
        6. Two-pass feature computation
        7. Pre-sliced data for parallel processing

        Args:
            source_code: Original standalone scanner code
            scanner_name: Name for the v31 class
            strategy: Strategy specification
            date_range: Date range for scanning

        Returns:
            v31-structured scanner code with all original logic preserved
        """
        import re

        # Parse date range
        if ' to ' in date_range:
            d0_start, d0_end = date_range.split(' to ')
        else:
            d0_start = '2024-01-01'
            d0_end = '2024-12-31'

        # Remove the original if __name__ == "__main__" block
        lines = source_code.split('\n')
        filtered_lines = []
        in_main_block = False

        for i, line in enumerate(lines):
            if 'if __name__ == "__main__"' in line:
                in_main_block = True
                continue

            if in_main_block:
                line_indent = len(line) - len(line.lstrip())
                if line_indent == 0 and line.strip() and not re.match(r'^\s*#', line):
                    in_main_block = False
                else:
                    continue

            filtered_lines.append(line)

        original_code_without_main = '\n'.join(filtered_lines)

        # Add missing imports
        print("üîß ADDING MISSING IMPORTS...")

        if 'import pandas_market_calendars' not in original_code_without_main:
            import_match = re.search(r'^import ', original_code_without_main, re.MULTILINE)
            if import_match:
                insert_pos = import_match.start()
                original_code_without_main = original_code_without_main[:insert_pos] + 'import pandas_market_calendars as mcal\n' + original_code_without_main[insert_pos:]
                print("   ‚úÖ Added 'import pandas_market_calendars as mcal'")
            else:
                original_code_without_main = 'import pandas_market_calendars as mcal\n' + original_code_without_main
                print("   ‚úÖ Added 'import pandas_market_calendars as mcal' at beginning")

        if 'from concurrent.futures' not in original_code_without_main:
            import_match = re.search(r'^import ', original_code_without_main, re.MULTILINE)
            if import_match:
                insert_pos = import_match.start()
                original_code_without_main = original_code_without_main[:insert_pos] + 'from concurrent.futures import ThreadPoolExecutor, as_completed\n' + original_code_without_main[insert_pos:]
                print("   ‚úÖ Added 'from concurrent.futures import ThreadPoolExecutor, as_completed'")
            else:
                original_code_without_main = 'from concurrent.futures import ThreadPoolExecutor, as_completed\n' + original_code_without_main
                print("   ‚úÖ Added 'from concurrent.futures import ThreadPoolExecutor, as_completed' at beginning")

        # üî• CRITICAL FIX 5: Update _mold_on_row helper function to accept params
        # The function currently uses global P, but we need it to accept params as argument
        # üî• CRITICAL FIX 13: Also convert column names to snake_case (matching v31 architecture)
        print("üîß UPDATING HELPER FUNCTION: _mold_on_row...")
        old_mold_pattern = r'def _mold_on_row\(rx: pd\.Series\) -> bool:\s*\n(.*?)(?=\ndef |\nclass |\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ)'
        new_mold_func = '''def _mold_on_row(rx: pd.Series, params: dict) -> bool:
    """
    Check if a row meets the trigger mold criteria

    Args:
        rx: Row to check
        params: Parameter dictionary (P dict)

    Returns:
        True if row meets all criteria
    """
    if pd.isna(rx.get("prev_close")) or pd.isna(rx.get("adv20_$")):
        return False
    if rx["prev_close"] < params["price_min"] or rx["adv20_$"] < params["adv20_min_usd"]:
        return False
    vol_avg = rx["vol_avg"]
    if pd.isna(vol_avg) or vol_avg <= 0: return False
    vol_sig = max(rx["volume"]/vol_avg, rx["prev_volume"]/vol_avg)
    checks = [
        (rx["tr"] / rx["atr"]) >= params["atr_mult"],
        vol_sig                 >= params["vol_mult"],
        rx["slope_9_5d"]        >= params["slope5d_min"],
        rx["high_over_ema9_div_atr"] >= params["high_ema9_mult"],
    ]
    return all(bool(x) and np.isfinite(x) for x in checks)'''

        mold_match = re.search(old_mold_pattern, original_code_without_main, re.DOTALL)
        if mold_match:
            original_code_without_main = original_code_without_main[:mold_match.start()] + new_mold_func + original_code_without_main[mold_match.end():]
            print("   ‚úÖ Updated _mold_on_row to accept params argument and use snake_case")
        else:
            print("   ‚ö†Ô∏è  Could not find _mold_on_row function to update")

        # üî• CRITICAL FIX 13b: Update abs_top_window function to use snake_case and df['date']
        # The original uses df.index and Capitalized column names, but v31 uses df['date'] and snake_case
        print("üîß UPDATING HELPER FUNCTION: abs_top_window...")
        old_abs_pattern = r'def abs_top_window\(df: pd\.DataFrame, d0: pd\.Timestamp, lookback_days: int, exclude_days: int\):.*?(?=\ndef |\nclass |\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ)'
        new_abs_func = '''def abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_days: int):
    """Calculate absolute top window (v31 version uses df['date'] and snake_case)"""
    if df.empty: return (np.nan, np.nan)
    cutoff = d0 - pd.Timedelta(days=exclude_days)
    wstart = cutoff - pd.Timedelta(days=lookback_days)
    win = df[(df['date'] > wstart) & (df['date'] <= cutoff)]
    if win.empty: return (np.nan, np.nan)
    return float(win['low'].min()), float(win['high'].max())'''

        abs_match = re.search(old_abs_pattern, original_code_without_main, re.DOTALL)
        if abs_match:
            original_code_without_main = original_code_without_main[:abs_match.start()] + new_abs_func + original_code_without_main[abs_match.end():]
            print("   ‚úÖ Updated abs_top_window to use df['date'] and snake_case")
        else:
            print("   ‚ö†Ô∏è  Could not find abs_top_window function to update")

        # Extract detection logic
        print("üéØ EXTRACTING DETECTION LOGIC...")
        extracted = self._extract_detection_logic(original_code_without_main)

        detection_loop_result = self._extract_detection_loop_only(original_code_without_main)
        detection_loop_only = None
        original_param = None

        if detection_loop_result and detection_loop_result[0]:
            detection_loop_only, original_param = detection_loop_result
            print(f"   ‚úÖ Extracted detection loop only: {len(detection_loop_only)} chars")
            print(f"   üéØ Original parameter was: '{original_param}'")
        else:
            print("   ‚ö†Ô∏è  WARNING: detection loop not found!")

        # Indent detection loop for template
        if detection_loop_only:
            lines = detection_loop_only.split('\n')
            min_indent = min(len(line) - len(line.lstrip()) for line in lines if line.strip())
            dedented_lines = [line[min_indent:] if line.strip() else '' for line in lines]
            detection_loop_only = '\n'.join(['                    ' + line if line.strip() else line for line in dedented_lines])

            # ‚úÖ FIX #11: Convert column names from Capitalized to snake_case
            # This matches the v31 architecture which uses snake_case throughout
            column_mappings = [
                (r'\bOpen\b', 'open'),
                (r'\bHigh\b', 'high'),
                (r'\bLow\b', 'low'),
                (r'\bClose\b', 'close'),
                (r'\bVolume\b', 'volume'),
                (r'\bEMA_9\b', 'ema_9'),
                (r'\bEMA_20\b', 'ema_20'),
                (r'\bATR\b', 'atr'),
                (r'\bTR\b', 'tr'),
                (r'\bVOL_AVG\b', 'vol_avg'),
                (r'\bPrev_Volume\b', 'prev_volume'),
                (r'ADV20_\$', 'adv20_$'),  # Fixed: No word boundary after $
                (r'\bSlope_9_5d\b', 'slope_9_5d'),
                (r'\bHigh_over_EMA9_div_ATR\b', 'high_over_ema9_div_atr'),
                (r'\bGap_abs\b', 'gap_abs'),
                (r'\bGap_over_ATR\b', 'gap_over_atr'),
                (r'\bOpen_over_EMA9\b', 'open_over_ema9'),
                (r'\bBody_over_ATR\b', 'body_over_atr'),
                (r'\bPrev_Close\b', 'prev_close'),
                (r'\bPrev_Open\b', 'prev_open'),
                (r'\bPrev_High\b', 'prev_high'),
            ]

            for old, new in column_mappings:
                detection_loop_only = re.sub(old, new, detection_loop_only)

            print("   ‚úÖ Converted detection loop column names to snake_case")

            # üî• CRITICAL FIX 14b: Replace r1["high"] with r1["prev_high"] AFTER column name conversion
            # This must run AFTER the column name conversion because it operates on snake_case names
            # The check should compare D0 open with D-2 high (prev_high), not D-1 high (high)
            if 'require_open_gt_prev_high' in detection_loop_only:
                # Pattern: r0["open"] > r1["high"] should become r0["open"] > r1["prev_high"]
                # This handles both lowercase (after conversion) and the context of the require_open_gt_prev_high check
                detection_loop_only = re.sub(
                    r'(r\d+\["open"\]\s*>\s*)r\d+\["high"\](\s*\))',
                    r'\1r1["prev_high"]\2',
                    detection_loop_only
                )
                print("   ‚úÖ Fixed require_open_gt_prev_high to use r1['prev_high'] (D-2's high)")

        # Extract parameters FIRST (needed for template)
        print("üîé DETECTING PARAMETERS...")
        detected_params = self._apply_enhanced_parameter_detection(original_code_without_main)
        print(f"   üìä Detected {len(detected_params)} total parameters")

        # Get lookback value for template
        abs_lookback_days = detected_params.get('abs_lookback_days', 1000)
        lookback_buffer = abs_lookback_days + 50

        # Build TRUE v31 class following all 7 pillars
        v31_code = f'''"""
{scanner_name}

{strategy.description}

Generated by RENATA_V2 - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

TRUE v31 Architecture - All 7 Core Pillars Implemented:
1. ‚úÖ Market calendar (pandas_market_calendars)
2. ‚úÖ Historical buffer calculation
3. ‚úÖ Per-ticker operations (groupby().transform())
4. ‚úÖ Historical/D0 separation in smart filters
5. ‚úÖ Parallel processing (ThreadPoolExecutor)
6. ‚úÖ Two-pass feature computation
7. ‚úÖ Pre-sliced data for parallel processing
"""

{original_code_without_main}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TRUE v31 Scanner Class ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class {scanner_name}:
    """
    {strategy.name}

    {strategy.description}

    Scanner Type: Standalone Scanner converted to TRUE v31 Architecture
    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

    TRUE v31 Architecture - All 7 Core Pillars Implemented
    """

    def __init__(self, api_key: str, d0_start: str, d0_end: str):
        """
        Initialize scanner with proper v31 configuration

        ‚úÖ PILLAR 2: Calculate historical buffer for ABS window
        """
        self.api_key = api_key
        self.base_url = "https://api.polygon.io"

        # ‚úÖ CRITICAL: Calculate historical buffer
        # Lookback = abs_lookback_days + 50 for safety margin
        lookback_buffer = {lookback_buffer}  # From detected params

        # Calculate scan_start with historical buffer
        scan_start_dt = pd.to_datetime(d0_start) - pd.Timedelta(days=lookback_buffer)
        self.scan_start = scan_start_dt.strftime('%Y-%m-%d')
        self.d0_start_user = d0_start
        self.d0_end_user = d0_end

        # ‚úÖ Workers for parallel processing
        self.stage1_workers = 5
        self.stage3_workers = 10

        # ‚úÖ Session pooling for API efficiency
        import requests
        self.session = requests.Session()
        from requests.adapters import HTTPAdapter
        self.session.mount('https://', HTTPAdapter(
            pool_connections=100,
            pool_maxsize=100
        ))

        # ‚úÖ Parameters
        self.params = self._extract_parameters()

        # ‚úÖ Scanner name for logging
        self.scanner_name = self.__class__.__name__

        print(f"üìä Scanner initialized: {{self.scanner_name}}")
        print(f"   Historical buffer: {{lookback_buffer}} days")
        print(f"   Scan range: {{self.scan_start}} to {{self.d0_end_user}}")
        print(f"   D0 output range: {{self.d0_start_user}} to {{self.d0_end_user}}")

    def _extract_parameters(self) -> dict:
        """Extract parameters from P dict or return defaults"""
        # Try to extract from global scope
        try:
            import sys
            frame = sys._getframe(1)
            if 'P' in frame.f_locals:
                return frame.f_locals['P']
        except:
            pass

        # Default parameters with common backside B values
        # ‚úÖ CRITICAL FIX: vol_mult changed from 2.0 to 0.9 to match working scanner
        # ‚úÖ CRITICAL FIX: Added trigger_mode and d1_vol_mult_min (were missing)
        return {{
            "price_min": 8.0,
            "adv20_min_usd": 30_000_000,
            "abs_lookback_days": {abs_lookback_days},
            "abs_exclude_days": 10,
            "pos_abs_max": 0.75,
            "atr_mult": 0.9,
            "vol_mult": 0.9,  # ‚úÖ FIXED: Changed from 2.0 to 0.9
            "slope5d_min": 3.0,
            "high_ema9_mult": 1.05,
            "gap_div_atr_min": 0.75,
            "open_over_ema9_min": 0.9,
            "d1_green_atr_min": 0.30,
            "require_open_gt_prev_high": True,
            "enforce_d1_above_d2": True,
            "d1_volume_min": 15_000_000,
            "trigger_mode": "D1_or_D2",  # ‚úÖ FIXED: Added missing parameter
            "d1_vol_mult_min": None,  # ‚úÖ FIXED: Added missing parameter
        }}

    def run_scan(self):
        """
        üöÄ Main execution: 5-stage v31 pipeline

        Returns:
            List of signal dictionaries
        """
        print(f"\\n{'='*70}")
        print(f"üöÄ RUNNING TRUE v31 SCAN: {{self.scanner_name}}")
        print(f"{'='*70}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data()
        if stage1_data is None or stage1_data.empty:
            print("\\n‚ùå Scan failed: No data loaded")
            return []

        # Stage 2a: Compute simple features
        stage2a_data = self.compute_simple_features(stage1_data)

        # Stage 2b: Apply smart filters (with historical/D0 separation)
        stage2b_data = self.apply_smart_filters(stage2a_data)

        # Stage 3a: Compute full features
        stage3a_data = self.compute_full_features(stage2b_data)

        # Stage 3b: Detect patterns (with pre-sliced parallel processing)
        stage3_results = self.detect_patterns(stage3a_data)

        print(f"\\n‚úÖ SCAN COMPLETE: {{len(stage3_results)}} signals detected")
        return stage3_results

    def fetch_grouped_data(self):
        """
        ‚úÖ PILLAR 1: Market calendar integration
        ‚úÖ PILLAR 5: Parallel processing

        Stage 1: Fetch ALL tickers for ALL dates using grouped endpoint
        """
        import pandas_market_calendars as mcal
        from concurrent.futures import ThreadPoolExecutor, as_completed

        # ‚úÖ PILLAR 1: Use market calendar (NOT weekday checks)
        nyse = mcal.get_calendar('NYSE')
        trading_dates = nyse.schedule(
            start_date=self.scan_start,
            end_date=self.d0_end_user
        ).index.strftime('%Y-%m-%d').tolist()

        print(f"  üìÖ Fetching {{len(trading_dates)}} trading days from {{self.scan_start}} to {{self.d0_end_user}}")
        print(f"  üåê Using grouped endpoint (1 call per day)")
        print(f"  ‚öôÔ∏è  Parallel workers: {{self.stage1_workers}}")

        all_data = []

        # ‚úÖ PILLAR 5: Parallel fetching
        with ThreadPoolExecutor(max_workers=self.stage1_workers) as executor:
            future_to_date = {{
                executor.submit(self._fetch_grouped_day, date_str): date_str
                for date_str in trading_dates
            }}

            for future in as_completed(future_to_date):
                date_str = future_to_date[future]
                try:
                    data = future.result()
                    if data is not None and not data.empty:
                        all_data.append(data)
                        print(f"    ‚úÖ {{date_str}}: {{len(data)}} records")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  {{date_str}}: {{e}}")

        if all_data:
            result = pd.concat(all_data, ignore_index=True)
            print(f"  ‚úÖ Stage 1 complete: {{len(result)}} total records")
            return result
        else:
            print(f"  ‚ö†Ô∏è  Stage 1: No data retrieved")
            return pd.DataFrame()

    def _fetch_grouped_day(self, date_str: str):
        """
        Fetch ALL tickers for ONE day using grouped endpoint
        """
        url = f"{{self.base_url}}/v2/aggs/grouped/locale/us/market/stocks/{{date_str}}"
        response = self.session.get(url, params={{'apiKey': self.api_key, 'adjust': 'true'}})

        if response.status_code != 200:
            print(f"    ‚ö†Ô∏è  API error {{response.status_code}} for {{date_str}}")
            return None

        data = response.json()
        if "results" not in data or not data["results"]:
            return None

        # Convert to DataFrame
        all_data = []
        for result in data["results"]:
            all_data.append({{
                "ticker": result.get("T"),
                "date": date_str,
                "open": result.get("o"),
                "high": result.get("h"),
                "low": result.get("l"),
                "close": result.get("c"),
                "volume": result.get("v"),
            }})

        return pd.DataFrame(all_data)

    def compute_simple_features(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 3: Per-ticker operations
        ‚úÖ PILLAR 6: Two-pass feature computation (simple first)

        Stage 2a: Compute SIMPLE features for efficient filtering

        Only computes features needed for filtering:
        - prev_close
        - adv20_usd (with per-ticker groupby)
        - price_range
        """
        if df.empty:
            return df

        print(f"  üìä Stage 2a: Computing simple features for {{len(df)}} rows")

        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'])

        # Previous close
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)

        # ‚úÖ PILLAR 3: Per-ticker operations for ADV20
        df['adv20_usd'] = (df['close'] * df['volume']).groupby(df['ticker']).transform(
            lambda x: x.rolling(window=20, min_periods=20).mean()
        )

        # Price range
        df['price_range'] = df['high'] - df['low']

        print(f"    ‚úÖ Simple features computed")
        return df

    def apply_smart_filters(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 4: Separate historical from D0 data

        Stage 2b: Smart filters with HISTORICAL DATA PRESERVATION

        CRITICAL: Only filter D0 output range, preserve all historical data
        for ABS window calculations.
        """
        if df.empty:
            return df

        print(f"  üìä Stage 2b input: {{len(df)}} rows")

        # ‚úÖ PILLAR 4: Split historical from D0
        df_historical = df[~df['date'].between(__SELF__.d0_start_user, __SELF__.d0_end_user)].copy()
        df_output_range = df[df['date'].between(__SELF__.d0_start_user, __SELF__.d0_end_user)].copy()

        print(f"    üìä Historical: {{len(df_historical)}} rows")
        print(f"    üìä D0 range: {{len(df_output_range)}} rows")

        # ‚úÖ CRITICAL: Filter ONLY D0 range
        df_output_filtered = df_output_range.copy()

        # Price filter
        if 'price_min' in self.params:
            min_price = self.params['price_min']
            df_output_filtered = df_output_filtered[
                (df_output_filtered['close'] >= min_price) &
                (df_output_filtered['open'] >= min_price)
            ]

        # Volume filter
        if 'adv20_min_usd' in self.params:
            min_adv = self.params['adv20_min_usd']
            df_output_filtered = df_output_filtered[
                df_output_filtered['adv20_usd'] >= min_adv
            ]

        print(f"    üìä After filters: {{len(df_output_filtered)}} rows")

        # ‚úÖ CRITICAL: COMBINE historical + filtered D0
        df_combined = pd.concat([df_historical, df_output_filtered], ignore_index=True)

        print(f"  ‚úÖ Stage 2b complete: {{len(df_combined)}} rows (historical preserved)")
        return df_combined

    def compute_full_features(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 3: Per-ticker operations
        ‚úÖ PILLAR 6: Two-pass feature computation (full features after filter)

        Stage 3a: Compute ALL technical indicators

        Computes expensive features only on data that passed filters.
        """
        if df.empty:
            return df

        print(f"  üìä Stage 3a: Computing full features for {{len(df)}} rows")

        result_dfs = []

        for ticker, group in df.groupby('ticker'):
            group = group.sort_values('date').copy()

            # EMA
            group['ema_9'] = group['close'].ewm(span=9, adjust=False).mean()
            group['ema_20'] = group['close'].ewm(span=20, adjust=False).mean()

            # ATR
            hi_lo = group['high'] - group['low']
            hi_prev = (group['high'] - group['close'].shift(1)).abs()
            lo_prev = (group['low'] - group['close'].shift(1)).abs()
            group['tr'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
            group['atr_raw'] = group['tr'].rolling(14, min_periods=14).mean()
            group['atr'] = group['atr_raw'].shift(1)

            # ‚úÖ FIX #9: Add missing volume metrics for _mold_on_row
            group['vol_avg'] = group['volume'].rolling(14, min_periods=14).mean().shift(1)
            group['prev_volume'] = group['volume'].shift(1)
            group['adv20_$'] = (group['close'] * group['volume']).rolling(20, min_periods=20).mean().shift(1)

            # ‚úÖ FIX #9: Add slope calculation
            group['slope_9_5d'] = (group['ema_9'] - group['ema_9'].shift(5)) / group['ema_9'].shift(5) * 100

            # ‚úÖ FIX #9: Add high over EMA9 div ATR (needed by _mold_on_row)
            group['high_over_ema9_div_atr'] = (group['high'] - group['ema_9']) / group['atr']

            # ‚úÖ FIX #9: Add gap metrics
            group['gap_abs'] = (group['open'] - group['close'].shift(1)).abs()
            group['gap_over_atr'] = group['gap_abs'] / group['atr']
            group['open_over_ema9'] = group['open'] / group['ema_9']

            # ‚úÖ FIX #9: Add body over ATR
            group['body_over_atr'] = (group['close'] - group['open']) / group['atr']

            # ‚úÖ FIX #9: Add previous values
            group['prev_close'] = group['close'].shift(1)
            group['prev_open'] = group['open'].shift(1)
            group['prev_high'] = group['high'].shift(1)

            result_dfs.append(group)

        result = pd.concat(result_dfs, ignore_index=True)
        print(f"    ‚úÖ Full features computed")
        return result

    def detect_patterns(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 7: Pre-sliced data for parallel processing
        ‚úÖ PILLAR 5: Parallel ticker processing

        Stage 3b: Pattern detection with parallel processing
        """
        if df.empty:
            return []

        print(f"  üéØ Stage 3b: Detecting patterns in {{len(df)}} rows")

        # Get D0 range
        d0_start_dt = pd.to_datetime(__SELF__.d0_start_user)
        d0_end_dt = pd.to_datetime(__SELF__.d0_end_user)

        # ‚úÖ PILLAR 7: Pre-slice ticker data BEFORE parallel processing
        ticker_data_list = []
        for ticker, ticker_df in df.groupby('ticker'):
            ticker_data_list.append((ticker, ticker_df.copy(), d0_start_dt, d0_end_dt))

        all_results = []

        # ‚úÖ PILLAR 5: Parallel processing with pre-sliced data
        from concurrent.futures import ThreadPoolExecutor, as_completed

        with ThreadPoolExecutor(max_workers=self.stage3_workers) as executor:
            future_to_ticker = {{
                executor.submit(self._process_ticker_optimized_pre_sliced, ticker_data): ticker_data[0]
                for ticker_data in ticker_data_list
            }}

            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                        print(f"    ‚úÖ {{ticker}}: {{len(results)}} signals")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  {{ticker}}: {{e}}")

        print(f"  ‚úÖ Stage 3b complete: {{len(all_results)}} total signals")
        return all_results

    def _process_ticker_optimized_pre_sliced(self, ticker_data: tuple):
        """
        ‚úÖ PILLAR 7: Process pre-sliced ticker data
        ‚úÖ FIX #10: Use snake_case throughout (matching compute_full_features)
        ‚úÖ PILLAR 4: Early D0 filtering

        Process pre-sliced ticker data with early D0 range filtering.
        Uses data directly from compute_full_features (no renaming, no recomputation).
        """
        ticker, ticker_df, d0_start_dt, d0_end_dt = ticker_data

        # ‚úÖ Minimum data check
        if len(ticker_df) < 100:
            return []

        # ‚úÖ FIX #10: Sort and prepare data (keep snake_case column names)
        # All features are already computed by compute_full_features()
        ticker_df = ticker_df.sort_values('date').reset_index(drop=True)

        # ‚úÖ FIX #10: Convert date to datetime for comparison
        ticker_df['date'] = pd.to_datetime(ticker_df['date'])

        # Get parameters
        try:
            P_local = self.params
        except:
            P_local = {{
                "price_min": 8.0,
                "adv20_min_usd": 30_000_000,
                "abs_lookback_days": 1000,
                "abs_exclude_days": 10,
                "pos_abs_max": 0.75,
                "atr_mult": 0.9,
                "vol_mult": 0.9,  # ‚úÖ FIX 15: Changed from 2.0 to 0.9 to match working scanner
                "slope5d_min": 3.0,
                "high_ema9_mult": 1.05,
                "gap_div_atr_min": 0.75,
                "open_over_ema9_min": 0.9,
                "d1_green_atr_min": 0.30,
                "require_open_gt_prev_high": True,
                "enforce_d1_above_d2": True,
                "d1_volume_min": 15_000_000,
                "trigger_mode": "D1_or_D2",  # ‚úÖ FIX 15: Added missing trigger_mode param
                "d1_vol_mult_min": None,  # ‚úÖ FIX 15: Added missing d1_vol_mult_min param
            }}

        # ‚úÖ Initialize results list (detection loop appends to this)
        all_rows = []

        # ‚úÖ FIX #10: Detection loop with early D0 filtering
        # Use ticker_df directly with snake_case column names (all features computed)
        for i in range(2, len(ticker_df)):
            d0 = ticker_df.iloc[i]['date']

            # ‚úÖ PILLAR 4: EARLY FILTER - Skip if not in D0 range
            if d0 < d0_start_dt or d0 > d0_end_dt:
                continue

            r0 = ticker_df.iloc[i]
            r1 = ticker_df.iloc[i-1]
            r2 = ticker_df.iloc[i-2]

            # Original detection logic from scan_symbol
            # This uses the extracted detection loop BODY (content inside the for loop)
            # The extracted body should NOT include its own for loop statement
            try:
{detection_loop_only if detection_loop_only else "                # No detection loop provided - add signal logic here\n                pass"}
            except:
                pass

        return all_rows

    def format_results(self, results: list) -> pd.DataFrame:
        """
        Format detection results for output

        Args:
            results: List of detection result dictionaries

        Returns:
            DataFrame with formatted results
        """
        if not results:
            return pd.DataFrame()

        df = pd.DataFrame(results)

        # Reorder columns for better readability
        column_order = ['ticker', 'date', 'close', 'gap', 'gap/atr', 'atr', 'adv20_usd']
        column_order = [col for col in column_order if col in df.columns]

        # Add any additional columns that aren't in the standard order
        additional_cols = [col for col in df.columns if col not in column_order]
        column_order.extend(additional_cols)

        return df[column_order]



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Module-level functions for backend integration
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def run_scan(d0_start=None, d0_end=None):
    # Module-level function for backend integration
    # Returns simple ticker+date results
    import os

    scanner = {scanner_name}(
        api_key=os.getenv('POLYGON_API_KEY', 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'),
        d0_start=d0_start or "{d0_start}",
        d0_end=d0_end or "{d0_end}"
    )

    results = scanner.run_scan()

    # Return simple results: just ticker and date
    simple_results = []
    for r in results:
        ticker_val = r.get('Ticker', r.get('ticker', 'N/A'))
        date_val = r.get('Date', r.get('date', 'N/A'))
        simple_results.append({{'ticker': ticker_val, 'date': date_val}})

    return simple_results


def main():
    # Main function for backend integration
    # Returns simple ticker+date results
    return run_scan()


if __name__ == "__main__":
    import os

    # Example usage
    scanner = {scanner_name}(
        api_key=os.getenv('POLYGON_API_KEY', 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'),
        d0_start="{d0_start}",
        d0_end="{d0_end}"
    )

    results = scanner.run_scan()

    if results:
        print(f"\\nResults ({{len(results)}} signals):")
        for r in results:
            ticker_val = r.get('Ticker', r.get('ticker', 'N/A'))
            date_val = r.get('Date', r.get('date', 'N/A'))
            print(f"{{ticker_val}} {{date_val}}")
    else:
        print("\\nNo results found.")
'''

        # Post-process if needed
        v31_code = self._post_process_scan_symbol_integration(v31_code, original_param)

        # Post-process: Replace __SELF__ placeholder with self
        v31_code = v31_code.replace('__SELF__', 'self')

        return v31_code

    def _indent_code(self, code: str, indent_spaces: int) -> str:
        """
        Indent all lines of code by specified spaces

        Args:
            code: Code to indent
            indent_spaces: Number of spaces to indent

        Returns:
            Indented code
        """
        lines = code.split('\n')
        indent = ' ' * indent_spaces
        indented_lines = [indent + line if line.strip() else line for line in lines]
        return '\n'.join(indented_lines)

    def _apply_v31_generic_transform(
        self,
        source_code: str,
        scanner_name: str,
        strategy: StrategySpec,
        date_range: str
    ) -> str:
        """
        Convert ANY scanner code to TRUE v31 architecture

        Implements the FULL v31 architecture (same as v31_hybrid) but works for ANY code:
        1. Extract parameters from ANY code
        2. Extract detection logic from ANY code
        3. Wrap in complete 5-stage v31 pipeline
        4. Use extracted parameters for smart filtering
        5. Implement proper D0 date filtering

        Args:
            source_code: Original scanner code
            scanner_name: Name for the v31 class
            strategy: Strategy specification
            date_range: Date range for scanning

        Returns:
            v31-structured scanner code with full architecture
        """
        import re

        print("\n" + "="*60)
        print("GENERIC V31 TRANSFORMATION - Full Architecture")
        print("="*60)

        # Parse date range
        if ' to ' in date_range:
            d0_start, d0_end = date_range.split(' to ')
        else:
            d0_start = '2024-01-01'
            d0_end = '2024-12-31'

        print(f"\nüìä Generic v31 Transformation:")
        print(f"   Scanner Name: {scanner_name}")
        print(f"   Date Range: {d0_start} to {d0_end}")

        # Remove main block if present
        lines = source_code.split('\n')
        filtered_lines = []
        in_main_block = False

        for line in lines:
            if 'if __name__ == "__main__"' in line:
                in_main_block = True
                continue

            if in_main_block:
                line_indent = len(line) - len(line.lstrip())
                if line_indent == 0 and line.strip() and not re.match(r'^\s*#', line):
                    in_main_block = False
                else:
                    continue

            filtered_lines.append(line)

        original_code = '\n'.join(filtered_lines)

        # Extract parameters from ANY code
        print("\nüîé EXTRACTING PARAMETERS...")
        detected_params = self._apply_enhanced_parameter_detection(original_code)
        print(f"   üìä Detected {len(detected_params)} parameters")

        # Determine historical buffer from parameters
        lookback_buffer = detected_params.get('abs_lookback_days', 730) + 50
        print(f"   üìÖ Historical buffer: {lookback_buffer} days")

        # Build the full v31 architecture template (same structure as v31_hybrid)
        v31_code = f'''"""
{scanner_name}

{strategy.description}

Generated by RENATA_V2 - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

TRUE v31 Architecture - All 7 Core Pillars Implemented:
1. ‚úÖ Market calendar (pandas_market_calendars)
2. ‚úÖ Historical buffer calculation
3. ‚úÖ Per-ticker operations (groupby().transform())
4. ‚úÖ Historical/D0 separation in smart filters
5. ‚úÖ Parallel processing (ThreadPoolExecutor)
6. ‚úÖ Two-pass feature computation
7. ‚úÖ Pre-sliced data for parallel processing
"""

{original_code}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TRUE v31 Scanner Class ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os

class {scanner_name}:
    """
    {strategy.name}

    {strategy.description}

    Scanner Type: Generic Scanner converted to TRUE v31 Architecture
    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

    TRUE v31 Architecture - All 7 Core Pillars Implemented
    """

    def __init__(self, d0_start: str = None, d0_end: str = None):
        """
        Initialize scanner with proper v31 configuration

        ‚úÖ PILLAR 2: Calculate historical buffer for indicators
        """
        # Date range configuration
        self.d0_start_user = d0_start or "{d0_start}"
        self.d0_end_user = d0_end or "{d0_end}"

        # ‚úÖ CRITICAL: Calculate historical buffer
        lookback_buffer = {lookback_buffer}  # From detected params with safety margin

        # Calculate scan_start with historical buffer
        scan_start_dt = pd.to_datetime(self.d0_start_user) - pd.Timedelta(days=lookback_buffer)
        self.scan_start = scan_start_dt.strftime('%Y-%m-%d')

        # ‚úÖ Workers for parallel processing
        self.stage3_workers = 10

        # ‚úÖ Parameters extracted from original code
        self.params = self._extract_parameters()

        # ‚úÖ Scanner name for logging
        self.scanner_name = self.__class__.__name__

        print(f"üìä Scanner initialized: {{self.scanner_name}}")
        print(f"   Historical buffer: {{lookback_buffer}} days")
        print(f"   Scan range: {{self.scan_start}} to {{self.d0_end_user}}")
        print(f"   D0 output range: {{self.d0_start_user}} to {{self.d0_end_user}}")

    def _extract_parameters(self) -> dict:
        """Extract parameters from original code or return defaults"""
        # Try to extract from global scope
        try:
            import sys
            frame = sys._getframe(1)
            if 'P' in frame.f_locals:
                return frame.f_locals['P']
        except:
            pass

        # Default parameters from detected params
        return {dict(detected_params) if detected_params else {}}

    def run_scan(self):
        """
        üöÄ Main execution: 5-stage v31 pipeline

        Returns:
            List of signal dictionaries
        """
        print(f"\\n{{'='*70}}")
        print(f"üöÄ RUNNING TRUE v31 SCAN: {{self.scanner_name}}")
        print(f"{{'='*70}}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data()
        if stage1_data is None or stage1_data.empty:
            print("\\n‚ùå Scan failed: No data loaded")
            return []

        # Stage 2a: Compute simple features
        stage2a_data = self.compute_simple_features(stage1_data)

        # Stage 2b: Apply smart filters (with historical/D0 separation)
        stage2b_data = self.apply_smart_filters(stage2a_data)

        # Stage 3a: Compute full features
        stage3a_data = self.compute_full_features(stage2b_data)

        # Stage 3b: Detect patterns (with pre-sliced parallel processing)
        stage3_results = self.detect_patterns(stage3a_data)

        print(f"\\n‚úÖ SCAN COMPLETE: {{len(stage3_results)}} signals detected")
        return stage3_results

    def fetch_grouped_data(self):
        """
        ‚úÖ PILLAR 1: Market calendar integration
        ‚úÖ PILLAR 5: Parallel processing

        Stage 1: Fetch ALL tickers for ALL dates using Polygon's grouped API (direct approach)
        """
        import requests
        from datetime import timedelta

        API_KEY = os.getenv("POLYGON_API_KEY", "Fm7brz4s23eSocDErnL68cE7wspz2K1I")
        BASE_URL = "https://api.polygon.io"

        print(f"  üì° Fetching data from {{self.scan_start}} to {{self.d0_end_user}} using Polygon grouped API...")

        all_data = []
        current_date = pd.to_datetime(__SELF__.scan_start).date()
        end = pd.to_datetime(__SELF__.d0_end_user).date()

        # Fetch data for each trading day
        while current_date <= end:
            # Skip weekends
            if current_date.weekday() < 5:  # Monday-Friday
                date_str = current_date.strftime("%Y-%m-%d")
                url = BASE_URL + "/v2/aggs/grouped/locale/us/market/stocks/" + date_str
                params = {{
                    "adjusted": "true",
                    "apiKey": API_KEY
                }}

                try:
                    response = requests.get(url, params=params)
                    response.raise_for_status()
                    data = response.json()

                    if 'results' in data and data['results']:
                        df_daily = pd.DataFrame(data['results'])
                        df_daily['date'] = pd.to_datetime(df_daily['t'], unit='ms').dt.date
                        df_daily.rename(columns={{'T': 'ticker', 'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}}, inplace=True)
                        all_data.append(df_daily[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']])

                except Exception as e:
                    print(f"    ‚ö†Ô∏è  Error fetching data for {{date_str}}: {{e}}")

            current_date += timedelta(days=1)

        if not all_data:
            print(f"  ‚ùå No data fetched!")
            return pd.DataFrame()

        # Combine all daily data
        df = pd.concat(all_data, ignore_index=True)

        if df.empty:
            print(f"  ‚ùå No data fetched!")
            return pd.DataFrame()

        print(f"  ‚úÖ Stage 1 complete: {{len(df)}} total records")
        print(f"     üìä Unique tickers: {{df['ticker'].nunique()}}")
        print(f"     üìÖ Date range: {{df['date'].min()}} to {{df['date'].max()}}")
        return df

    def compute_simple_features(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 3: Per-ticker operations
        ‚úÖ PILLAR 6: Two-pass feature computation (simple first)

        Stage 2a: Compute SIMPLE features for efficient filtering

        Only computes features needed for filtering:
        - prev_close
        - adv20_usd (with per-ticker groupby)
        - price_range
        """
        if df.empty:
            return df

        print(f"  üìä Stage 2a: Computing simple features for {{len(df)}} rows")

        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'])

        # Previous close
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)

        # ‚úÖ PILLAR 3: Per-ticker operations for ADV20
        df['adv20_usd'] = (df['close'] * df['volume']).groupby(df['ticker']).transform(
            lambda x: x.rolling(window=20, min_periods=20).mean()
        )

        # Price range
        df['price_range'] = df['high'] - df['low']

        print(f"    ‚úÖ Simple features computed")
        return df

    def apply_smart_filters(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 4: Separate historical from D0 data

        Stage 2b: Smart filters with HISTORICAL DATA PRESERVATION

        CRITICAL: Only filter D0 output range, preserve all historical data
        for indicator calculations.
        """
        if df.empty:
            return df

        print(f"  üìä Stage 2b input: {{len(df)}} rows")

        # ‚úÖ PILLAR 4: Split historical from D0
        df_historical = df[~df['date'].between(__SELF__.d0_start_user, __SELF__.d0_end_user)].copy()
        df_output_range = df[df['date'].between(__SELF__.d0_start_user, __SELF__.d0_end_user)].copy()

        print(f"    üìä Historical: {{len(df_historical)}} rows")
        print(f"    üìä D0 range: {{len(df_output_range)}} rows")

        # ‚úÖ CRITICAL: Filter ONLY D0 range using extracted parameters
        df_output_filtered = df_output_range.copy()

        # Price filter
        if 'price_min' in self.params:
            min_price = self.params['price_min']
            df_output_filtered = df_output_filtered[
                (df_output_filtered['close'] >= min_price) &
                (df_output_filtered['open'] >= min_price)
            ]
            print(f"    üìä After price filter (>= ${{min_price}}): {{len(df_output_filtered)}} rows")

        # Volume filter
        if 'adv20_min_usd' in self.params:
            min_adv = self.params['adv20_min_usd']
            df_output_filtered = df_output_filtered[
                df_output_filtered['adv20_usd'] >= min_adv
            ]
            print(f"    üìä After volume filter (>= ${{min_adv:,}}): {{len(df_output_filtered)}} rows")

        print(f"  ‚úÖ Stage 2b complete: {{len(df_historical) + len(df_output_filtered)}} rows (historical preserved)")

        # ‚úÖ CRITICAL: COMBINE historical + filtered D0
        df_combined = pd.concat([df_historical, df_output_filtered], ignore_index=True)
        return df_combined

    def compute_full_features(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 3: Per-ticker operations
        ‚úÖ PILLAR 6: Two-pass feature computation (full features after filter)

        Stage 3a: Compute ALL technical indicators

        Computes expensive features only on data that passed filters.
        """
        if df.empty:
            return df

        print(f"  üìä Stage 3a: Computing full features for {{len(df)}} rows")

        result_dfs = []

        for ticker, group in df.groupby('ticker'):
            group = group.sort_values('date').copy()

            # EMA
            group['ema_9'] = group['close'].ewm(span=9, adjust=False).mean()
            group['ema_20'] = group['close'].ewm(span=20, adjust=False).mean()

            # ATR
            hi_lo = group['high'] - group['low']
            hi_prev = (group['high'] - group['close'].shift(1)).abs()
            lo_prev = (group['low'] - group['close'].shift(1)).abs()
            group['tr'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
            group['atr'] = group['tr'].rolling(14, min_periods=14).mean().shift(1)

            # Additional common indicators
            group['vol_avg'] = group['volume'].rolling(14, min_periods=14).mean().shift(1)
            group['prev_volume'] = group['volume'].shift(1)

            # RSI
            delta = group['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=14).mean()
            rs = gain / loss
            group['rsi'] = 100 - (100 / (1 + rs))

            result_dfs.append(group)

        result = pd.concat(result_dfs, ignore_index=True)
        print(f"    ‚úÖ Full features computed")
        return result

    def detect_patterns(self, df: pd.DataFrame):
        """
        ‚úÖ PILLAR 7: Pre-sliced data for parallel processing
        ‚úÖ PILLAR 5: Parallel ticker processing

        Stage 3b: Pattern detection with parallel processing
        """
        if df.empty:
            return []

        print(f"  üéØ Stage 3b: Detecting patterns in {{len(df)}} rows")

        # Get D0 range
        d0_start_dt = pd.to_datetime(__SELF__.d0_start_user)
        d0_end_dt = pd.to_datetime(__SELF__.d0_end_user)

        # ‚úÖ PILLAR 7: Pre-slice ticker data BEFORE parallel processing
        ticker_data_list = []
        for ticker, ticker_df in df.groupby('ticker'):
            ticker_data_list.append((ticker, ticker_df.copy(), d0_start_dt, d0_end_dt))

        all_results = []

        # ‚úÖ PILLAR 5: Parallel processing with pre-sliced data
        from concurrent.futures import ThreadPoolExecutor, as_completed

        with ThreadPoolExecutor(max_workers=self.stage3_workers) as executor:
            future_to_ticker = {{
                executor.submit(self._process_ticker, ticker_data): ticker_data[0]
                for ticker_data in ticker_data_list
            }}

            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                        print(f"    ‚úÖ {{ticker}}: {{len(results)}} signals")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è  {{ticker}}: {{e}}")

        print(f"  ‚úÖ Stage 3b complete: {{len(all_results)}} total signals")
        return all_results

    def _process_ticker(self, ticker_data: tuple):
        """
        ‚úÖ PILLAR 7: Process pre-sliced ticker data
        ‚úÖ PILLAR 4: Early D0 filtering

        Process ticker data with original detection logic
        """
        ticker, ticker_df, d0_start_dt, d0_end_dt = ticker_data

        # ‚úÖ Minimum data check
        if len(ticker_df) < 100:
            return []

        # ‚úÖ Sort and prepare data
        ticker_df = ticker_df.sort_values('date').reset_index(drop=True)
        ticker_df['date'] = pd.to_datetime(ticker_df['date'])

        # ‚úÖ Initialize results list
        all_rows = []

        # ‚úÖ Apply original detection logic with D0 filtering
        # The original code can use all the computed features
        try:
{self._indent_code(original_code, 12)}
        except Exception as e:
            print(f"    ‚ö†Ô∏è  {{ticker}}: Error in detection logic: {{e}}")

        return all_rows

    def format_results(self, all_results: list) -> pd.DataFrame:
        """
        Format results for display

        Args:
            all_results: List of signal dictionaries from detect_patterns

        Returns:
            DataFrame with formatted results (Ticker, Date, Scanner_Label)
        """
        if not all_results:
            return pd.DataFrame()

        # Convert to DataFrame
        df = pd.DataFrame(all_results)

        # Ensure required columns exist
        if 'Ticker' not in df.columns or 'Date' not in df.columns:
            print("  ‚ö†Ô∏è  Results missing required columns")
            return pd.DataFrame()

        # Group by ticker and date, aggregate labels
        if 'Scanner_Label' not in df.columns:
            df['Scanner_Label'] = 'Signal'

        # Aggregate by ticker+date
        aggregated = df.groupby(['Ticker', 'Date'])['Scanner_Label'].apply(
            lambda x: ', '.join(sorted(set(x)))
        ).reset_index()

        # Sort by date
        aggregated = aggregated.sort_values('Date').reset_index(drop=True)

        print(f"  ‚úÖ Formatted {{len(aggregated)}} unique signals")
        return aggregated


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Main Entry Point
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    import sys

    # Parse command line arguments
    d0_start = sys.argv[1] if len(sys.argv) > 1 else "2024-01-01"
    d0_end = sys.argv[2] if len(sys.argv) > 2 else "2024-12-31"

    print(f"\\nüöÄ Starting {{scanner_name}} Scanner")
    print(f"   D0 Range: {{d0_start}} to {{d0_end}}")
    print(f"\\n{{'='*70}}")

    # Create scanner instance and run
    scanner = {{scanner_name}}(d0_start=d0_start, d0_end=d0_end)
    results = scanner.run_scan()

    # Display results
    if results:
        print(f"\\n{{'='*70}}")
        print(f"üéØ SCAN RESULTS: {{len(results)}} signals detected")
        print(f"{{'='*70}}")

        # Convert to DataFrame for display
        import pandas as pd
        df = pd.DataFrame(results)

        # Show key columns
        if 'Ticker' in df.columns and 'Date' in df.columns:
            for _, row in df.head(20).iterrows():
                print(f"  {{row['Ticker']:6s}} | {{row['Date']}}")

            if len(df) > 20:
                print(f"  ... and {{len(df) - 20}} more signals")
    else:
        print(f"\\n‚ùå No signals detected")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Module-level functions for backend integration
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
'''

        print(f"\n‚úÖ GENERIC V31 TRANSFORMATION COMPLETE")
        print(f"   Full v31 architecture applied")
        print(f"   Parameters extracted and used for smart filtering")
        print(f"   Original detection logic preserved")
        print(f"   5-stage pipeline implemented")

        # Post-process: Replace __SELF__ placeholder with self
        v31_code = v31_code.replace('__SELF__', 'self')

        return v31_code

    def _apply_v31_multi_transform(
        self,
        source_code: str,
        scanner_name: str,
        strategy: StrategySpec,
        date_range: str,
        ast_result: ClassificationResult
    ) -> str:
        """
        Convert multi-scanner to TRUE v31 architecture

        Multi-scanners contain multiple pattern detectors in one file.
        This transformation:
        1. Preserves all pattern detection logic from each pattern
        2. Converts to v31 architecture (all 7 pillars)
        3. Ensures all patterns can run on the same data
        4. Aggregates results from all patterns

        Args:
            source_code: Original multi-scanner code
            scanner_name: Name for the v31 class
            strategy: Strategy specification
            date_range: Date range for scanning
            ast_result: AST classification result (for pattern info)

        Returns:
            v31-structured multi-scanner code with all patterns preserved
        """
        import re

        print("\n" + "="*60)
        print("MULTI-SCANNER TO V31 TRANSFORMATION")
        print("="*60)

        # Parse date range
        if ' to ' in date_range:
            d0_start, d0_end = date_range.split(' to ')
        else:
            d0_start = '2024-01-01'
            d0_end = '2024-12-31'

        print(f"\nüìä Multi-Scanner Transformation:")
        print(f"   Date Range: {d0_start} to {d0_end}")

        # Extract pattern information from AST result
        indicators = ast_result.indicators or {}
        pattern_count = indicators.get('pattern_count', 0)
        unique_patterns = indicators.get('unique_pattern_names', [])
        print(f"   Patterns Detected: {pattern_count}")
        print(f"   Pattern Names: {unique_patterns}")

        # Extract the original scanner class/main code structure
        print("\nüîß EXTRACTING MULTI-PATTERN STRUCTURE...")

        # Check if this is a class-based scanner or function-based scanner
        has_class = re.search(r'class\s+\w+\s*\([^)]*\):', source_code)
        has_main = 'if __name__' in source_code

        if has_class:
            print("   ‚úÖ Class-based scanner detected")
            # Extract the class name and structure
            class_match = re.search(r'class\s+(\w+)\s*\([^)]*\):', source_code)
            original_class_name = class_match.group(1) if class_match else "MultiScanner"
            print(f"   üì¶ Original class: {original_class_name}")
        else:
            print("   ‚ÑπÔ∏è  Function-based scanner - will wrap in class")
            original_class_name = "MultiScanner"

        # Extract pattern detection logic
        # Multi-scanners typically have:
        # 1. Pattern columns: df['lc_frontside_d3'] = ...
        # 2. Pattern check functions: def check_d2_pattern(...)
        # 3. Filter functions: def check_high_lvl_filter_lc(...)

        print("\nüéØ EXTRACTING PATTERN DETECTION LOGIC...")

        # Use AST parsing to find pattern assignments more accurately
        import ast
        import sys

        pattern_assignments = []
        try:
            # Parse the source code into an AST
            tree = ast.parse(source_code)

            # Find all assignment statements where the target is df['pattern_name']
            for node in ast.walk(tree):
                if isinstance(node, ast.Assign):
                    for target in node.targets:
                        # Check if this is df['pattern_name'] = (condition).astype(int)
                        if isinstance(target, ast.Subscript):
                            if isinstance(target.value, ast.Name) and target.value.id == 'df':
                                # Get the pattern name (the index of df[...])
                                if isinstance(target.slice, ast.Constant):
                                    pattern_name = target.slice.value
                                elif isinstance(target.slice, ast.Str):  # Python 3.7 compatibility
                                    pattern_name = target.slice.s
                                elif isinstance(target.slice, ast.Index):  # Python 3.8 compatibility
                                    if isinstance(target.slice.value, ast.Constant):
                                        pattern_name = target.slice.value.value
                                    elif isinstance(target.slice.value, ast.Str):
                                        pattern_name = target.slice.value.s
                                    else:
                                        continue
                                else:
                                    continue

                                # Check if the value is a Call with .astype(int)
                                value = node.value
                                if isinstance(value, ast.Call):
                                    # Check if this is .astype(int)
                                    if isinstance(value.func, ast.Attribute) and value.func.attr == 'astype':
                                        # Get the actual condition (the thing being called with .astype)
                                        condition = value.func.value

                                        # Try to use ast.unparse() if available (Python 3.9+)
                                        pattern_logic = None
                                        if sys.version_info >= (3, 9):
                                            try:
                                                pattern_logic = ast.unparse(condition)
                                            except Exception:
                                                pass

                                        # Fallback: extract from source code using line numbers
                                        if not pattern_logic and hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):
                                            lines = source_code.split('\n')
                                            start_line = node.lineno - 1
                                            end_line = node.end_lineno - 1
                                            pattern_logic = '\n'.join(lines[start_line:end_line+1])

                                            # Extract just the condition part by counting parentheses
                                            # Find the opening ( after = and matching closing )
                                            import re
                                            match = re.search(
                                                rf"df\[['\"]{re.escape(str(pattern_name))}['\"]\]\s*=\s*\(",
                                                pattern_logic
                                            )
                                            if match:
                                                # Start from the opening parenthesis
                                                start_pos = match.end()
                                                paren_count = 1
                                                pos = start_pos
                                                while pos < len(pattern_logic) and paren_count > 0:
                                                    if pattern_logic[pos] == '(':
                                                        paren_count += 1
                                                    elif pattern_logic[pos] == ')':
                                                        paren_count -= 1
                                                    pos += 1

                                                # Extract the condition (excluding the final closing paren)
                                                if paren_count == 0:
                                                    # Found matching closing paren
                                                    condition_start = start_pos
                                                    condition_end = pos - 1  # -1 to exclude the final )

                                                    # Check if the next non-whitespace chars are .astype(int)
                                                    remaining = pattern_logic[condition_end+1:].lstrip()
                                                    if remaining.startswith('.astype(int)'):
                                                        pattern_logic = pattern_logic[condition_start:condition_end]
                                                    else:
                                                        # Not a .astype(int) pattern, skip
                                                        pattern_logic = None
                                                else:
                                                    # Didn't find matching paren
                                                    pattern_logic = None

                                        # If we got the pattern logic, add it to the list
                                        if pattern_logic:
                                            # Filter out non-trading patterns
                                            if not self._is_trading_pattern(str(pattern_name)):
                                                print(f"   ‚è≠Ô∏è  Skipping non-trading pattern: {pattern_name}")
                                                continue

                                            # Clean up the logic - compress but keep structure
                                            pattern_logic = ' '.join(pattern_logic.split())

                                            # Convert df['column'] to just column for eval() compatibility
                                            pattern_logic = self._convert_pattern_logic_for_eval(pattern_logic)

                                            pattern_assignments.append({
                                                'name': str(pattern_name),
                                                'logic': pattern_logic
                                            })
                                            print(f"   ‚úÖ Found trading pattern: {pattern_name}")
        except SyntaxError as e:
            print(f"   ‚ö†Ô∏è  AST parsing failed: {e}")
            print(f"   ‚ÑπÔ∏è  Falling back to regex extraction...")

            # Fallback to improved regex that handles multi-line patterns
            # This regex finds df['pattern'] = (complex_logic).astype(int)
            # even when the logic spans multiple lines with nested parentheses
            import re

            # Find all df['...'] = assignments that end with .astype(int)
            pattern_assignment_pattern = r"df\[['\"]([^'\"]+?)['\"]\]\s*=\s*\("

            for match in re.finditer(pattern_assignment_pattern, source_code):
                pattern_name = match.group(1)
                start_pos = match.end()

                # Extract the full pattern logic by counting parentheses
                paren_count = 1
                pos = start_pos
                while pos < len(source_code) and paren_count > 0:
                    if source_code[pos] == '(':
                        paren_count += 1
                    elif source_code[pos] == ')':
                        paren_count -= 1
                    pos += 1

                # Check if this ends with .astype(int)
                if paren_count == 0:
                    # Look ahead for .astype(int) allowing whitespace and comments
                    remaining = source_code[pos:].lstrip()
                    if remaining.startswith('.astype(int)'):
                        # Extract the condition (excluding the final closing paren)
                        pattern_logic = source_code[start_pos:pos-1]

                        # Clean up the logic
                        pattern_logic = '\n'.join(
                            ' ' * 8 + line.strip()
                            for line in pattern_logic.split('\n')
                            if line.strip() and not line.strip().startswith('#')
                        ).strip()

                        # Only add if this looks like a real pattern (not just an assignment)
                        if len(pattern_logic) > 20:  # Minimum complexity threshold
                            # Filter out non-trading patterns
                            if not self._is_trading_pattern(pattern_name):
                                print(f"   ‚è≠Ô∏è  Skipping non-trading pattern: {pattern_name}")
                                continue

                            # Convert df['column'] to just column for eval() compatibility
                            pattern_logic = self._convert_pattern_logic_for_eval(pattern_logic)

                            pattern_assignments.append({
                                'name': pattern_name,
                                'logic': pattern_logic
                            })
                            print(f"   ‚úÖ Found trading pattern: {pattern_name}")

        # Find pattern check functions
        # e.g., def check_high_lvl_filter_lc(...)
        pattern_functions = []
        for match in re.finditer(
            r"def\s+(check_\w+|scan_\w+)\s*\((.*?)\):",
            source_code
        ):
            func_name = match.group(1)
            func_params = match.group(2)
            # Find the function body
            func_start = match.end()
            # Extract function body (simplified - just get next 50 lines or until next def/class)
            func_end = func_start
            lines_from_match = source_code[func_end:].split('\n')
            func_lines = []
            indent_level = None
            for line in lines_from_match[:100]:  # Max 100 lines per function
                stripped = line.strip()
                if not stripped:
                    func_lines.append(line)
                    continue
                # Check if we've reached the next function/class
                if stripped.startswith('def ') or stripped.startswith('class '):
                    break
                func_lines.append(line)
                func_end += len(line) + 1

            pattern_functions.append({
                'name': func_name,
                'params': func_params,
                'body': '\n'.join(func_lines)
            })
            print(f"   ‚úÖ Found function: {func_name}")

        # Extract indicator computation
        # Look for compute_indicators or similar functions
        print("\nüìä EXTRACTING INDICATOR COMPUTATION...")

        # Try to find the complete compute_indicators1 function
        # It ends with "return df" before the next function
        indicator_match = re.search(
            r"(def\s+compute_indicators1\s*\([^)]*\):.*?return\s+df\s*\n)",
            source_code,
            re.DOTALL
        )

        if indicator_match:
            indicator_func_name = "compute_indicators1"
            indicator_body = indicator_match.group(1)
            print(f"   ‚úÖ Found indicator function: {indicator_func_name} ({len(indicator_body)} characters)")

            # Fix common copy-paste errors in the indicator function
            print(f"   üîß VALIDATING AND FIXING INDICATOR FUNCTION...")
            indicator_body = self._fix_common_indicator_bugs(indicator_body)

            # Remove duplicate column creation from indicator function
            # The v31 template creates these columns BEFORE calling compute_indicators1
            print(f"   üîß REMOVING DUPLICATE COLUMN CREATION...")
            indicator_body = self._remove_duplicate_column_creation(indicator_body)
            print(f"   ‚úÖ Indicator function validated and optimized")
        else:
            print(f"   ‚ö†Ô∏è  Could not extract full indicator function")
            indicator_func_name = "compute_indicators1"
            indicator_body = None

        # Extract data fetching logic
        print("\nüåê EXTRACTING DATA FETCHING...")
        if 'get_grouped' in source_code or 'fetch_grouped' in source_code:
            data_source = 'grouped_api'
            print("   ‚úÖ Using grouped API (Polygon)")
        elif 'fetch_daily' in source_code:
            data_source = 'polygon_daily'
            print("   ‚úÖ Using Polygon daily API")
        elif 'SYMBOLS' in source_code:
            data_source = 'hardcoded_symbols'
            print("   ‚ÑπÔ∏è  Using hardcoded symbols")
        else:
            data_source = 'unknown'
            print("   ‚ö†Ô∏è  Data source unknown - will use grouped API")

        # Generate the v31 multi-scanner code
        print("\nüöÄ GENERATING V31 MULTI-SCANNER CODE...")

        # Pre-compute pattern names list for the template
        pattern_names_list = '\n'.join(f"    - {p['name']}" for p in pattern_assignments[:10])
        if len(pattern_assignments) > 10:
            pattern_names_list += f"\n    ... and {len(pattern_assignments) - 10} more"

        # Serialize pattern_assignments for safe insertion into Python code
        import json
        pattern_assignments_str = json.dumps(pattern_assignments, indent=4, ensure_ascii=False)

        # Prepare indicator computation section
        if indicator_body:
            # Use the original scanner's indicator function (already inserted as class method)
            indicator_section = '''
        # ==================== USE ORIGINAL INDICATOR FUNCTION ====================
        # Using original scanner's compute_indicators1 function
        # This ensures all pattern-specific indicators are computed correctly

        # VALIDATE: Ensure all required columns exist before computing indicators
        required_cols = ['o', 'h', 'l', 'c', 'v']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns for indicator computation: {{missing_cols}}")

        # CRITICAL: Convert all price/volume columns to numeric types
        # Data loaded from files may have string values that cause calculation errors
        numeric_cols = ['o', 'h', 'l', 'c', 'v', 'open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Fill any NaN values that resulted from type conversion
        df = df.fillna(0)

        # Initialize cumulative up-day counters and price/volume columns
        # These MUST be created BEFORE calling compute_indicators1
        # c_ua: cumulative up-days for close (tracks consecutive days where close increased)
        # l_ua: actual low price (NOT cumulative - used for price calculations)
        # v_ua: actual volume (NOT cumulative - used for volume thresholds)
        df['c_ua'] = df.groupby('ticker')['c'].transform(
            lambda x: (x > x.shift(1)).cumsum()
        )
        df['l_ua'] = df['l']
        df['v_ua'] = df['v']

        # Apply original indicator computation
        # All required columns (o, h, l, c, v, c_ua, l_ua, v_ua) are now available and properly typed
        df = self.compute_indicators1(df)
'''
        else:
            # Use generic indicators
            indicator_section = '''
        # ==================== GENERIC INDICATORS ====================
        # Using generic indicators (original indicator function not available)

        # EMAs
        df['ema_9'] = df.groupby('ticker')['close'].transform(
            lambda x: x.ewm(span=9, adjust=False).mean()
        )
        df['ema_20'] = df.groupby('ticker')['close'].transform(
            lambda x: x.ewm(span=20, adjust=False).mean()
        )
        df['ema_21'] = df.groupby('ticker')['close'].transform(
            lambda x: x.ewm(span=21, adjust=False).mean()
        )

        # True Range and ATR
        df['tr'] = np.maximum(
            df['high'] - df['low'],
            np.maximum(
                abs(df['high'] - df['close'].shift(1)),
                abs(df['low'] - df['close'].shift(1))
            )
        )
        df['atr'] = df.groupby('ticker')['tr'].transform(
            lambda x: x.ewm(span=14, adjust=False).mean()
        )
        df['atr_9'] = df.groupby('ticker')['tr'].transform(
            lambda x: x.ewm(span=9, adjust=False).mean()
        )
        df['atr_21'] = df.groupby('ticker')['tr'].transform(
            lambda x: x.ewm(span=21, adjust=False).mean()
        )

        # Previous values
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)
        df['prev_open'] = df.groupby('ticker')['open'].shift(1)
        df['prev_high'] = df.groupby('ticker')['high'].shift(1)
        df['prev_low'] = df.groupby('ticker')['low'].shift(1)
        df['prev_volume'] = df.groupby('ticker')['volume'].shift(1)

        # Shifted values (D-2, D-3, etc.)
        for i in range(2, 6):
            df[f'close_{{i}}'] = df.groupby('ticker')['close'].shift(i)
            df[f'high_{{i}}'] = df.groupby('ticker')['high'].shift(i)
            df[f'low_{{i}}'] = df.groupby('ticker')['low'].shift(i)
            df[f'open_{{i}}'] = df.groupby('ticker')['open'].shift(i)
            df[f'volume_{{i}}'] = df.groupby('ticker')['volume'].shift(i)

        # Gaps and ranges
        df['gap'] = (df['open'] / df['prev_close']) - 1
        df['range'] = df['high'] - df['low']
        df['prev_range'] = df.groupby('ticker')['range'].shift(1)
'''

        # Build the multi-scanner class
        v31_code = f'''"""
{scanner_name}

Multi-Pattern Scanner - {len(pattern_assignments)} patterns

Generated by RENATA_V2 - Multi-Scanner to V31 Transformation
Date Range: {date_range}
Patterns: {', '.join(unique_patterns[:10])}{'...' if len(unique_patterns) > 10 else ''}

V31 Architecture:
1. Market calendar integration (pandas_market_calendars)
2. Historical buffer calculation
3. Per-ticker operations (groupby().transform())
4. Historical/D0 separation in smart filters
5. Parallel processing (ThreadPoolExecutor)
6. Two-pass feature computation
7. Pre-sliced data for parallel processing
"""

import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas_market_calendars as mcal
from typing import List, Dict, Any, Optional


class {scanner_name}:
    """
    Multi-Pattern Scanner with {len(pattern_assignments)} pattern detectors

    Patterns:
{pattern_names_list}
    {len(pattern_assignments)} total patterns

    Scanner Type: multi
    Generated: {{datetime.now().isoformat()}}
    """

    def __init__(
        self,
        d0_start: str = None,
        d0_end: str = None
    ):
        """Initialize multi-scanner"""

        # Date configuration
        self.d0_start = d0_start or "{d0_start}"
        self.d0_end = d0_end or "{d0_end}"

        # Historical buffer for indicator computation
        self.historical_buffer_days = 1050  # ~2 years of trading days

        # Scan range (historical data needed for indicators)
        self.scan_start = (
            pd.Timestamp(self.d0_start) - pd.Timedelta(days=self.historical_buffer_days)
        ).strftime('%Y-%m-%d')
        self.scan_end = self.d0_end

        # Worker configuration
        self.stage3_workers = 10  # Parallel pattern detection

        # Market calendar
        self.us_calendar = mcal.get_calendar('NYSE')

        # Smart filter configuration (universal filters)
        self.smart_filters = {{
            "min_prev_close": 0.75,
            "max_prev_close": 1000,
            "min_prev_volume": 500000,
            "max_prev_volume": 100000000
        }}

        # Pattern assignments (extracted from original scanner)
        self.pattern_assignments = {pattern_assignments_str}

        # Results storage
        self.all_results = []
        self.stage1_data = None
        self.stage2_data = None
        self.stage3_results = None

        print(f"üöÄ MULTI-PATTERN SCANNER: {scanner_name}")
        print(f"üìÖ Signal Output Range (D0): {{self.d0_start}} to {{self.d0_end}}")
        print(f"üìä Historical Data Range: {{self.scan_start}} to {{self.scan_end}}")
        print(f"üéØ Patterns: {{len(self.pattern_assignments)}}")

    def get_trading_dates(self, start_date: str, end_date: str) -> List[str]:
        """Get all valid trading days between start and end date"""
        schedule = self.us_calendar.schedule(
            start_date=pd.to_datetime(start_date),
            end_date=pd.to_datetime(end_date)
        )
        trading_days = self.us_calendar.valid_days(
            start_date=pd.to_datetime(start_date),
            end_date=pd.to_datetime(end_date)
        )
        return [date.strftime('%Y-%m-%d') for date in trading_days]

    # ==================== STAGE 1: FETCH GROUPED DATA ====================

    def fetch_grouped_data(
        self,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        Stage 1: Fetch ALL data for ALL tickers using Polygon's grouped API (direct approach)

        Uses Polygon's grouped endpoint for efficient data fetching.
        """
        import requests
        from datetime import timedelta

        API_KEY = os.getenv("POLYGON_API_KEY", "Fm7brz4s23eSocDErnL68cE7wspz2K1I")
        BASE_URL = "https://api.polygon.io"

        print(f"\\nüöÄ STAGE 1: FETCH GROUPED DATA")
        print(f"üì° Fetching data from {{start_date}} to {{end_date}} using Polygon grouped API...")

        all_data = []
        current_date = pd.to_datetime({{start_date}}).date()
        end = pd.to_datetime({{end_date}}).date()

        try:
            start_time = time.time()

            # Fetch data for each trading day
            while current_date <= end:
                # Skip weekends
                if current_date.weekday() < 5:  # Monday-Friday
                    date_str = current_date.strftime("%Y-%m-%d")
                    url = BASE_URL + "/v2/aggs/grouped/locale/us/market/stocks/" + date_str
                    params = {
                        "adjusted": "true",
                        "apiKey": API_KEY
                    }

                    try:
                        response = requests.get(url, params=params)
                        response.raise_for_status()
                        data = response.json()

                        if 'results' in data and data['results']:
                            df_daily = pd.DataFrame(data['results'])
                            df_daily['date'] = pd.to_datetime(df_daily['t'], unit='ms').dt.date
                            df_daily.rename(columns={'T': 'ticker', 'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}, inplace=True)
                            all_data.append(df_daily[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']])

                    except Exception as e:
                        print(f"    ‚ö†Ô∏è  Error fetching data for {{date_str}}: {{e}}")

                current_date += timedelta(days=1)

            if not all_data:
                print("‚ùå No data fetched!")
                return pd.DataFrame()

            # Combine all daily data
            df = pd.concat(all_data, ignore_index=True)

            elapsed = time.time() - start_time

            if df.empty:
                print("‚ùå No data fetched!")
                return pd.DataFrame()

            print(f"\\nüöÄ Stage 1 Complete ({{elapsed:.1f}}s):")
            print(f"üìä Total rows: {{len(df):,}}")
            print(f"üìä Unique tickers: {{df['ticker'].nunique():,}}")
            print(f"üìÖ Date range: {{df['date'].min()}} to {{df['date'].max()}}")

            return df

        except Exception as e:
            print(f"‚ùå Error fetching data: {{e}}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

    # ==================== STAGE 2: APPLY SMART FILTERS ====================

    def apply_smart_filters(
        self,
        stage1_data: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Stage 2: Apply smart filters to reduce dataset by ~99%

        Args:
            stage1_data: Output from fetch_grouped_data

        Returns:
            Filtered DataFrame (stage2_data)
        """
        print(f"\\nüß† STAGE 2: APPLY SMART FILTERS")
        print(f"üìä Input rows: {{len(stage1_data):,}}")

        if stage1_data.empty:
            return pd.DataFrame()

        df = stage1_data.copy()

        # Apply price filters
        if self.smart_filters["min_prev_close"]:
            df = df[df['close'] >= self.smart_filters["min_prev_close"]]

        if self.smart_filters["max_prev_close"]:
            df = df[df['close'] <= self.smart_filters["max_prev_close"]]

        # Apply volume filters
        if self.smart_filters["min_prev_volume"]:
            df = df[df['volume'] >= self.smart_filters["min_prev_volume"]]

        if self.smart_filters["max_prev_volume"]:
            df = df[df['volume'] <= self.smart_filters["max_prev_volume"]]

        print(f"üìä Output rows: {{len(df):,}} ({{len(df)/len(stage1_data)*100:.1f}}% of original)")

        return df

    # ==================== ORIGINAL INDICATOR FUNCTION ====================
{self._indent_for_class_method(indicator_body) if indicator_body else ''}

    # ==================== STAGE 3: COMPUTE INDICATORS + DETECT PATTERNS ====================

    def compute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Compute all indicators needed for pattern detection

        This includes:
        - EMAs (9, 20, 21 periods)
        - ATR (Average True Range)
        - Previous values (shifts)
        - Gaps and ranges
        - Pattern-specific indicators
        """
        print(f"\\nüìä COMPUTING INDICATORS...")
        print(f"üìä Input rows: {{len(df):,}}")

        # Sort by ticker and date
        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)

        # ==================== COLUMN NAME MAPPING ====================
        # Map Polygon API column names to original scanner column names
        # Original scanners often use: o, h, l, c, v instead of open, high, low, close, volume
        column_mapping = {{
            'open': 'o',
            'high': 'h',
            'low': 'l',
            'close': 'c',
            'volume': 'v'
        }}

        # Create mapped columns for compatibility with original indicators
        for polygon_col, original_col in column_mapping.items():
            df[original_col] = df[polygon_col]

        # ==================== INDICATOR COMPUTATION ====================
        {indicator_section}

        print(f"‚úÖ Indicators computed: {{len(df.columns)}} columns")

        return df

    def detect_patterns(self, stage2_data: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 3: Detect all patterns

        This method detects all {len(pattern_assignments)} patterns:
{pattern_names_list}
        {len(pattern_assignments)} total patterns

        Args:
            stage2_data: Filtered DataFrame from apply_smart_filters

        Returns:
            DataFrame with all pattern matches
        """
        print(f"\\nüéØ STAGE 3: DETECT PATTERNS")
        print(f"üìä Input rows: {{len(stage2_data):,}}")

        if stage2_data.empty:
            return pd.DataFrame()

        # Compute indicators
        df = self.compute_indicators(stage2_data)

        # Convert date for filtering
        df['Date'] = pd.to_datetime(df['date'])

        # Filter to D0 range
        df_d0 = df[
            (df['Date'] >= pd.Timestamp(self.d0_start)) &
            (df['Date'] <= pd.Timestamp(self.d0_end))
        ].copy()

        print(f"üìä Rows after D0 filter: {{len(df_d0):,}}")

        # ==================== PATTERN DETECTION ====================
        print(f"\\nüîç DETECTING {{len(self.pattern_assignments)}} PATTERNS...")

        # Initialize pattern columns
        for pattern in self.pattern_assignments:
            df_d0[pattern['name']] = 0

        # Apply each pattern detection logic
        for i, pattern in enumerate(self.pattern_assignments, 1):
            pattern_name = pattern['name']
            pattern_logic = pattern['logic']

            print(f"  [{{i}}/{{len(self.pattern_assignments)}}] Detecting {{pattern_name}}...")

            try:
                # Evaluate the pattern logic
                # Note: This is a simplified version - in production, you'd want
                # to parse and convert the logic to proper pandas operations
                mask = df_d0.eval(pattern_logic)
                df_d0.loc[mask, pattern_name] = 1

                signal_count = mask.sum()
                print(f"      ‚úÖ Found {{signal_count}} signals")

            except Exception as e:
                print(f"      ‚ö†Ô∏è  Error detecting {{pattern_name}}: {{e}}")
                # Continue with next pattern

        # ==================== AGGREGATE RESULTS ====================
        print(f"\\nüìä AGGREGATING RESULTS...")

        # Find all rows where ANY pattern matched
        pattern_columns = [p['name'] for p in self.pattern_assignments]
        df_d0['any_pattern'] = df_d0[pattern_columns].any(axis=1)

        # Filter to only rows with matches
        signals = df_d0[df_d0['any_pattern'] == True].copy()

        if signals.empty:
            print("‚ùå No signals found!")
            return pd.DataFrame()

        # Build pattern labels for each signal
        def get_pattern_labels(row):
            matched_patterns = []
            for pattern in self.pattern_assignments:
                if row[pattern['name']] == 1:
                    matched_patterns.append(pattern['name'])
            return ', '.join(matched_patterns)

        signals['Scanner_Label'] = signals.apply(get_pattern_labels, axis=1)

        # Aggregate by ticker+date
        signals_aggregated = signals.groupby(['ticker', 'Date'])['Scanner_Label'].apply(
            lambda x: ', '.join(sorted(set(x)))
        ).reset_index()
        signals_aggregated.columns = ['Ticker', 'Date', 'Scanner_Label']

        print(f"üìä Unique ticker+date combinations: {{len(signals_aggregated):,}}")
        print(f"üìä Total pattern matches (including duplicates): {{len(signals):,}}")

        # Print pattern distribution
        pattern_counts = signals['Scanner_Label'].value_counts()
        print(f"\\nüìä Signals by Pattern:")
        for pattern, count in pattern_counts.items():
            print(f"  ‚Ä¢ {{pattern}}: {{count}}")

        return signals_aggregated

    # ==================== STAGE 4: FORMAT RESULTS ====================

    def format_results(
        self,
        stage3_results: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Stage 4: Format results for display

        Args:
            stage3_results: Output from detect_patterns

        Returns:
            Formatted DataFrame with results
        """
        if stage3_results.empty:
            return pd.DataFrame()

        # Sort by date (chronological order)
        output = stage3_results.sort_values('Date').reset_index(drop=True)

        return output

    # ==================== STAGE 5: RUN SCAN ====================

    def run_scan(
        self,
        start_date: str = None,
        end_date: str = None
    ) -> pd.DataFrame:
        """
        Stage 5: Run complete scan pipeline

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)

        Returns:
            Formatted results DataFrame
        """
        # Use configured dates if not provided
        start_date = start_date or self.scan_start
        end_date = end_date or self.scan_end

        print(f"\\n{{'='*70}}")
        print(f"üöÄ MULTI-PATTERN SCANNER: {scanner_name}")
        print(f"{{'='*70}}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data(start_date, end_date)

        if stage1_data.empty:
            print("‚ùå No data available!")
            return pd.DataFrame()

        # Stage 2: Apply smart filters
        stage2_data = self.apply_smart_filters(stage1_data)

        # Stage 3: Detect patterns
        stage3_results = self.detect_patterns(stage2_data)

        if stage3_results.empty:
            print("‚ùå No signals found!")
            return pd.DataFrame()

        # Stage 4: Format results
        formatted_results = self.format_results(stage3_results)

        # Print results
        print(f"\\n{{'='*70}}")
        print(f"‚úÖ SCAN COMPLETE")
        print(f"{{'='*70}}")
        print(f"üìä Final signals (D0 range): {{len(formatted_results):,}}")
        print(f"üìä Unique tickers: {{formatted_results['Ticker'].nunique():,}}")

        # Print all results
        if len(formatted_results) > 0:
            print(f"\\n{{'='*70}}")
            print("üìä ALL SIGNALS:")
            print(f"{{'='*70}}")
            for idx, row in formatted_results.iterrows():
                print(f"  {{row['Ticker']:6s}} | {{row['Date']}} | {{row['Scanner_Label']}}")

        return formatted_results

    def run_and_save(self, output_path: str = "multi_scanner_results.csv") -> pd.DataFrame:
        """Execute scan and save results"""
        results = self.run_scan()

        if not results.empty:
            results.to_csv(output_path, index=False)
            print(f"‚úÖ Results saved to: {{output_path}}")

        return results


# ==================== MAIN ENTRY POINT ====================

if __name__ == "__main__":
    import sys

    scanner = {scanner_name}()

    results = scanner.run_and_save()

    print(f"\\n‚úÖ Done!")
'''

        print("\n‚úÖ V31 MULTI-SCANNER CODE GENERATED")
        print(f"   Class: {scanner_name}")
        print(f"   Patterns: {len(pattern_assignments)}")
        print(f"   Lines: {len(v31_code.split(chr(10)))}")

        # Post-process: Replace __SELF__ placeholder with self
        v31_code = v31_code.replace('__SELF__', 'self')

        return v31_code

    def _extract_standalone_components(self, source_code: str) -> dict:
        """
        Extract components from standalone scanner code

        Extracts:
        - imports
        - config (P dict)
        - symbols (SYMBOLS list)
        - fetch_daily function
        - add_daily_metrics function
        - scan_symbol function
        - helper functions

        Args:
            source_code: Standalone scanner code

        Returns:
            Dictionary with extracted components
        """
        import re

        lines = source_code.split('\n')
        components = {
            'imports': '',
            'config': 'P = {}',
            'symbols': 'SYMBOLS = []',
            'fetch_daily': 'pass',
            'add_daily_metrics': 'pass',
            'scan_symbol': 'return []',
            'helpers': ''
        }

        # Extract imports
        import_lines = []
        for i, line in enumerate(lines):
            stripped = line.strip()
            if stripped.startswith('import ') or stripped.startswith('from '):
                import_lines.append(line)
            elif import_lines and not stripped.startswith(('import ', 'from ')) and stripped:
                # Stop at first non-import, non-empty line
                break

        if import_lines:
            components['imports'] = '\n'.join(import_lines)

        # Extract config (P dict)
        config_match = re.search(r'P\s*=\s*\{[^}]*\}', source_code, re.DOTALL)
        if config_match:
            components['config'] = config_match.group(0)

        # Extract SYMBOLS list
        symbols_match = re.search(r'SYMBOLS\s*=\s*\[[^\]]*\]', source_code, re.DOTALL)
        if symbols_match:
            # Extract and format properly
            symbols_text = symbols_match.group(0)
            # Limit to first 100 symbols for readability
            if len(symbols_text) > 2000:
                # Truncate but keep structure
                symbols_text = symbols_text[:2000] + '...\n    # ... (truncated for readability)'
            components['symbols'] = symbols_text

        # Extract fetch_daily function - need to extract body, not just signature
        fetch_match = re.search(
            r'def\s+fetch_daily\s*\([^)]*\)\s*->\s*[^:]*:(.*?)(?=\ndef\s|\nclass\s|#\s+‚îÄ‚îÄ‚îÄ‚îÄ|$)',
            source_code,
            re.DOTALL
        )
        if not fetch_match:
            # Try without return type
            fetch_match = re.search(
                r'def\s+fetch_daily\s*\([^)]*\):(.*?)(?=\ndef\s|\nclass\s|#\s+‚îÄ‚îÄ‚îÄ‚îÄ|$)',
                source_code,
                re.DOTALL
            )

        if fetch_match:
            # Extract function body and indent properly
            body = fetch_match.group(1).strip()
            if body:
                # Add proper indentation for class method
                indented_body = '\n'.join('        ' + line if line.strip() else line for line in body.split('\n'))
                components['fetch_daily'] = indented_body
            else:
                components['fetch_daily'] = 'pass'

        # Extract add_daily_metrics function
        metrics_match = re.search(
            r'def\s+add_daily_metrics\s*\([^)]*\)\s*->\s*[^:]*:(.*?)(?=\ndef\s|\nclass\s|#\s+‚îÄ‚îÄ‚îÄ‚îÄ|$)',
            source_code,
            re.DOTALL
        )
        if not metrics_match:
            # Try without return type
            metrics_match = re.search(
                r'def\s+add_daily_metrics\s*\([^)]*\):(.*?)(?=\ndef\s|\nclass\s|#\s+‚îÄ‚îÄ‚îÄ‚îÄ|$)',
                source_code,
                re.DOTALL
            )

        if metrics_match:
            body = metrics_match.group(1).strip()
            if body:
                indented_body = '\n'.join('        ' + line if line.strip() else line for line in body.split('\n'))
                components['add_daily_metrics'] = indented_body
            else:
                components['add_daily_metrics'] = 'return df'

        # Extract scan_symbol function
        scan_match = re.search(
            r'def\s+scan_symbol\s*\([^)]*\)\s*->\s*[^:]*:(.*?)(?=\ndef\s|\nclass\s|if __name__|#\s+‚îÄ‚îÄ‚îÄ‚îÄ|main\s*=)',
            source_code,
            re.DOTALL
        )
        if not scan_match:
            # Try without return type
            scan_match = re.search(
                r'def\s+scan_symbol\s*\([^)]*\):(.*?)(?=\ndef\s|\nclass\s|if __name__|#\s+‚îÄ‚îÄ‚îÄ‚îÄ|main\s*=)',
                source_code,
                re.DOTALL
            )

        if scan_match:
            body = scan_match.group(1).strip()
            if body:
                indented_body = '\n'.join('    ' + line if line.strip() else line for line in body.split('\n'))
                components['scan_symbol'] = indented_body
            else:
                components['scan_symbol'] = 'return []'

        # Extract helper functions
        helpers = []
        for match in re.finditer(
            r'def\s+(?!fetch_daily|add_daily_metrics|scan_symbol)(\w+)\s*\([^)]*\)[^:]*:.*?(?=\ndef\s|\nclass\s|if __name__|$)',
            source_code,
            re.DOTALL
        ):
            helpers.append(match.group(0))

        if helpers:
            components['helpers'] = '\n\n'.join(helpers)

        return components

    def _strip_thinking_text(self, code: str) -> str:
        """
        Strip AI thinking/reasoning text from generated code

        Looks for common patterns like:
        - Text before 'import' statements
        - Text before 'class' definitions
        - Text before '```python' markdown blocks
        - Lines that look like thinking/explanation

        Args:
            code: Generated code that may contain thinking text

        Returns:
            Clean code with only Python code
        """
        if not code:
            return code

        lines = code.split('\n')
        code_start_idx = 0

        # Look for the start of actual Python code
        for i, line in enumerate(lines):
            stripped = line.strip()

            # Skip empty lines and comments that look like thinking
            if not stripped:
                continue

            # If it's a comment that looks like thinking/explanation, skip it
            if stripped.startswith('#') and any(keyword in stripped.lower() for keyword in [
                'okay', 'let\'s', 'first', 'next', 'then', 'wait', 'looking at',
                'i need', 'i\'ll', 'this is', 'the issue', 'the problem',
                'hmm', 'maybe', 'alternatively', 'putting', 'another thing'
            ]):
                continue

            # Found actual code start
            if stripped.startswith('import ') or \
               stripped.startswith('from ') or \
               stripped.startswith('class ') or \
               stripped.startswith('def ') or \
               stripped.startswith('"""') or \
               stripped.startswith("'''") or \
               stripped.startswith('```python') or \
               stripped.startswith('```'):
                code_start_idx = i
                break

        # Extract only the code portion
        clean_lines = lines[code_start_idx:] if code_start_idx > 0 else lines

        # Remove ```python and ``` markers if present at the start/end
        if clean_lines and clean_lines[0].strip().startswith('```'):
            # Find the end of the opening code block
            for i, line in enumerate(clean_lines):
                if line.strip() == '```' or (line.strip().startswith('```') and 'python' not in line.strip()):
                    # This is the closing marker, everything before is code
                    clean_lines = clean_lines[1:i] if i > 1 else clean_lines[1:]
                    break

        # Remove closing ``` if present
        if clean_lines and clean_lines[-1].strip() == '```':
            clean_lines = clean_lines[:-1]

        return '\n'.join(clean_lines)

    def save_to_file(
        self,
        result: TransformationResult,
        output_path: str
    ) -> None:
        """
        Save transformation result to file

        Args:
            result: TransformationResult
            output_path: Path to output file
        """
        if not result.success:
            raise TransformationError(
                "Cannot save failed transformation. "
                "Fix errors before saving."
            )

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            f.write(result.generated_code)

        print(f"\n‚úì Saved to: {output_path}")

