# Template System for Renata V2

## ğŸ“‹ Overview

The Template System is the third stage in the Renata V2 transformation pipeline. It uses Jinja2 templates to **enforce v31 structure**, ensuring all generated scanners follow the exact same architecture.

---

## ğŸ¯ Why Templates?

### The Problem with Pure AI Generation

**Issue**: AI is creative, but creativity is bad for structure

```python
# AI generates this code:
def scan_stocks(data):
    results = []
    for stock in data:
        if stock['price'] > stock['ma']:
            results.append(stock)
    return results

# âŒ Not v31 compliant
# âŒ No fetch_grouped_data method
# âŒ No apply_smart_filters method
# âŒ No proper structure
# âŒ Won't work in EdgeDev ecosystem
```

### The Solution: Template Enforcement

```python
# Template guarantees this structure:
class ScannerNameScanner:
    def fetch_grouped_data(self, start_date, end_date):
        # âœ… GUARANTEED: Polygon API integration
        pass

    def apply_smart_filters(self, stage1_data):
        # âœ… GUARANTEED: Smart filtering
        pass

    def detect_patterns(self, stage2_data):
        # ğŸ¤– AI GENERATED: Pattern-specific logic
        pass

    def run_scan(self, start_date, end_date):
        # âœ… GUARANTEED: Orchestration
        pass
```

### Benefits

1. **100% Structure Compliance**: Every scanner has exact same structure
2. **Validation Friendly**: Predictable structure = easy validation
3. **Maintainable**: Change template, update all scanners
4. **Fast**: Don't need to generate entire scanner with AI
5. **Reliable**: Templates never have syntax errors

---

## ğŸ—ï¸ Jinja2 Setup

### Installation

```bash
pip install jinja2
```

### Basic Configuration

```python
# RENATA_V2/core/template_engine.py

from jinja2 import Environment, FileSystemLoader, StrictUndefined

class TemplateEngine:
    """Jinja2 template engine for v31 code generation"""

    def __init__(self, template_dir: str):
        """
        Initialize template engine

        Args:
            template_dir: Path to templates directory
        """
        self.env = Environment(
            loader=FileSystemLoader(template_dir),
            undefined=StrictUndefined,  # Catch undefined variables
            trim_blocks=True,  # Remove newline after blocks
            lstrip_blocks=True  # Strip leading whitespace
        )

        # Add custom filters if needed
        self.env.filters['snake_case'] = self._snake_case_filter
        self.env.filters['pascal_case'] = self._pascal_case_filter

    def render(self, template_name: str, **context) -> str:
        """
        Render template with context

        Args:
            template_name: Name of template file
            **context: Template variables

        Returns:
            Rendered code
        """
        template = self.env.get_template(template_name)
        return template.render(**context)

    @staticmethod
    def _snake_case_filter(text: str) -> str:
        """Convert text to snake_case"""
        import re
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    @staticmethod
    def _pascal_case_filter(text: str) -> str:
        """Convert text to PascalCase"""
        return ''.join(word.capitalize() for word in text.split('_'))
```

---

## ğŸ“ Template Hierarchy

### Directory Structure

```
RENATA_V2/templates/
â”œâ”€â”€ base.j2                      # Base v31 structure
â”œâ”€â”€ single_scanner.j2           # Single-scanner template
â”œâ”€â”€ multi_scanner.j2            # Multi-scanner template
â””â”€â”€ components/                  # Reusable components
    â”œâ”€â”€ fetch_data.j2
    â”œâ”€â”€ smart_filters.j2
    â”œâ”€â”€ detect_patterns.j2
    â”œâ”€â”€ format_results.j2
    â””â”€â”€ orchestration.j2
```

### Template Inheritance

```
base.j2 (Base structure)
    â”œâ”€â”€ single_scanner.j2 (Extends base)
    â”‚   â””â”€â”€ Includes components/
    â”‚
    â””â”€â”€ multi_scanner.j2 (Extends base)
        â””â”€â”€ Includes components/
```

---

## ğŸ”§ Base Template

### `base.j2`

```python
# RENATA_V2/templates/base.j2

"""
{{ scanner_name }} - EdgeDev v31 Standard Scanner

{{ description }}

Generated by Renata V2
Scanner Type: {{ scanner_type|upper }}
Generation Date: {{ generation_date }}
"""

import os
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta


class {{ scanner_name }}Scanner:
    """
    EdgeDev v31 Standard Scanner

    {{ description }}

    Scanner Type: {{ scanner_type }}
    Generated by: Renata V2
    """

    def __init__(self):
        """Initialize scanner with configuration"""
        self.stage1_workers = {{ stage1_workers|default(5) }}
        self.stage3_workers = {{ stage3_workers|default(10) }}
        self.api_key = os.getenv('POLYGON_API_KEY')
        self.base_url = "https://api.polygon.io"
        self.user_agent = "EdgeDev/{{ scanner_name }}/1.0"

        # Validate API key
        if not self.api_key:
            raise ValueError("POLYGON_API_KEY environment variable not set")

    {% block fetch_data %}{% endblock %}

    {% block smart_filters %}{% endblock %}

    {% block detect_patterns %}{% endblock %}

    {% block format_results %}{% endblock %}

    {% block orchestration %}{% endblock %}
```

---

## ğŸ“ Single-Scanner Template

### `single_scanner.j2`

```python
# RENATA_V2/templates/single_scanner.j2

{% extends "base.j2" %}

{% block fetch_data %}
def fetch_grouped_data(self, start_date: str, end_date: str) -> pd.DataFrame:
    """
    STAGE 1: Fetch ALL tickers that traded each day

    Uses Polygon grouped endpoint for full market coverage.
    Parallel processing for efficiency.

    Args:
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)

    Returns:
        DataFrame with columns: [date, ticker, open, high, low, close, volume]
    """
    # Generate date range
    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')
    dates = [
        (start + timedelta(days=i)).strftime('%Y-%m-%d')
        for i in range((end - start).days + 1)
    ]

    # Fetch data in parallel
    all_data = []

    with ThreadPoolExecutor(max_workers=self.stage1_workers) as executor:
        futures = {
            executor.submit(self._fetch_date, date): date
            for date in dates
        }

        for future in as_completed(futures):
            date = futures[future]
            try:
                daily_data = future.result()
                if daily_data:
                    all_data.extend(daily_data)
                    print(f"âœ… Fetched {len(daily_data)} tickers for {date}")
            except Exception as e:
                print(f"âŒ Error fetching {date}: {e}")

    # Convert to DataFrame
    df = pd.DataFrame(all_data)

    if df.empty:
        print("âš ï¸  No data fetched")
        return pd.DataFrame()

    # Add previous day data for gap calculations
    df = self._add_previous_day_data(df)

    return df

def _fetch_date(self, date: str) -> List[Dict]:
    """Fetch data for a single date"""
    url = f"{self.base_url}/v2/aggs/grouped/locale/us/market/stocks/{date}"
    params = {
        'adjusted': 'true',
        'include_otc': 'false',
        'apikey': self.api_key
    }
    headers = {
        'User-Agent': self.user_agent
    }

    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        if data.get('status') != 'OK':
            return []

        results = data.get('results', [])
        if not results:
            return []

        # Extract relevant fields
        extracted = []
        for item in results:
            extracted.append({
                'date': date,
                'ticker': item['T'],
                'open': item['o'],
                'high': item['h'],
                'low': item['l'],
                'close': item['c'],
                'volume': item['v']
            })

        return extracted

    except Exception as e:
        print(f"Error fetching {date}: {e}")
        return []

def _add_previous_day_data(self, df: pd.DataFrame) -> pd.DataFrame:
    """Add previous day's data for gap calculations"""
    df = df.sort_values(['ticker', 'date'])

    # Shift data to get previous day
    df['prev_close'] = df.groupby('ticker')['close'].shift(1)
    df['prev_volume'] = df.groupby('ticker')['volume'].shift(1)
    df['prev_high'] = df.groupby('ticker')['high'].shift(1)
    df['prev_low'] = df.groupby('ticker')['low'].shift(1)

    # Calculate gap
    df['gap_pct'] = (df['open'] / df['prev_close']) - 1

    # Calculate range
    df['range'] = df['high'] - df['low']
    df['prev_range'] = df.groupby('ticker')['range'].shift(1)

    return df
{% endblock %}

{% block smart_filters %}
def apply_smart_filters(self, stage1_data: pd.DataFrame) -> pd.DataFrame:
    """
    STAGE 2: Reduce dataset by 99% with smart filters

    Apply price and volume filters to efficiently reduce dataset.

    Args:
        stage1_data: Output from fetch_grouped_data

    Returns:
        Filtered DataFrame (typically 1-2% of original size)
    """
    if stage1_data.empty:
        return stage1_data

    print(f"ğŸ“Š Stage 1: {len(stage1_data):,} rows")

    # Remove rows without previous day data
    filtered = stage1_data.dropna(subset=['prev_close', 'prev_volume']).copy()

    # Apply price filters
    filtered = filtered[
        (filtered['prev_close'] >= {{ smart_filters.min_price|default(0.75) }})
    ]

    # Apply volume filters
    filtered = filtered[
        (filtered['prev_volume'] >= {{ smart_filters.min_volume|default(1_000_000) }})
    ]

    # Apply gap filters (optional)
    {% if smart_filters.min_gap is defined %}
    filtered = filtered[
        (filtered['gap_pct'] >= {{ smart_filters.min_gap }})
    ]
    {% endif %}

    print(f"âœ… Stage 2: {len(filtered):,} rows ({len(filtered)/len(stage1_data)*100:.1f}%)")

    return filtered
{% endblock %}

{% block detect_patterns %}
def detect_patterns(self, stage2_data: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    STAGE 3: Pattern detection logic

    AI-generated strategy logic for {{ strategy_name }}.

    Args:
        stage2_data: Filtered DataFrame from apply_smart_filters

    Returns:
        List of pattern results
    """
    if stage2_data.empty:
        return []

    print(f"ğŸ” Scanning for {{ strategy_name }} setup...")

    {{ pattern_logic }}

    print(f"âœ… Found {len(results)} patterns")

    return results
{% endblock %}

{% block format_results %}
def _format_results(self, pattern_results: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    Format pattern results for output

    Args:
        pattern_results: DataFrame of pattern matches

    Returns:
        List of formatted result dictionaries
    """
    results = []

    for idx, row in pattern_results.iterrows():
        result = {
            'ticker': row['ticker'],
            'date': row['date'].strftime('%Y-%m-%d'),
            'entry': float(row['open']),
            'entry_time': '9:30 AM'
        }

        # Add pattern-specific fields
        {% for field in result_fields %}
        if '{{ field }}' in row:
            result['{{ field }}'] = row['{{ field }}']
        {% endfor %}

        results.append(result)

    return results
{% endblock %}

{% block orchestration %}
def run_scan(
    self,
    start_date: str,
    end_date: str
) -> Dict[str, Any]:
    """
    Orchestrate the complete scan workflow

    Coordinates all stages:
    1. Fetch grouped data
    2. Apply smart filters
    3. Detect patterns
    4. Format results

    Args:
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)

    Returns:
        Dictionary with scan results
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ {{ scanner_name }} Scanner")
    print(f"ğŸ“… {start_date} to {end_date}")
    print(f"{'='*60}\n")

    # STAGE 1: Fetch data
    print("â³ Stage 1: Fetching grouped data...")
    stage1_data = self.fetch_grouped_data(start_date, end_date)

    if stage1_data.empty:
        return {
            'success': False,
            'message': 'No data fetched',
            'results': []
        }

    # STAGE 2: Apply smart filters
    print("\nâ³ Stage 2: Applying smart filters...")
    stage2_data = self.apply_smart_filters(stage1_data)

    if stage2_data.empty:
        return {
            'success': True,
            'message': 'No stocks passed smart filters',
            'results': []
        }

    # STAGE 3: Detect patterns
    print("\nâ³ Stage 3: Detecting patterns...")
    pattern_results = self.detect_patterns(stage2_data)

    # Format results
    print("\nâœ… Scan complete!")
    print(f"ğŸ“Š Found {len(pattern_results)} results\n")

    return {
        'success': True,
        'scanner': '{{ scanner_name }}',
        'date_range': f"{start_date} to {end_date}",
        'results': pattern_results,
        'metadata': {
            'stage1_rows': len(stage1_data),
            'stage2_rows': len(stage2_data),
            'pattern_matches': len(pattern_results)
        }
    }
{% endblock %}
```

---

## ğŸ“ Multi-Scanner Template

### `multi_scanner.j2`

```python
# RENATA_V2/templates/multi_scanner.j2

{% extends "base.j2" %}

{% block fetch_data %}
# Same as single_scanner.j2
{% endblock %}

{% block smart_filters %}
def apply_smart_filters(
    self,
    stage1_data: pd.DataFrame,
    pattern_filters: Dict[str, Any]
) -> pd.DataFrame:
    """
    STAGE 2: Apply pattern-specific smart filters

    KEY: Each pattern gets its OWN filters based on its parameters!

    Args:
        stage1_data: Output from fetch_grouped_data
        pattern_filters: Filter parameters for this specific pattern

    Returns:
        Filtered DataFrame
    """
    if stage1_data.empty:
        return stage1_data

    # Remove rows without previous day data
    filtered = stage1_data.dropna(
        subset=['prev_close', 'prev_volume']
    ).copy()

    # Apply price filters
    {% if pattern_filters.min_price is defined %}
    filtered = filtered[
        (filtered['prev_close'] >= {{ pattern_filters.min_price }})
    ]
    {% endif %}

    # Apply volume filters
    {% if pattern_filters.min_volume is defined %}
    filtered = filtered[
        (filtered['prev_volume'] >= {{ pattern_filters.min_volume }})
    ]
    {% endif %}

    # Apply gap filters
    {% if pattern_filters.min_gap is defined %}
    filtered = filtered[
        (filtered['gap_pct'] >= {{ pattern_filters.min_gap }})
    ]
    {% endif %}

    # Apply pattern-specific filters
    {% if pattern_filters.custom_filters is defined %}
    {% for filter in pattern_filters.custom_filters %}
    filtered = filtered[
        (filtered['{{ filter.column }}'] {{ filter.operator }} {{ filter.value }})
    ]
    {% endfor %}
    {% endif %}

    return filtered
{% endblock %}

{% block detect_patterns %}
def detect_patterns(
    self,
    stage1_data: pd.DataFrame
) -> Dict[str, List[Dict[str, Any]]]:
    """
    STAGE 3: Multi-pattern detection with specific filters

    For each pattern:
    1. Apply pattern-specific smart filters
    2. Calculate pattern-specific indicators
    3. Check pattern conditions
    4. Format results

    Args:
        stage1_data: Output from fetch_grouped_data

    Returns:
        Dictionary of results grouped by pattern
    """
    if stage1_data.empty:
        return {}

    results_by_pattern = {}

    # For each pattern
    {% for pattern in patterns %}
    # Pattern: {{ pattern.name }}
    print(f"ğŸ” Scanning for {{ pattern.name }}...")

    # Apply pattern-specific smart filters
    {{ pattern.name }}_stage2 = self.apply_smart_filters(
        stage1_data,
        {{ pattern.name }}_filters  # Different for each pattern!
    )

    # Calculate pattern-specific indicators
    {{ pattern.name }}_stage2 = self._calculate_{{ pattern.name }}_indicators(
        {{ pattern.name }}_stage2
    )

    # Check if {{ pattern.name }} conditions are met
    {{ pattern.name }}_mask = self._check_{{ pattern.name }}_conditions(
        {{ pattern.name }}_stage2
    )

    {{ pattern.name }}_results = {{ pattern.name }}_stage2[{{ pattern.name }}_mask]

    # Format results
    results_by_pattern['{{ pattern.name }}'] = \
        self._format_{{ pattern.name }}_results({{ pattern.name }}_results)

    print(f"âœ… Found {len(results_by_pattern['{{ pattern.name }}'])} {{ pattern.name }} patterns")
    {% endfor %}

    return results_by_pattern

{% for pattern in patterns %}
def _calculate_{{ pattern.name }}_indicators(
    self,
    data: pd.DataFrame
) -> pd.DataFrame:
    """
    Calculate indicators specific to {{ pattern.name }}

    Different patterns need different indicators!
    """
    {{ pattern.indicator_logic }}

    return data

def _check_{{ pattern.name }}_conditions(
    self,
    data: pd.DataFrame
) -> pd.Series:
    """
    Check if {{ pattern.name }} conditions are met

    Returns:
        Boolean series indicating pattern matches
    """
    {{ pattern.condition_logic }}

    return pattern_mask

def _format_{{ pattern.name }}_results(
    self,
    results: pd.DataFrame
) -> List[Dict[str, Any]]:
    """Format {{ pattern.name }} results"""
    formatted = []

    for idx, row in results.iterrows():
        formatted.append({
            'ticker': row['ticker'],
            'date': row['date'].strftime('%Y-%m-%d'),
            'entry': float(row['open']),
            'pattern': '{{ pattern.name }}'
            {% for field in pattern.result_fields %}
            , '{{ field }}': row['{{ field }}']
            {% endfor %}
        })

    return formatted
{% endfor %}
{% endblock %}
```

---

## ğŸ”§ Component Templates

### `components/fetch_data.j2`

```python
# RENATA_V2/templates/components/fetch_data.j2

def fetch_grouped_data(
    self,
    start_date: str,
    end_date: str
) -> pd.DataFrame:
    """
    Fetch all tickers that traded each day using Polygon grouped endpoint
    """
    # Implementation...
    pass
```

### `components/smart_filters.j2`

```python
# RENATA_V2/templates/components/smart_filters.j2

def apply_smart_filters(
    self,
    stage1_data: pd.DataFrame
    {%- if pattern_filters %}, pattern_filters: Dict[str, Any]{% endif %}
) -> pd.DataFrame:
    """
    Apply smart filters to reduce dataset by 99%
    """
    # Implementation...
    pass
```

---

## ğŸ’» Usage Examples

### Rendering Single-Scanner

```python
# Example: Transform A+ Parabolic scanner

from RENATA_V2.core.template_engine import TemplateEngine

engine = TemplateEngine('RENATA_V2/templates')

# Render template
v31_code = engine.render(
    'single_scanner.j2',
    scanner_name='APlusParabolic',
    scanner_type='single',
    description='A+ Daily Parabolic momentum scanner',
    generation_date='2025-01-02',
    stage1_workers=5,
    stage3_workers=10,
    strategy_name='A+ Parabolic',
    smart_filters={
        'min_price': 0.75,
        'min_volume': 5_000_000,
        'min_gap': 0.5
    },
    pattern_logic=ai_generated_pattern_logic,
    result_fields=['gap_pct', 'volume', 'ema9', 'ema21']
)

# Save to file
with open('APlusParabolicScanner.py', 'w') as f:
    f.write(v31_code)
```

### Rendering Multi-Scanner

```python
# Example: Transform SC DMR scanner

# Define patterns
patterns = [
    {
        'name': 'd2_pm_setup',
        'filters': {
            'min_price': 0.75,
            'min_volume': 10_000_000,
            'min_gap': 0.5
        },
        'indicator_logic': '''# Calculate D2 PM Setup indicators
data['pct_pmh_gap'] = (data['open'] / data['prev_high']) - 1
data['close_range'] = (data['close'] - data['low']) / data['range']''',
        'condition_logic': '''pattern_mask = (
    (data['pct_pmh_gap'] >= 0.5) &
    (data['close_range'] >= 0.5)
)''',
        'result_fields': ['pct_pmh_gap', 'close_range']
    },
    # ... more patterns
]

# Render template
v31_code = engine.render(
    'multi_scanner.j2',
    scanner_name='DMRMultiScanner',
    scanner_type='multi',
    patterns=patterns,
    # ... other variables
)
```

---

## ğŸ¨ Custom Filters

### String Filters

```python
# Add custom filters to template engine

def setup_custom_filters(env: Environment):
    """Setup custom Jinja2 filters"""

    @env.filter('snake_case')
    def snake_case(text):
        """Convert to snake_case"""
        import re
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    @env.filter('pascal_case')
    def pascal_case(text):
        """Convert to PascalCase"""
        return ''.join(word.capitalize() for word in text.split('_'))

    @env.filter('commas')
    def commas(number):
        """Format number with commas"""
        return f"{number:,}"

# Usage in templates:
{{ scanner_name|pascal_case }}Scanner  # APlusParabolicScanner
{{ min_price|commas }}  # 1,000,000
```

### Global Functions

```python
# Add global functions

def setup_globals(env: Environment):
    """Setup global variables and functions"""

    env.globals['now'] = datetime.now
    env.globals['version'] = '2.0'

# Usage in templates:
Generated: {{ now().strftime('%Y-%m-%d') }}
Version: {{ version }}
```

---

## ğŸ§ª Testing Templates

### Unit Tests

```python
# tests/test_templates.py

import pytest
from RENATA_V2.core.template_engine import TemplateEngine

def test_render_single_scanner():
    """Test single-scanner template rendering"""
    engine = TemplateEngine('RENATA_V2/templates')

    code = engine.render(
        'single_scanner.j2',
        scanner_name='TestScanner',
        scanner_type='single',
        description='Test scanner',
        generation_date='2025-01-02',
        stage1_workers=5,
        stage3_workers=10,
        strategy_name='Test Strategy',
        smart_filters={'min_price': 0.75},
        pattern_logic='return []',
        result_fields=[]
    )

    # Verify it's valid Python
    import ast
    try:
        ast.parse(code)
    except SyntaxError as e:
        pytest.fail(f"Generated code has syntax error: {e}")

    # Verify it contains required methods
    assert 'def fetch_grouped_data(' in code
    assert 'def apply_smart_filters(' in code
    assert 'def detect_patterns(' in code
    assert 'def run_scan(' in code

def test_render_multi_scanner():
    """Test multi-scanner template rendering"""
    engine = TemplateEngine('RENATA_V2/templates')

    patterns = [
        {
            'name': 'pattern1',
            'filters': {'min_price': 0.75},
            'indicator_logic': 'pass',
            'condition_logic': 'pattern_mask = pd.Series([False], index=data.index)',
            'result_fields': []
        }
    ]

    code = engine.render(
        'multi_scanner.j2',
        scanner_name='TestMultiScanner',
        scanner_type='multi',
        patterns=patterns,
        # ... other variables
    )

    # Verify it's valid Python
    import ast
    ast.parse(code)

    # Verify multi-pattern structure
    assert 'results_by_pattern' in code
    assert 'for pattern in self.patterns' in code
```

---

## ğŸ“ Best Practices

### DO:
âœ… Use template inheritance (DRY principle)
âœ… Define all variables with defaults
âœ… Include type hints
âœ… Add comprehensive docstrings
âœ… Validate generated code
âœ… Test templates thoroughly

### DON'T:
âŒ Don't hardcode values in templates
âŒ Don't repeat code (use components)
âŒ Don't forget error handling
âŒ Don't skip validation
âŒ Don't use undefined variables

---

## ğŸ¯ Key Takeaways

1. **Templates Enforce Structure**: 100% v31 compliance
2. **AI Generates Logic**: Only pattern detection logic
3. **Templates Handle Architecture**: Fetch, filter, orchestrate
4. **Jinja2 is Powerful**: Inheritance, filters, globals
5. **Test Thoroughly**: Validate all generated code
6. **Keep It Simple**: Don't over-engineer templates
7. **Document Well**: Clear comments in templates

---

## ğŸ“š References

- [Jinja2 Documentation](https://jinja.palletsprojects.com/)
- [Jinja2 Template Designer Documentation](https://jinja.palletsprojects.com/templates/)

---

**Version**: 2.0
**Last Updated**: 2025-01-02
