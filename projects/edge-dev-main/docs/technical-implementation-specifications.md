# Technical Implementation Specifications
## CE-Hub AI-Powered Multi-Scanner System

**Version**: 1.0
**Date**: November 11, 2025
**Engineer**: CE-Hub Technical Team

---

## 1. Core Algorithm Designs

### 1.1 AI Boundary Detection Algorithm

#### 1.1.1 Multi-Layer Detection Engine
```python
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from enum import Enum
import ast
import re
from abc import ABC, abstractmethod

class BoundaryType(Enum):
    FUNCTION_BASED = "function_based"
    COMMENT_BASED = "comment_based"
    SEMANTIC_BASED = "semantic_based"
    PATTERN_BASED = "pattern_based"

class BoundaryValidationStatus(Enum):
    VALID = "valid"
    NEEDS_REVIEW = "needs_review"
    INVALID = "invalid"

@dataclass
class ScannerBoundary:
    """Detected scanner boundary with validation metadata"""
    scanner_name: str
    start_line: int
    end_line: int
    confidence_score: float
    boundary_type: BoundaryType
    detected_patterns: List[str]
    parameter_namespace: str
    dependencies: List[str]
    validation_status: BoundaryValidationStatus
    context_metadata: Dict[str, any]

class BoundaryDetectionStrategy(ABC):
    """Abstract base class for boundary detection strategies"""

    @abstractmethod
    async def detect_boundaries(self, code: str) -> List[ScannerBoundary]:
        pass

    @abstractmethod
    def get_confidence_weight(self) -> float:
        pass

class ASTStructuralDetector(BoundaryDetectionStrategy):
    """AST-based structural boundary detection"""

    def __init__(self):
        self.function_patterns = [
            r'def\s+(\w+_scanner|scan_\w+|.*_scan)\s*\(',
            r'def\s+(lc_|a_plus_|half_a_plus_)',
            r'def\s+(\w+)\s*\(.*symbols.*\)',
        ]

    async def detect_boundaries(self, code: str) -> List[ScannerBoundary]:
        boundaries = []

        try:
            tree = ast.parse(code)
            lines = code.split('\n')

            # Detect function-based boundaries
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    if self._is_scanner_function(node.name, lines, node.lineno):
                        boundary = await self._create_function_boundary(
                            node, lines, code
                        )
                        if boundary:
                            boundaries.append(boundary)

        except Exception as e:
            logger.warning(f"AST structural detection failed: {e}")

        return boundaries

    def _is_scanner_function(self, func_name: str, lines: List[str], line_no: int) -> bool:
        """Determine if function represents a scanner boundary"""

        # Check function name patterns
        for pattern in self.function_patterns:
            if re.match(pattern, f"def {func_name}("):
                return True

        # Check function context (comments, docstrings)
        context_lines = lines[max(0, line_no-3):line_no+5]
        context = '\n'.join(context_lines).lower()

        scanner_indicators = ['scanner', 'scan', 'pattern', 'strategy', 'backtest']
        return any(indicator in context for indicator in scanner_indicators)

    async def _create_function_boundary(self,
                                      node: ast.FunctionDef,
                                      lines: List[str],
                                      code: str) -> Optional[ScannerBoundary]:
        """Create boundary from function definition"""

        # Calculate function end line
        end_line = self._calculate_function_end_line(node, lines)

        # Extract scanner name from function name
        scanner_name = self._extract_scanner_name(node.name)

        # Analyze function content for patterns
        func_content = '\n'.join(lines[node.lineno-1:end_line])
        detected_patterns = self._analyze_function_patterns(func_content)

        # Calculate confidence score
        confidence = self._calculate_ast_confidence(node, func_content, detected_patterns)

        return ScannerBoundary(
            scanner_name=scanner_name,
            start_line=node.lineno,
            end_line=end_line,
            confidence_score=confidence,
            boundary_type=BoundaryType.FUNCTION_BASED,
            detected_patterns=detected_patterns,
            parameter_namespace=f"scanner_{scanner_name}",
            dependencies=self._extract_function_dependencies(node),
            validation_status=BoundaryValidationStatus.VALID if confidence > 0.8 else BoundaryValidationStatus.NEEDS_REVIEW,
            context_metadata={
                "function_name": node.name,
                "args_count": len(node.args.args),
                "has_docstring": ast.get_docstring(node) is not None
            }
        )

class SemanticAnalysisDetector(BoundaryDetectionStrategy):
    """Semantic analysis for scanner boundary detection"""

    def __init__(self):
        self.pattern_library = PatternLibrary()
        self.semantic_analyzer = SemanticCodeAnalyzer()

    async def detect_boundaries(self, code: str) -> List[ScannerBoundary]:
        boundaries = []

        # Semantic analysis using AI
        semantic_segments = await self._perform_semantic_segmentation(code)

        for segment in semantic_segments:
            boundary = await self._create_semantic_boundary(segment, code)
            if boundary:
                boundaries.append(boundary)

        return boundaries

    async def _perform_semantic_segmentation(self, code: str) -> List[SemanticSegment]:
        """Use AI to perform semantic code segmentation"""

        # This would integrate with OpenAI/OpenRouter API
        prompt = f"""
        Analyze the following Python code and identify distinct scanner/strategy segments.
        Each segment should represent a complete trading scanner or analysis strategy.

        Code to analyze:
        {code}

        Return JSON format:
        {{
            "segments": [
                {{
                    "name": "scanner_name",
                    "start_line": number,
                    "end_line": number,
                    "confidence": 0.0-1.0,
                    "reasoning": "explanation",
                    "patterns": ["list", "of", "detected", "patterns"]
                }}
            ]
        }}
        """

        # Call AI service
        response = await self.semantic_analyzer.analyze(prompt)
        return self._parse_semantic_response(response)

class PatternBasedDetector(BoundaryDetectionStrategy):
    """Pattern-based boundary detection using known scanner patterns"""

    def __init__(self):
        self.known_patterns = {
            'lc_patterns': [
                r'#\s*LC\s+.*',
                r'#\s*Lazy\s+Cat.*',
                r'def.*lc.*',
                r'price.*gap.*ema',
                r'atr.*ema.*slope'
            ],
            'a_plus_patterns': [
                r'#\s*A\+.*',
                r'#\s*A\s+Plus.*',
                r'def.*a_plus.*',
                r'backside.*para.*slope',
                r'volume.*threshold.*breakout'
            ],
            'generic_scanner_patterns': [
                r'#\s*Scanner\s*\d+',
                r'#\s*Strategy\s*:.*',
                r'#\s*Pattern\s*:.*',
                r'def\s+scan_.*',
                r'def\s+.*_scanner.*'
            ]
        }

    async def detect_boundaries(self, code: str) -> List[ScannerBoundary]:
        boundaries = []
        lines = code.split('\n')

        for pattern_type, patterns in self.known_patterns.items():
            type_boundaries = await self._detect_pattern_boundaries(
                lines, patterns, pattern_type
            )
            boundaries.extend(type_boundaries)

        # Merge overlapping boundaries
        merged_boundaries = self._merge_overlapping_boundaries(boundaries)

        return merged_boundaries

class AIBoundaryDetector:
    """Main AI-powered boundary detection coordinator"""

    def __init__(self):
        self.detectors = [
            ASTStructuralDetector(),
            SemanticAnalysisDetector(),
            PatternBasedDetector()
        ]
        self.consensus_engine = BoundaryConsensusEngine()
        self.validation_engine = BoundaryValidationEngine()

    async def detect_scanner_boundaries(self, code: str, filename: str) -> List[ScannerBoundary]:
        """Main boundary detection workflow with consensus"""

        # Run all detection strategies in parallel
        detection_tasks = [
            detector.detect_boundaries(code) for detector in self.detectors
        ]

        detection_results = await asyncio.gather(*detection_tasks, return_exceptions=True)

        # Filter out exceptions and combine results
        valid_results = [
            result for result in detection_results
            if not isinstance(result, Exception)
        ]

        # Apply consensus algorithm
        consensus_boundaries = await self.consensus_engine.resolve_consensus(
            valid_results, code
        )

        # Validate boundaries
        validated_boundaries = await self.validation_engine.validate_boundaries(
            consensus_boundaries, code
        )

        return validated_boundaries

class BoundaryConsensusEngine:
    """Consensus algorithm for combining multiple boundary detection results"""

    async def resolve_consensus(self,
                              detection_results: List[List[ScannerBoundary]],
                              code: str) -> List[ScannerBoundary]:
        """Resolve consensus from multiple detection strategies"""

        if not detection_results:
            return []

        # Flatten all boundaries
        all_boundaries = []
        for result in detection_results:
            all_boundaries.extend(result)

        if not all_boundaries:
            return []

        # Group boundaries by overlap
        boundary_groups = self._group_overlapping_boundaries(all_boundaries)

        # Resolve consensus for each group
        consensus_boundaries = []
        for group in boundary_groups:
            consensus = await self._resolve_group_consensus(group, code)
            if consensus:
                consensus_boundaries.append(consensus)

        return consensus_boundaries

    def _group_overlapping_boundaries(self, boundaries: List[ScannerBoundary]) -> List[List[ScannerBoundary]]:
        """Group boundaries that overlap significantly"""
        groups = []
        used_boundaries = set()

        for i, boundary in enumerate(boundaries):
            if i in used_boundaries:
                continue

            group = [boundary]
            used_boundaries.add(i)

            # Find overlapping boundaries
            for j, other_boundary in enumerate(boundaries[i+1:], i+1):
                if j in used_boundaries:
                    continue

                if self._boundaries_overlap(boundary, other_boundary):
                    group.append(other_boundary)
                    used_boundaries.add(j)

            groups.append(group)

        return groups

    async def _resolve_group_consensus(self,
                                     group: List[ScannerBoundary],
                                     code: str) -> Optional[ScannerBoundary]:
        """Resolve consensus boundary from group of similar boundaries"""

        if len(group) == 1:
            return group[0]

        # Weight boundaries by confidence and detector type
        weighted_scores = []
        for boundary in group:
            weight = self._calculate_boundary_weight(boundary)
            weighted_scores.append((boundary, weight * boundary.confidence_score))

        # Select highest weighted boundary as base
        best_boundary = max(weighted_scores, key=lambda x: x[1])[0]

        # Refine boundary using group information
        refined_boundary = await self._refine_boundary_with_group(best_boundary, group, code)

        return refined_boundary
```

### 1.2 Parameter Isolation Algorithm

#### 1.2.1 Namespace Isolation Engine
```python
from typing import Dict, List, Set
from dataclasses import dataclass, field
import hashlib

@dataclass
class ParameterNamespace:
    """Isolated parameter namespace for a scanner"""
    namespace_id: str
    scanner_name: str
    parameters: Dict[str, ExtractedParameter] = field(default_factory=dict)
    shared_parameters: Set[str] = field(default_factory=set)
    global_dependencies: Set[str] = field(default_factory=set)
    isolation_score: float = 0.0
    contamination_risks: List[str] = field(default_factory=list)

class NamespaceManager:
    """Manages parameter namespaces with contamination prevention"""

    def __init__(self):
        self.namespaces: Dict[str, ParameterNamespace] = {}
        self.global_parameters: Dict[str, ExtractedParameter] = {}
        self.conflict_resolver = ParameterConflictResolver()

    def create_namespace(self, scanner_name: str) -> str:
        """Create isolated namespace for scanner"""
        namespace_id = f"ns_{scanner_name}_{self._generate_namespace_hash(scanner_name)}"

        self.namespaces[namespace_id] = ParameterNamespace(
            namespace_id=namespace_id,
            scanner_name=scanner_name
        )

        return namespace_id

    async def isolate_parameters(self,
                                scanner_name: str,
                                parameters: List[ExtractedParameter]) -> ParameterNamespace:
        """Isolate parameters into scanner-specific namespace"""

        namespace_id = self.create_namespace(scanner_name)
        namespace = self.namespaces[namespace_id]

        # Categorize parameters
        for param in parameters:
            if self._is_global_parameter(param):
                # Handle global parameters (API keys, shared config)
                await self._handle_global_parameter(param, namespace)
            else:
                # Isolate scanner-specific parameter
                await self._isolate_scanner_parameter(param, namespace)

        # Calculate isolation score
        namespace.isolation_score = await self._calculate_isolation_score(namespace)

        # Check for contamination risks
        namespace.contamination_risks = await self._detect_contamination_risks(namespace)

        return namespace

    def _is_global_parameter(self, param: ExtractedParameter) -> bool:
        """Determine if parameter should be global (shared)"""
        global_patterns = [
            'api_key', 'base_url', 'start_date', 'end_date',
            'symbols', 'max_workers', 'timeout'
        ]

        return any(pattern in param.name.lower() for pattern in global_patterns)

    async def _isolate_scanner_parameter(self,
                                       param: ExtractedParameter,
                                       namespace: ParameterNamespace):
        """Isolate parameter to specific scanner namespace"""

        # Check for naming conflicts
        if param.name in namespace.parameters:
            # Handle parameter conflict
            resolved_param = await self.conflict_resolver.resolve_parameter_conflict(
                param, namespace.parameters[param.name], namespace
            )
            namespace.parameters[resolved_param.name] = resolved_param
        else:
            namespace.parameters[param.name] = param

class IsolatedParameterExtractor(EnhancedParameterExtractor):
    """Enhanced parameter extractor with scanner isolation"""

    def __init__(self):
        super().__init__()
        self.namespace_manager = NamespaceManager()
        self.boundary_detector = AIBoundaryDetector()
        self.isolation_validator = IsolationValidator()

    async def extract_parameters_isolated(self,
                                        code: str,
                                        filename: str = "") -> IsolatedParameterResult:
        """Extract parameters with complete scanner isolation"""

        logger.info(f"ðŸ” Starting isolated parameter extraction for {filename}")

        try:
            # Step 1: Detect scanner boundaries
            boundaries = await self.boundary_detector.detect_scanner_boundaries(code, filename)
            logger.info(f"ðŸ“ Detected {len(boundaries)} scanner boundaries")

            # Step 2: Extract parameters per scanner
            isolated_extractions = {}
            for boundary in boundaries:
                scanner_code = self._extract_scanner_code(code, boundary)

                # Extract parameters using existing 3-layer system
                scanner_params = await self._extract_scanner_parameters(
                    scanner_code, boundary.scanner_name
                )

                # Isolate into namespace
                namespace = await self.namespace_manager.isolate_parameters(
                    boundary.scanner_name, scanner_params
                )

                isolated_extractions[boundary.scanner_name] = namespace

            # Step 3: Extract global parameters
            global_params = await self._extract_global_parameters(code)

            # Step 4: Validate isolation integrity
            isolation_validation = await self.isolation_validator.validate_isolation(
                isolated_extractions, global_params
            )

            result = IsolatedParameterResult(
                session_id=self._generate_session_id(),
                scanner_boundaries=boundaries,
                isolated_parameters=isolated_extractions,
                global_parameters=global_params,
                isolation_validation=isolation_validation,
                extraction_metadata=self._compile_extraction_metadata(boundaries, isolated_extractions)
            )

            logger.info(f"âœ… Isolated parameter extraction completed with {isolation_validation.isolation_score:.2f} score")
            return result

        except Exception as e:
            logger.error(f"âŒ Isolated parameter extraction failed: {e}")
            raise ParameterIsolationError(f"Extraction failed: {e}")

    def _extract_scanner_code(self, code: str, boundary: ScannerBoundary) -> str:
        """Extract code segment for specific scanner"""
        lines = code.split('\n')
        start_idx = max(0, boundary.start_line - 1)
        end_idx = min(len(lines), boundary.end_line)

        scanner_lines = lines[start_idx:end_idx]
        return '\n'.join(scanner_lines)

    async def _extract_scanner_parameters(self,
                                        scanner_code: str,
                                        scanner_name: str) -> List[ExtractedParameter]:
        """Extract parameters for specific scanner using existing 3-layer system"""

        # Use existing proven extraction methods
        ast_params = self._extract_ast_parameters(scanner_code)
        pattern_params = self._extract_pattern_parameters(scanner_code)
        config_params = self._extract_config_blocks(scanner_code)

        # CRITICAL: Do NOT use the problematic _combine_parameters method
        # Instead use new isolated combination
        combined_params = self._combine_parameters_isolated(ast_params, pattern_params, config_params)

        # Enhance with scanner context
        enhanced_params = self._enhance_parameters_with_scanner_context(
            combined_params, scanner_code, scanner_name
        )

        return enhanced_params

    def _combine_parameters_isolated(self, *param_lists) -> List[ExtractedParameter]:
        """REPLACES the problematic _combine_parameters() method - NO cross-contamination"""
        isolated_params = {}

        for param_list in param_lists:
            for param in param_list:
                key = param.name

                # Keep highest confidence version within THIS scanner only
                if key not in isolated_params or param.confidence > isolated_params[key].confidence:
                    isolated_params[key] = param

        return list(isolated_params.values())
```

### 1.3 Smart Template Generation Algorithm

#### 1.3.1 Code Generation Engine
```python
from jinja2 import Template
from typing import Dict, List, Optional

class SmartTemplateGenerator:
    """AI-enhanced template generation for isolated scanners"""

    def __init__(self):
        self.code_extractor = CodeExtractionEngine()
        self.dependency_resolver = DependencyResolver()
        self.template_validator = TemplateValidator()
        self.template_library = TemplateLibrary()

    async def generate_isolated_templates(self,
                                        boundaries: List[ScannerBoundary],
                                        isolated_params: IsolatedParameterResult) -> List[GeneratedTemplate]:
        """Generate complete, functional scanner templates"""

        templates = []

        for boundary in boundaries:
            try:
                # Extract scanner-specific code
                scanner_code = await self.code_extractor.extract_scanner_code(
                    isolated_params.original_code, boundary
                )

                # Resolve all dependencies
                dependencies = await self.dependency_resolver.resolve_dependencies(
                    scanner_code, boundary, isolated_params
                )

                # Generate complete template
                template_code = await self._generate_complete_template(
                    scanner_code, dependencies, boundary, isolated_params
                )

                # Validate template functionality
                validation_result = await self.template_validator.validate_template(
                    template_code, boundary
                )

                template = GeneratedTemplate(
                    scanner_name=boundary.scanner_name,
                    template_code=template_code,
                    dependencies=dependencies,
                    validation_result=validation_result,
                    confidence_score=boundary.confidence_score,
                    generation_metadata=self._compile_generation_metadata(boundary, dependencies)
                )

                templates.append(template)

            except Exception as e:
                logger.error(f"Template generation failed for {boundary.scanner_name}: {e}")
                # Create error template for debugging
                error_template = self._create_error_template(boundary, str(e))
                templates.append(error_template)

        return templates

    async def _generate_complete_template(self,
                                        scanner_code: str,
                                        dependencies: List[str],
                                        boundary: ScannerBoundary,
                                        isolated_params: IsolatedParameterResult) -> str:
        """Generate complete, functional scanner template"""

        # Get scanner parameters from isolated result
        scanner_namespace = isolated_params.isolated_parameters.get(boundary.scanner_name)
        if not scanner_namespace:
            raise TemplateGenerationError(f"No parameters found for {boundary.scanner_name}")

        # Use Jinja2 template for consistent structure
        template_structure = self.template_library.get_scanner_template()

        template_data = {
            'scanner_name': boundary.scanner_name,
            'imports': dependencies,
            'parameters': scanner_namespace.parameters,
            'global_parameters': isolated_params.global_parameters,
            'scanner_code': scanner_code,
            'metadata': {
                'generated_date': datetime.now().isoformat(),
                'confidence_score': boundary.confidence_score,
                'boundary_type': boundary.boundary_type.value
            }
        }

        rendered_template = template_structure.render(**template_data)

        # Post-process and optimize
        optimized_template = await self._optimize_template_code(rendered_template)

        return optimized_template

# Scanner Template (Jinja2)
SCANNER_TEMPLATE = """#!/usr/bin/env python3
\"\"\"
{{ scanner_name }} Scanner
Auto-generated from multi-scanner file using CE-Hub AI-powered extraction
Generated: {{ metadata.generated_date }}
Confidence Score: {{ metadata.confidence_score }}
\"\"\"

# Standard imports
import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging

# API imports
{% for import_stmt in imports %}
{{ import_stmt }}
{% endfor %}

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global Configuration
{% for param_name, param in global_parameters.items() %}
{{ param_name.upper() }} = {{ param.value | repr }}  # {{ param.description }}
{% endfor %}

# Scanner Parameters
{% for param_name, param in parameters.items() %}
{{ param_name }} = {{ param.value | repr }}  # {{ param.description }}
{% endfor %}

# Scanner Implementation
{{ scanner_code }}

# Execution Framework
class {{ scanner_name.title() }}Scanner:
    \"\"\"Isolated {{ scanner_name }} scanner with parameter isolation\"\"\"

    def __init__(self):
        self.scanner_name = "{{ scanner_name }}"
        self.results = []
        self.execution_metadata = {}

    async def run_scan(self, symbols: List[str] = None, scan_date: str = None) -> Dict[str, Any]:
        \"\"\"Execute scanner with isolated parameters\"\"\"

        start_time = datetime.now()
        logger.info(f"ðŸ” Starting {self.scanner_name} scan")

        try:
            # Use scanner-specific function
            if 'symbols' in locals() or 'symbols' in globals():
                scan_symbols = symbols or globals().get('SYMBOLS', [])
            else:
                scan_symbols = symbols or []

            if scan_date:
                # Override scan date if provided
                globals()['SCAN_DATE'] = scan_date

            # Execute main scanner logic
            results = await self._execute_scanner(scan_symbols)

            # Compile execution metadata
            execution_time = (datetime.now() - start_time).total_seconds()
            self.execution_metadata = {
                'execution_time_seconds': execution_time,
                'symbols_processed': len(scan_symbols),
                'results_count': len(results),
                'scanner_version': '1.0',
                'isolation_verified': True
            }

            logger.info(f"âœ… {self.scanner_name} scan completed in {execution_time:.2f}s")

            return {
                'scanner_name': self.scanner_name,
                'results': results,
                'metadata': self.execution_metadata
            }

        except Exception as e:
            logger.error(f"âŒ {self.scanner_name} scan failed: {e}")
            raise ScannerExecutionError(f"Scanner execution failed: {e}")

    async def _execute_scanner(self, symbols: List[str]) -> List[Dict[str, Any]]:
        \"\"\"Execute the main scanner logic\"\"\"

        # This will be replaced with the actual scanner function call
        # Based on the extracted scanner code

        results = []

        # Call the main scanner function (this will be dynamically determined)
        {% if 'main_function' in metadata %}
        results = await {{ metadata.main_function }}(symbols)
        {% else %}
        # Default execution pattern
        for symbol in symbols:
            try:
                # Execute scanner logic for each symbol
                result = await self._process_symbol(symbol)
                if result:
                    results.append(result)
            except Exception as e:
                logger.warning(f"Failed to process {symbol}: {e}")
                continue
        {% endif %}

        return results

    async def _process_symbol(self, symbol: str) -> Optional[Dict[str, Any]]:
        \"\"\"Process individual symbol (fallback method)\"\"\"
        # This is a fallback if main scanner function cannot be determined
        # Will be replaced with actual scanner logic during template generation
        pass

# Main execution
if __name__ == "__main__":
    async def main():
        scanner = {{ scanner_name.title() }}Scanner()

        # Default execution
        results = await scanner.run_scan()

        # Display results
        print(f"\\nðŸ“Š {scanner.scanner_name} Results:")
        print(f"Found {len(results.get('results', []))} signals")

        if results.get('results'):
            df = pd.DataFrame(results['results'])
            print(df.head(10))

        # Save results
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{scanner.scanner_name}_results_{timestamp}.csv"

        if results.get('results'):
            df.to_csv(filename, index=False)
            print(f"\\nðŸ’¾ Results saved to {filename}")

    asyncio.run(main())
"""

class TemplateLibrary:
    """Manages template library and variations"""

    def get_scanner_template(self) -> Template:
        return Template(SCANNER_TEMPLATE)

    def get_minimal_template(self) -> Template:
        # Simplified template for basic scanners
        pass

    def get_enterprise_template(self) -> Template:
        # Advanced template for enterprise scanners
        pass
```

---

## 2. Code Structure and Organization

### 2.1 Directory Structure
```
backend/universal_scanner_engine/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ ai_enhanced/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ boundary_detection/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ai_boundary_detector.py
â”‚   â”‚   â”œâ”€â”€ ast_detector.py
â”‚   â”‚   â”œâ”€â”€ semantic_detector.py
â”‚   â”‚   â”œâ”€â”€ pattern_detector.py
â”‚   â”‚   â””â”€â”€ consensus_engine.py
â”‚   â”œâ”€â”€ parameter_isolation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ isolated_extractor.py
â”‚   â”‚   â”œâ”€â”€ namespace_manager.py
â”‚   â”‚   â”œâ”€â”€ conflict_resolver.py
â”‚   â”‚   â””â”€â”€ validation_engine.py
â”‚   â”œâ”€â”€ template_generation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ smart_generator.py
â”‚   â”‚   â”œâ”€â”€ code_extractor.py
â”‚   â”‚   â”œâ”€â”€ dependency_resolver.py
â”‚   â”‚   â””â”€â”€ template_validator.py
â”‚   â””â”€â”€ multi_scanner/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ orchestrator.py
â”‚       â”œâ”€â”€ progress_tracker.py
â”‚       â””â”€â”€ result_aggregator.py
â”œâ”€â”€ classification/
â”‚   â””â”€â”€ scanner_classifier.py (existing)
â”œâ”€â”€ extraction/
â”‚   â””â”€â”€ parameter_extractor.py (existing - enhanced)
â”œâ”€â”€ api/
â”‚   â””â”€â”€ polygon_manager.py (existing)
â”œâ”€â”€ resource/
â”‚   â””â”€â”€ thread_manager.py (existing)
â””â”€â”€ core/
    â””â”€â”€ use_orchestrator.py (existing - enhanced)
```

### 2.2 Enhanced Orchestrator Implementation

```python
# backend/universal_scanner_engine/ai_enhanced/multi_scanner/orchestrator.py

import asyncio
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

from ..boundary_detection.ai_boundary_detector import AIBoundaryDetector
from ..parameter_isolation.isolated_extractor import IsolatedParameterExtractor
from ..template_generation.smart_generator import SmartTemplateGenerator
from ...core.use_orchestrator import UniversalScannerOrchestrator, ExecutionStatus

@dataclass
class MultiScannerExecutionRequest:
    """Request for multi-scanner processing"""
    scanner_id: str
    filename: str
    code: str
    user_params: Optional[Dict[str, Any]] = None
    scan_date: Optional[str] = None
    generate_templates: bool = False
    priority: int = 3
    timeout_seconds: int = 3600

@dataclass
class MultiScannerExecutionResult:
    """Complete multi-scanner execution result"""
    session_id: str
    status: ExecutionStatus
    start_time: datetime
    end_time: Optional[datetime]
    execution_time_seconds: Optional[float]

    # Multi-scanner specific results
    scanner_boundaries: List[ScannerBoundary] = None
    isolated_parameters: IsolatedParameterResult = None
    individual_results: Dict[str, ExecutionResult] = None
    aggregated_results: AggregatedScanResults = None
    generated_templates: List[GeneratedTemplate] = None

    # Quality assurance
    validation_report: ValidationReport = None
    performance_metrics: PerformanceMetrics = None

    # Error handling
    error_message: Optional[str] = None
    error_details: Optional[Dict[str, Any]] = None

class MultiScannerOrchestrator(UniversalScannerOrchestrator):
    """Enhanced orchestrator for AI-powered multi-scanner processing"""

    def __init__(self):
        super().__init__()
        self.boundary_detector = AIBoundaryDetector()
        self.parameter_isolator = IsolatedParameterExtractor()
        self.template_generator = SmartTemplateGenerator()
        self.progress_tracker = MultiScannerProgressTracker()
        self.result_aggregator = ResultAggregator()

    async def execute_multi_scanner_request(self,
                                          request: MultiScannerExecutionRequest
                                          ) -> MultiScannerExecutionResult:
        """Main execution workflow for multi-scanner processing"""

        session_id = str(uuid.uuid4())
        start_time = datetime.now()

        logger.info(f"ðŸš€ Starting multi-scanner execution: {session_id}")

        try:
            # Initialize progress tracking
            progress = self.progress_tracker.create_tracker(session_id, request.filename)

            # Phase 1: Boundary Detection (10-20%)
            progress.update_phase("boundary_detection", 10)
            boundaries = await self.boundary_detector.detect_scanner_boundaries(
                request.code, request.filename
            )

            if not boundaries:
                return await self._handle_no_boundaries_detected(request, session_id)

            progress.update_progress(20, f"Detected {len(boundaries)} scanner boundaries")

            # Phase 2: Parameter Isolation (20-40%)
            progress.update_phase("parameter_isolation", 25)
            isolated_params = await self.parameter_isolator.extract_parameters_isolated(
                request.code, request.filename
            )
            progress.update_progress(40, "Parameters isolated successfully")

            # Phase 3: Multi-Scanner Execution (40-80%)
            progress.update_phase("multi_scanner_execution", 45)
            individual_results = await self._execute_scanners_parallel(
                boundaries, isolated_params, request, progress
            )
            progress.update_progress(80, f"Executed {len(individual_results)} scanners")

            # Phase 4: Result Aggregation (80-90%)
            progress.update_phase("result_aggregation", 85)
            aggregated_results = await self.result_aggregator.aggregate_results(
                individual_results, boundaries
            )
            progress.update_progress(90, "Results aggregated")

            # Phase 5: Template Generation (90-95%) - Optional
            generated_templates = None
            if request.generate_templates:
                progress.update_phase("template_generation", 92)
                generated_templates = await self.template_generator.generate_isolated_templates(
                    boundaries, isolated_params
                )
                progress.update_progress(95, f"Generated {len(generated_templates)} templates")

            # Phase 6: Validation & Quality Assurance (95-100%)
            progress.update_phase("validation", 97)
            validation_report = await self._validate_execution_quality(
                boundaries, isolated_params, individual_results
            )

            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()

            progress.update_phase("completed", 100)

            result = MultiScannerExecutionResult(
                session_id=session_id,
                status=ExecutionStatus.COMPLETED,
                start_time=start_time,
                end_time=end_time,
                execution_time_seconds=execution_time,
                scanner_boundaries=boundaries,
                isolated_parameters=isolated_params,
                individual_results=individual_results,
                aggregated_results=aggregated_results,
                generated_templates=generated_templates,
                validation_report=validation_report,
                performance_metrics=await self._compile_performance_metrics(session_id)
            )

            logger.info(f"âœ… Multi-scanner execution completed: {session_id} ({execution_time:.2f}s)")
            return result

        except Exception as e:
            logger.error(f"âŒ Multi-scanner execution failed: {session_id} - {e}")
            return await self._handle_execution_error(e, request, session_id, start_time)

    async def _execute_scanners_parallel(self,
                                       boundaries: List[ScannerBoundary],
                                       isolated_params: IsolatedParameterResult,
                                       request: MultiScannerExecutionRequest,
                                       progress: ProgressTracker) -> Dict[str, ExecutionResult]:
        """Execute multiple scanners in parallel with full isolation"""

        scanner_tasks = []
        for boundary in boundaries:
            # Create completely isolated execution request
            isolated_request = await self._create_isolated_execution_request(
                boundary, isolated_params, request
            )

            # Create isolated execution task
            task = asyncio.create_task(
                self._execute_single_scanner_isolated(isolated_request, progress)
            )
            scanner_tasks.append((boundary.scanner_name, task))

        # Execute all scanners concurrently with progress monitoring
        results = {}
        completed_count = 0

        for scanner_name, task in scanner_tasks:
            try:
                result = await task
                results[scanner_name] = result
                completed_count += 1

                # Update overall progress
                scanner_progress = 40 + (completed_count / len(scanner_tasks)) * 40
                progress.update_progress(scanner_progress, f"Completed {scanner_name}")

            except Exception as e:
                logger.error(f"Scanner {scanner_name} execution failed: {e}")
                results[scanner_name] = self._create_error_result(scanner_name, e)

        return results

    async def _create_isolated_execution_request(self,
                                               boundary: ScannerBoundary,
                                               isolated_params: IsolatedParameterResult,
                                               original_request: MultiScannerExecutionRequest) -> ScannerExecutionRequest:
        """Create completely isolated execution request for single scanner"""

        # Extract scanner code
        scanner_code = self._extract_scanner_code(original_request.code, boundary)

        # Get isolated parameters for this scanner
        scanner_namespace = isolated_params.isolated_parameters.get(boundary.scanner_name)
        scanner_params = scanner_namespace.parameters if scanner_namespace else {}

        # Combine with global parameters
        combined_params = {**isolated_params.global_parameters, **scanner_params}

        # Create isolated request
        isolated_request = ScannerExecutionRequest(
            scanner_id=f"{original_request.scanner_id}_{boundary.scanner_name}",
            filename=f"{boundary.scanner_name}_{original_request.filename}",
            code=scanner_code,
            user_params=combined_params,
            scan_date=original_request.scan_date,
            priority=original_request.priority
        )

        return isolated_request
```

---

## 3. Performance Benchmarks

### 3.1 Target Performance Metrics

| Component | Metric | Target | Measurement Method |
|-----------|--------|--------|--------------------|
| AI Boundary Detection | Processing Time | < 5 seconds | Per 1000-line file |
| Parameter Isolation | Extraction Time | < 10 seconds | Per scanner boundary |
| Template Generation | Generation Time | < 15 seconds | Per isolated template |
| Multi-Scanner Execution | Total Time | < 30 minutes | For 3 scanners, enterprise-level |
| Memory Usage | Peak Memory | < 4 GB | During parallel execution |
| API Performance | Response Time | < 500ms | Status/progress endpoints |
| Database Operations | Query Time | < 100ms | Parameter retrieval/storage |
| Contamination Detection | Accuracy | > 99% | Validation against test cases |

### 3.2 Performance Optimization Framework

```python
# backend/universal_scanner_engine/ai_enhanced/performance/optimizer.py

import time
import psutil
import asyncio
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

@dataclass
class PerformanceMetrics:
    """Comprehensive performance tracking"""
    operation: str
    start_time: float
    end_time: Optional[float] = None
    duration_seconds: Optional[float] = None
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    api_calls_count: int = 0
    cache_hits: int = 0
    cache_misses: int = 0

class PerformanceOptimizer:
    """AI-enhanced performance optimization"""

    def __init__(self):
        self.metrics_history: List[PerformanceMetrics] = []
        self.optimization_patterns = OptimizationPatterns()
        self.resource_monitor = ResourceMonitor()

    async def optimize_execution_strategy(self,
                                        boundaries: List[ScannerBoundary],
                                        system_resources: Dict[str, Any]) -> OptimizationStrategy:
        """Determine optimal execution strategy based on current conditions"""

        # Analyze workload characteristics
        workload_analysis = await self._analyze_workload(boundaries)

        # Check system resource availability
        resource_availability = await self.resource_monitor.check_resources()

        # Apply ML-based optimization
        strategy = await self._apply_ml_optimization(
            workload_analysis, resource_availability
        )

        return strategy

    async def _analyze_workload(self, boundaries: List[ScannerBoundary]) -> WorkloadAnalysis:
        """Analyze workload characteristics for optimization"""

        total_complexity = sum(self._estimate_scanner_complexity(b) for b in boundaries)
        estimated_execution_time = sum(self._estimate_execution_time(b) for b in boundaries)
        memory_requirements = sum(self._estimate_memory_usage(b) for b in boundaries)

        return WorkloadAnalysis(
            scanner_count=len(boundaries),
            total_complexity=total_complexity,
            estimated_execution_time=estimated_execution_time,
            memory_requirements=memory_requirements,
            parallelization_benefit=self._calculate_parallelization_benefit(boundaries)
        )

class IntelligentCache:
    """AI-powered caching with learning capabilities"""

    def __init__(self):
        self.boundary_cache = {}
        self.parameter_cache = {}
        self.template_cache = {}
        self.cache_performance = CachePerformanceTracker()

    async def get_cached_boundaries(self, code_hash: str) -> Optional[List[ScannerBoundary]]:
        """Get cached boundary detection results with intelligent expiration"""

        if code_hash in self.boundary_cache:
            cache_entry = self.boundary_cache[code_hash]

            # Check if cache is still valid using AI
            if await self._is_cache_valid(cache_entry):
                self.cache_performance.record_hit('boundaries')
                return cache_entry.boundaries
            else:
                # Invalidate expired cache
                del self.boundary_cache[code_hash]

        self.cache_performance.record_miss('boundaries')
        return None

    async def _is_cache_valid(self, cache_entry: CacheEntry) -> bool:
        """AI-powered cache validation"""

        # Consider multiple factors for cache validity
        age_hours = (time.time() - cache_entry.timestamp) / 3600

        # Dynamic TTL based on confidence score
        max_age = 24 * cache_entry.confidence_score  # High confidence = longer cache

        return age_hours < max_age
```

### 3.3 Resource Scaling Engine

```python
class ResourceScalingEngine:
    """Automatic resource scaling based on workload"""

    def __init__(self):
        self.resource_monitor = ResourceMonitor()
        self.scaling_policies = ScalingPolicies()

    async def scale_resources(self,
                            current_workload: WorkloadAnalysis) -> ResourceAllocation:
        """Automatically scale resources based on workload"""

        # Get current resource utilization
        current_resources = await self.resource_monitor.get_utilization()

        # Calculate required resources
        required_resources = await self._calculate_required_resources(current_workload)

        # Apply scaling policies
        scaling_decision = await self.scaling_policies.make_scaling_decision(
            current_resources, required_resources
        )

        # Execute scaling if needed
        if scaling_decision.should_scale:
            await self._execute_scaling(scaling_decision)

        return ResourceAllocation(
            cpu_cores=scaling_decision.allocated_cpu,
            memory_gb=scaling_decision.allocated_memory,
            thread_pool_size=scaling_decision.thread_pool_size,
            api_rate_limit=scaling_decision.api_rate_limit
        )
```

---

## 4. Security Considerations

### 4.1 Security Architecture

```python
# backend/universal_scanner_engine/security/security_framework.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import hashlib
import re

class SecurityLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class ThreatType(Enum):
    MALICIOUS_CODE = "malicious_code"
    DATA_EXFILTRATION = "data_exfiltration"
    SYSTEM_ACCESS = "system_access"
    API_ABUSE = "api_abuse"

@dataclass
class SecurityThreat:
    """Detected security threat"""
    threat_type: ThreatType
    severity: SecurityLevel
    description: str
    line_number: int
    code_snippet: str
    mitigation: str

class CodeSecurityAnalyzer:
    """Comprehensive security analysis for uploaded code"""

    def __init__(self):
        self.malicious_patterns = MaliciousPatternDatabase()
        self.data_flow_analyzer = DataFlowAnalyzer()
        self.api_usage_monitor = APIUsageMonitor()

    async def analyze_code_security(self, code: str, filename: str) -> SecurityAnalysisResult:
        """Comprehensive security analysis"""

        threats = []

        # Static analysis for malicious patterns
        static_threats = await self._analyze_static_patterns(code)
        threats.extend(static_threats)

        # Data flow analysis
        data_flow_threats = await self._analyze_data_flows(code)
        threats.extend(data_flow_threats)

        # API usage analysis
        api_threats = await self._analyze_api_usage(code)
        threats.extend(api_threats)

        # Calculate overall risk score
        risk_score = self._calculate_risk_score(threats)

        # Determine if code is safe for execution
        execution_allowed = self._determine_execution_safety(threats, risk_score)

        return SecurityAnalysisResult(
            filename=filename,
            threats_detected=threats,
            risk_score=risk_score,
            execution_allowed=execution_allowed,
            security_recommendations=self._generate_security_recommendations(threats)
        )

    async def _analyze_static_patterns(self, code: str) -> List[SecurityThreat]:
        """Detect malicious patterns in code"""

        threats = []
        lines = code.split('\n')

        # Check for dangerous imports
        dangerous_imports = [
            'subprocess', 'os.system', 'eval', 'exec',
            'pickle', 'marshal', '__import__'
        ]

        for i, line in enumerate(lines, 1):
            for dangerous in dangerous_imports:
                if dangerous in line and ('import' in line or 'from' in line):
                    threats.append(SecurityThreat(
                        threat_type=ThreatType.SYSTEM_ACCESS,
                        severity=SecurityLevel.HIGH,
                        description=f"Dangerous import detected: {dangerous}",
                        line_number=i,
                        code_snippet=line.strip(),
                        mitigation="Remove dangerous import or use safer alternatives"
                    ))

        # Check for file system access
        file_operations = ['open(', 'file(', 'read(', 'write(']
        for i, line in enumerate(lines, 1):
            for op in file_operations:
                if op in line:
                    threats.append(SecurityThreat(
                        threat_type=ThreatType.DATA_EXFILTRATION,
                        severity=SecurityLevel.MEDIUM,
                        description=f"File system access detected: {op}",
                        line_number=i,
                        code_snippet=line.strip(),
                        mitigation="Ensure file operations are necessary and secure"
                    ))

        return threats

class ExecutionSandbox:
    """Secure execution environment for scanner code"""

    def __init__(self):
        self.allowed_modules = AllowedModulesList()
        self.resource_limits = ResourceLimits()
        self.network_policy = NetworkPolicy()

    async def execute_in_sandbox(self,
                                code: str,
                                parameters: Dict[str, Any]) -> SandboxExecutionResult:
        """Execute code in secure sandbox environment"""

        # Pre-execution security check
        security_result = await self._pre_execution_security_check(code)
        if not security_result.safe_to_execute:
            raise SecurityError("Code failed security validation")

        # Set up resource limits
        await self._apply_resource_limits()

        # Set up network restrictions
        await self._apply_network_restrictions()

        # Execute with monitoring
        try:
            result = await self._monitored_execution(code, parameters)
            return result
        except Exception as e:
            # Log security incident if execution fails suspiciously
            await self._log_security_incident(code, str(e))
            raise

    async def _monitored_execution(self,
                                 code: str,
                                 parameters: Dict[str, Any]) -> Any:
        """Execute code with real-time security monitoring"""

        # Create isolated execution context
        execution_context = {
            '__builtins__': self._get_safe_builtins(),
            **parameters
        }

        # Monitor resource usage during execution
        resource_monitor = ResourceUsageMonitor()

        try:
            # Execute code in restricted context
            exec_result = exec(code, execution_context)

            # Validate execution results
            await self._validate_execution_results(exec_result)

            return exec_result

        except Exception as e:
            # Analyze if exception indicates security issue
            if self._is_security_related_exception(e):
                await self._handle_security_exception(e, code)
            raise
```

### 4.2 Data Integrity Protection

```python
class DataIntegrityProtector:
    """Comprehensive data integrity protection"""

    def __init__(self):
        self.checksum_generator = ChecksumGenerator()
        self.integrity_monitor = IntegrityMonitor()
        self.backup_manager = BackupManager()

    async def protect_parameter_data(self,
                                   isolated_params: IsolatedParameterResult) -> IntegrityProtection:
        """Implement comprehensive data integrity protection"""

        # Generate integrity checksums
        integrity_checksums = {}
        for scanner_name, namespace in isolated_params.isolated_parameters.items():
            checksum = await self.checksum_generator.generate_namespace_checksum(namespace)
            integrity_checksums[scanner_name] = checksum

        # Create backup of parameter data
        backup_id = await self.backup_manager.create_backup(isolated_params)

        # Set up real-time integrity monitoring
        monitor_id = await self.integrity_monitor.start_monitoring(
            isolated_params.session_id, integrity_checksums
        )

        return IntegrityProtection(
            checksums=integrity_checksums,
            backup_id=backup_id,
            monitor_id=monitor_id,
            protection_level=SecurityLevel.HIGH
        )

    async def verify_integrity(self, session_id: str) -> IntegrityVerificationResult:
        """Verify data integrity for session"""

        # Get current data state
        current_data = await self._get_current_session_data(session_id)

        # Get stored checksums
        stored_checksums = await self._get_stored_checksums(session_id)

        # Calculate current checksums
        current_checksums = await self._calculate_current_checksums(current_data)

        # Compare checksums
        integrity_violations = []
        for scanner_name, stored_checksum in stored_checksums.items():
            current_checksum = current_checksums.get(scanner_name)
            if current_checksum != stored_checksum:
                integrity_violations.append(IntegrityViolation(
                    scanner_name=scanner_name,
                    stored_checksum=stored_checksum,
                    current_checksum=current_checksum,
                    violation_type="checksum_mismatch"
                ))

        return IntegrityVerificationResult(
            session_id=session_id,
            integrity_intact=len(integrity_violations) == 0,
            violations=integrity_violations,
            verification_timestamp=datetime.now()
        )
```

---

## 5. Database Schema Changes

### 5.1 New Tables for Multi-Scanner Support

```sql
-- Scanner boundary tracking
CREATE TABLE scanner_boundaries (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES scanner_sessions(id) ON DELETE CASCADE,
    scanner_name VARCHAR(255) NOT NULL,
    start_line INTEGER NOT NULL,
    end_line INTEGER NOT NULL,
    confidence_score DECIMAL(5,4) NOT NULL CHECK (confidence_score BETWEEN 0 AND 1),
    boundary_type VARCHAR(50) NOT NULL,
    detected_patterns TEXT[],
    parameter_namespace VARCHAR(255) NOT NULL,
    dependencies TEXT[],
    validation_status VARCHAR(50) NOT NULL,
    context_metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Isolated parameter storage
CREATE TABLE isolated_parameters (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    boundary_id UUID REFERENCES scanner_boundaries(id) ON DELETE CASCADE,
    namespace_id VARCHAR(255) NOT NULL,
    parameter_name VARCHAR(255) NOT NULL,
    parameter_value TEXT,
    parameter_type VARCHAR(50),
    confidence_score DECIMAL(5,4),
    line_number INTEGER,
    context TEXT,
    impact_level VARCHAR(20),
    validation_status VARCHAR(50),
    description TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(boundary_id, parameter_name)
);

-- Parameter isolation validation
CREATE TABLE isolation_validations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES scanner_sessions(id) ON DELETE CASCADE,
    overall_isolation_score DECIMAL(5,4) NOT NULL,
    contamination_detected BOOLEAN DEFAULT FALSE,
    contamination_details JSONB,
    validation_tests JSONB,
    recommendations TEXT[],
    validation_passed BOOLEAN,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Generated templates storage
CREATE TABLE generated_templates (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES scanner_sessions(id) ON DELETE CASCADE,
    boundary_id UUID REFERENCES scanner_boundaries(id) ON DELETE CASCADE,
    template_name VARCHAR(255) NOT NULL,
    template_code TEXT NOT NULL,
    dependencies TEXT[],
    validation_result JSONB,
    confidence_score DECIMAL(5,4),
    generation_metadata JSONB,
    file_path VARCHAR(500),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Security analysis results
CREATE TABLE security_analyses (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES scanner_sessions(id) ON DELETE CASCADE,
    filename VARCHAR(500) NOT NULL,
    risk_score DECIMAL(5,4) NOT NULL,
    execution_allowed BOOLEAN NOT NULL,
    threats_detected JSONB,
    security_recommendations TEXT[],
    analysis_timestamp TIMESTAMP DEFAULT NOW()
);

-- Performance metrics tracking
CREATE TABLE performance_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES scanner_sessions(id) ON DELETE CASCADE,
    metric_type VARCHAR(100) NOT NULL,
    metric_name VARCHAR(200) NOT NULL,
    metric_value DECIMAL(15,6),
    metric_unit VARCHAR(50),
    measurement_timestamp TIMESTAMP DEFAULT NOW(),
    metadata JSONB
);

-- Data integrity tracking
CREATE TABLE data_integrity_checksums (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES scanner_sessions(id) ON DELETE CASCADE,
    namespace_id VARCHAR(255) NOT NULL,
    scanner_name VARCHAR(255) NOT NULL,
    checksum_value VARCHAR(64) NOT NULL,
    checksum_algorithm VARCHAR(50) DEFAULT 'sha256',
    created_at TIMESTAMP DEFAULT NOW(),
    verified_at TIMESTAMP,
    UNIQUE(session_id, namespace_id)
);

-- AI model performance tracking
CREATE TABLE ai_model_performance (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model_type VARCHAR(100) NOT NULL, -- boundary_detection, parameter_extraction, etc.
    operation_type VARCHAR(100) NOT NULL,
    confidence_score DECIMAL(5,4),
    accuracy_score DECIMAL(5,4),
    execution_time_ms INTEGER,
    model_version VARCHAR(50),
    input_size INTEGER,
    success BOOLEAN,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### 5.2 Enhanced Scanner Sessions Table

```sql
-- Enhance existing scanner_sessions table
ALTER TABLE scanner_sessions ADD COLUMN IF NOT EXISTS
    is_multi_scanner BOOLEAN DEFAULT FALSE,
    scanner_count INTEGER DEFAULT 1,
    boundary_detection_time_ms INTEGER,
    parameter_isolation_time_ms INTEGER,
    template_generation_time_ms INTEGER,
    overall_isolation_score DECIMAL(5,4),
    ai_processing_metadata JSONB;

-- Add indexes for performance
CREATE INDEX idx_scanner_boundaries_session_id ON scanner_boundaries(session_id);
CREATE INDEX idx_scanner_boundaries_scanner_name ON scanner_boundaries(scanner_name);
CREATE INDEX idx_isolated_parameters_boundary_id ON isolated_parameters(boundary_id);
CREATE INDEX idx_isolated_parameters_namespace ON isolated_parameters(namespace_id);
CREATE INDEX idx_performance_metrics_session_type ON performance_metrics(session_id, metric_type);
CREATE INDEX idx_ai_model_performance_type_time ON ai_model_performance(model_type, created_at);

-- Add constraints
ALTER TABLE scanner_boundaries ADD CONSTRAINT check_positive_lines
    CHECK (start_line > 0 AND end_line >= start_line);
ALTER TABLE isolated_parameters ADD CONSTRAINT check_confidence_range
    CHECK (confidence_score IS NULL OR (confidence_score BETWEEN 0 AND 1));
```

---

## 6. Integration Points with Existing Code

### 6.1 Enhanced Parameter Extractor Integration

```python
# backend/universal_scanner_engine/extraction/parameter_extractor.py
# Add these enhancements to the existing file

class EnhancedParameterExtractor:
    """Enhanced with AI-powered multi-scanner support"""

    def __init__(self):
        # Existing initialization
        super().__init__()

        # Add new AI components
        self.multi_scanner_detector = None
        self.isolation_engine = None

    def enable_multi_scanner_mode(self):
        """Enable AI-powered multi-scanner processing"""
        from ..ai_enhanced.boundary_detection.ai_boundary_detector import AIBoundaryDetector
        from ..ai_enhanced.parameter_isolation.namespace_manager import NamespaceManager

        self.multi_scanner_detector = AIBoundaryDetector()
        self.isolation_engine = NamespaceManager()

    async def extract_parameters(self, code: str, filename: str = "") -> ParameterExtractionResult:
        """Enhanced extraction with multi-scanner detection"""

        # Check if multi-scanner mode is enabled and multiple scanners detected
        if self.multi_scanner_detector:
            boundaries = await self.multi_scanner_detector.detect_scanner_boundaries(code, filename)

            if len(boundaries) > 1:
                logger.info(f"ðŸŽ¯ Multi-scanner file detected: {len(boundaries)} scanners")
                # Delegate to isolated extraction
                isolated_result = await self._extract_with_isolation(code, filename, boundaries)
                return self._convert_to_legacy_result(isolated_result)

        # Fall back to original extraction for single scanners
        return await super().extract_parameters(code, filename)

    def _combine_parameters(self, *param_lists) -> List[ExtractedParameter]:
        """MODIFIED: Check for isolation mode to prevent contamination"""

        if self.isolation_engine and len(param_lists) > 1:
            # If isolation is enabled, warn about potential contamination
            logger.warning("âš ï¸  Parameter combination without isolation detected")
            logger.warning("Consider using isolated extraction for multi-scanner files")

        # Use original combination logic for backward compatibility
        return super()._combine_parameters(*param_lists)
```

### 6.2 Enhanced Orchestrator Integration

```python
# backend/universal_scanner_engine/core/use_orchestrator.py
# Add these enhancements to the existing file

class UniversalScannerOrchestrator:
    """Enhanced with multi-scanner capabilities"""

    def __init__(self):
        # Existing initialization
        super().__init__()

        # Add multi-scanner components
        self.multi_scanner_orchestrator = None

    def enable_multi_scanner_processing(self):
        """Enable AI-powered multi-scanner processing"""
        from ..ai_enhanced.multi_scanner.orchestrator import MultiScannerOrchestrator
        self.multi_scanner_orchestrator = MultiScannerOrchestrator()

    async def execute_scanner(self, request: ScannerExecutionRequest) -> ExecutionResult:
        """Enhanced execution with multi-scanner detection"""

        # Check if this might be a multi-scanner file
        if self.multi_scanner_orchestrator:
            is_multi_scanner = await self._detect_multi_scanner_file(request.code)

            if is_multi_scanner:
                logger.info(f"ðŸš€ Multi-scanner file detected, using enhanced processing")

                # Convert to multi-scanner request
                multi_request = MultiScannerExecutionRequest(
                    scanner_id=request.scanner_id,
                    filename=request.filename,
                    code=request.code,
                    user_params=request.user_params,
                    scan_date=request.scan_date,
                    generate_templates=False  # Default to False for backward compatibility
                )

                # Process with multi-scanner orchestrator
                multi_result = await self.multi_scanner_orchestrator.execute_multi_scanner_request(multi_request)

                # Convert result back to single-scanner format for compatibility
                return self._convert_multi_result_to_single(multi_result)

        # Use original single-scanner processing
        return await self._execute_single_scanner(request)
```

### 6.3 API Endpoint Integration

```python
# backend/api/endpoints/scanner_upload.py
# Enhance existing endpoints

from ..universal_scanner_engine.ai_enhanced.multi_scanner.orchestrator import (
    MultiScannerOrchestrator, MultiScannerExecutionRequest
)

@app.post("/api/scanner/upload")
async def upload_scanner_enhanced(
    file: UploadFile = File(...),
    user_params: Optional[str] = Form(None),
    scan_date: Optional[str] = Form(None),
    enable_multi_scanner: bool = Form(True),  # New parameter
    generate_templates: bool = Form(False)    # New parameter
):
    """Enhanced scanner upload with multi-scanner support"""

    # Existing validation logic
    if not file.filename.endswith('.py'):
        raise HTTPException(400, "Only Python files are supported")

    content = await file.read()
    code = content.decode('utf-8')

    # Parse user parameters
    parsed_params = json.loads(user_params) if user_params else {}

    # Check if multi-scanner processing is enabled and file has multiple scanners
    if enable_multi_scanner:
        orchestrator = MultiScannerOrchestrator()
        boundaries = await orchestrator.boundary_detector.detect_scanner_boundaries(code, file.filename)

        if len(boundaries) > 1:
            # Use multi-scanner processing
            request = MultiScannerExecutionRequest(
                scanner_id=str(uuid.uuid4()),
                filename=file.filename,
                code=code,
                user_params=parsed_params,
                scan_date=scan_date,
                generate_templates=generate_templates
            )

            result = await orchestrator.execute_multi_scanner_request(request)

            return {
                "session_id": result.session_id,
                "status": result.status.value,
                "processing_type": "multi_scanner",
                "scanner_count": len(result.scanner_boundaries) if result.scanner_boundaries else 0,
                "isolation_score": result.isolated_parameters.isolation_validation.isolation_score if result.isolated_parameters else None,
                "templates_generated": len(result.generated_templates) if result.generated_templates else 0,
                "execution_time_seconds": result.execution_time_seconds
            }

    # Fall back to single-scanner processing
    # ... existing single scanner logic ...
```

---

## 7. Error Handling and Recovery Mechanisms

### 7.1 Comprehensive Error Classification

```python
# backend/universal_scanner_engine/ai_enhanced/error_handling/error_classifier.py

from enum import Enum
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

class ErrorCategory(Enum):
    BOUNDARY_DETECTION = "boundary_detection"
    PARAMETER_ISOLATION = "parameter_isolation"
    TEMPLATE_GENERATION = "template_generation"
    EXECUTION_RUNTIME = "execution_runtime"
    SECURITY_VIOLATION = "security_violation"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    DATA_INTEGRITY = "data_integrity"

class ErrorSeverity(Enum):
    LOW = "low"           # Warning, processing can continue
    MEDIUM = "medium"     # Error, but recoverable
    HIGH = "high"         # Critical error, requires intervention
    CRITICAL = "critical" # System-level error, stop all processing

class RecoveryAction(Enum):
    RETRY = "retry"
    FALLBACK = "fallback"
    MANUAL_INTERVENTION = "manual_intervention"
    ABORT = "abort"

@dataclass
class ErrorEvent:
    """Comprehensive error event tracking"""
    error_id: str
    category: ErrorCategory
    severity: ErrorSeverity
    message: str
    details: Dict[str, Any]
    stack_trace: str
    context: Dict[str, Any]
    timestamp: datetime
    session_id: Optional[str] = None
    scanner_name: Optional[str] = None
    recovery_action: Optional[RecoveryAction] = None
    recovery_attempted: bool = False
    recovery_success: bool = False

class ErrorClassifier:
    """Intelligent error classification and recovery planning"""

    def __init__(self):
        self.error_patterns = ErrorPatternDatabase()
        self.recovery_strategies = RecoveryStrategyEngine()

    def classify_error(self, exception: Exception, context: Dict[str, Any]) -> ErrorEvent:
        """Classify error and determine appropriate recovery action"""

        # Extract error information
        error_message = str(exception)
        error_type = type(exception).__name__
        stack_trace = self._extract_stack_trace(exception)

        # Classify error category
        category = self._classify_error_category(exception, error_message, context)

        # Determine severity
        severity = self._determine_error_severity(category, error_message, context)

        # Determine recovery action
        recovery_action = self._determine_recovery_action(category, severity, context)

        # Create error event
        error_event = ErrorEvent(
            error_id=self._generate_error_id(),
            category=category,
            severity=severity,
            message=error_message,
            details={
                "error_type": error_type,
                "original_exception": str(exception)
            },
            stack_trace=stack_trace,
            context=context,
            timestamp=datetime.now(),
            session_id=context.get("session_id"),
            scanner_name=context.get("scanner_name"),
            recovery_action=recovery_action
        )

        return error_event

    def _classify_error_category(self,
                               exception: Exception,
                               error_message: str,
                               context: Dict[str, Any]) -> ErrorCategory:
        """Classify error into appropriate category"""

        # Boundary detection errors
        if "boundary" in error_message.lower() or isinstance(exception, BoundaryDetectionError):
            return ErrorCategory.BOUNDARY_DETECTION

        # Parameter isolation errors
        if "parameter" in error_message.lower() or "isolation" in error_message.lower():
            return ErrorCategory.PARAMETER_ISOLATION

        # Template generation errors
        if "template" in error_message.lower() or isinstance(exception, TemplateGenerationError):
            return ErrorCategory.TEMPLATE_GENERATION

        # Security errors
        if isinstance(exception, SecurityError) or "security" in error_message.lower():
            return ErrorCategory.SECURITY_VIOLATION

        # Resource exhaustion
        if isinstance(exception, (MemoryError, TimeoutError)) or "resource" in error_message.lower():
            return ErrorCategory.RESOURCE_EXHAUSTION

        # Data integrity errors
        if "integrity" in error_message.lower() or "checksum" in error_message.lower():
            return ErrorCategory.DATA_INTEGRITY

        # Default to runtime error
        return ErrorCategory.EXECUTION_RUNTIME

class RecoveryEngine:
    """Automated error recovery and fallback mechanisms"""

    def __init__(self):
        self.recovery_strategies = {
            ErrorCategory.BOUNDARY_DETECTION: BoundaryDetectionRecovery(),
            ErrorCategory.PARAMETER_ISOLATION: ParameterIsolationRecovery(),
            ErrorCategory.TEMPLATE_GENERATION: TemplateGenerationRecovery(),
            ErrorCategory.EXECUTION_RUNTIME: RuntimeRecovery(),
            ErrorCategory.SECURITY_VIOLATION: SecurityRecovery(),
            ErrorCategory.RESOURCE_EXHAUSTION: ResourceRecovery(),
            ErrorCategory.DATA_INTEGRITY: DataIntegrityRecovery()
        }

    async def attempt_recovery(self, error_event: ErrorEvent) -> RecoveryResult:
        """Attempt automated recovery based on error classification"""

        recovery_strategy = self.recovery_strategies.get(error_event.category)
        if not recovery_strategy:
            return RecoveryResult(
                success=False,
                message="No recovery strategy available for error category"
            )

        try:
            # Mark recovery attempt
            error_event.recovery_attempted = True

            # Attempt recovery
            recovery_result = await recovery_strategy.attempt_recovery(error_event)

            # Update error event
            error_event.recovery_success = recovery_result.success

            # Log recovery attempt
            await self._log_recovery_attempt(error_event, recovery_result)

            return recovery_result

        except Exception as recovery_exception:
            # Recovery itself failed
            logger.error(f"Recovery attempt failed: {recovery_exception}")

            return RecoveryResult(
                success=False,
                message=f"Recovery attempt failed: {str(recovery_exception)}",
                fallback_required=True
            )

class BoundaryDetectionRecovery:
    """Recovery strategies for boundary detection failures"""

    async def attempt_recovery(self, error_event: ErrorEvent) -> RecoveryResult:
        """Attempt to recover from boundary detection failure"""

        context = error_event.context
        code = context.get("code", "")
        filename = context.get("filename", "")

        # Strategy 1: Fall back to manual boundary detection
        if "confidence" in error_event.message.lower():
            logger.info("Attempting manual boundary detection fallback")

            try:
                manual_detector = ManualBoundaryDetector()
                boundaries = await manual_detector.detect_boundaries(code, filename)

                if boundaries:
                    return RecoveryResult(
                        success=True,
                        message="Manual boundary detection successful",
                        recovered_data={"boundaries": boundaries}
                    )

            except Exception as e:
                logger.warning(f"Manual boundary detection failed: {e}")

        # Strategy 2: Single scanner fallback
        logger.info("Falling back to single-scanner processing")

        return RecoveryResult(
            success=True,
            message="Falling back to single-scanner mode",
            fallback_mode="single_scanner"
        )

class ParameterIsolationRecovery:
    """Recovery strategies for parameter isolation failures"""

    async def attempt_recovery(self, error_event: ErrorEvent) -> RecoveryResult:
        """Attempt to recover from parameter isolation failure"""

        # Strategy 1: Conservative isolation
        if "contamination" in error_event.message.lower():
            logger.info("Attempting conservative parameter isolation")

            try:
                conservative_isolator = ConservativeParameterIsolator()
                isolated_params = await conservative_isolator.isolate_with_strict_separation(
                    error_event.context
                )

                return RecoveryResult(
                    success=True,
                    message="Conservative parameter isolation successful",
                    recovered_data={"isolated_parameters": isolated_params}
                )

            except Exception as e:
                logger.warning(f"Conservative isolation failed: {e}")

        # Strategy 2: Parameter validation and correction
        logger.info("Attempting parameter validation and correction")

        try:
            validator = ParameterValidator()
            corrected_params = await validator.validate_and_correct_parameters(
                error_event.context
            )

            return RecoveryResult(
                success=True,
                message="Parameter validation and correction successful",
                recovered_data={"corrected_parameters": corrected_params}
            )

        except Exception as e:
            return RecoveryResult(
                success=False,
                message=f"Parameter correction failed: {e}",
                fallback_required=True
            )
```

### 7.2 Graceful Degradation Framework

```python
class GracefulDegradationManager:
    """Manages graceful degradation when AI components fail"""

    def __init__(self):
        self.fallback_modes = FallbackModeRegistry()
        self.degradation_monitor = DegradationMonitor()

    async def handle_ai_component_failure(self,
                                        component_name: str,
                                        error: Exception,
                                        context: Dict[str, Any]) -> DegradationResult:
        """Handle AI component failure with graceful degradation"""

        # Determine appropriate fallback mode
        fallback_mode = await self._determine_fallback_mode(component_name, error, context)

        # Execute fallback strategy
        fallback_result = await self._execute_fallback(fallback_mode, context)

        # Monitor degradation impact
        await self.degradation_monitor.record_degradation(
            component_name, fallback_mode, fallback_result
        )

        return DegradationResult(
            original_component=component_name,
            fallback_mode=fallback_mode,
            fallback_result=fallback_result,
            degradation_impact=await self._assess_degradation_impact(fallback_mode)
        )

    async def _execute_fallback(self,
                              fallback_mode: FallbackMode,
                              context: Dict[str, Any]) -> Any:
        """Execute specific fallback strategy"""

        if fallback_mode == FallbackMode.MANUAL_BOUNDARY_DETECTION:
            return await self._manual_boundary_detection_fallback(context)
        elif fallback_mode == FallbackMode.CONSERVATIVE_PARAMETER_EXTRACTION:
            return await self._conservative_parameter_extraction_fallback(context)
        elif fallback_mode == FallbackMode.SINGLE_SCANNER_MODE:
            return await self._single_scanner_mode_fallback(context)
        elif fallback_mode == FallbackMode.BASIC_TEMPLATE_GENERATION:
            return await self._basic_template_generation_fallback(context)
        else:
            raise ValueError(f"Unknown fallback mode: {fallback_mode}")
```

---

This comprehensive technical implementation specification provides the detailed foundation for implementing the AI-powered multi-scanner system. The architecture leverages existing CE-Hub strengths while introducing intelligent automation to solve the parameter contamination problem.

**Key Implementation Files Created:**
- `/Users/michaeldurante/ai dev/ce-hub/edge-dev/docs/ai-powered-multi-scanner-architecture.md`
- `/Users/michaeldurante/ai dev/ce-hub/edge-dev/docs/technical-implementation-specifications.md`

The solution maintains backward compatibility while providing advanced AI-powered capabilities for multi-scanner processing with complete parameter isolation and intelligent template generation.