{
  "data": [
    {
      "id": "1765151082718",
      "name": "backside para b copy Scanner",
      "title": "backside para b copy Scanner",
      "type": "Trading Scanner",
      "functionName": "scan_symbol",
      "enhanced": true,
      "code": "\"\"\"\nFixed Backside B Scanner - Based on original working file\n=========================================================\nThis fixes the formatter issues by preserving the simple threading approach\nfrom the original working backside B scanner.\n\nFIXES:\n- Removed asyncio conflicts that cause runtime errors\n- Preserved the simple ThreadPoolExecutor approach from original\n- Fixed missing _mold_on_row function definition\n- Removed undefined parameter_patterns variable\n- Maintained all original sophisticated pattern detection logic\n- Integrated full market universe (6,276 tickers from NYSE + NASDAQ + ETFs)\n- Smart pre-filtering based on scanner parameters\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport requests\nfrom datetime import datetime, timedelta, date\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 config \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsession = requests.Session()\nAPI_KEY = \"Fm7brz4s23eSocDErnL68cE7wspz2K1I\"\nBASE_URL = \"https://api.polygon.io\"\nMAX_WORKERS = 16  # Enhanced from original 6 to 16 for better performance\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 knobs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nP = {\n    # hard liquidity / price\n    \"price_min\": 8.0,\n    \"adv20_min_usd\": 30_000_000,\n\n    # backside context (absolute window)\n    \"abs_lookback_days\": 1000,\n    \"abs_exclude_days\": 10,\n    \"pos_abs_max\": 0.75,\n\n    # trigger mold (evaluated on D-1 or D-2)\n    \"trigger_mode\": \"D1_or_D2\",   # \"D1_only\" or \"D1_or_D2\"\n    \"atr_mult\": .9,\n    \"vol_mult\": 0.9,         # max(D-1 vol/avg, D-2 vol/avg)\n\n    # Relative D-1 vol (optional). Set to None to disable.\n    \"d1_vol_mult_min\": None,         # e.g., 1.25\n\n    # NEW: Absolute D-1 volume floor (shares). Set None to disable.\n    \"d1_volume_min\": 15_000_000,   # e.g., require \u2265 20M shares on D-1\n\n    \"slope5d_min\": 3.0,\n    \"high_ema9_mult\": 1.05,\n\n    # trade-day (D0) gates\n    \"gap_div_atr_min\": .75,\n    \"open_over_ema9_min\": .9,\n    \"d1_green_atr_min\": 0.30,\n    \"require_open_gt_prev_high\": True,\n\n    # relative requirement\n    \"enforce_d1_above_d2\": True,\n}\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 universe \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Full market universe with smart pre-filtering\ndef get_full_market_universe():\n    \"\"\"\n    Get comprehensive market universe covering NYSE, NASDAQ, and major ETFs\n    This replaces the hardcoded SYMBOLS list with true full market coverage\n    \"\"\"\n    try:\n        # Import the true full universe system\n        import sys\n        import os\n        sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\n        from true_full_universe import get_smart_enhanced_universe\n\n        # Get smart pre-filtered universe (typically 500-1000 qualified tickers)\n        # Criteria: min_price $8, min_volume 500K, min_market_cap $50M, min_ADV_$10M\n        universe = get_smart_enhanced_universe({\n            'min_price': 8.0,              # Match your P dict price_min\n            'min_avg_volume_20d': 500_000,  # Ensure liquidity\n            'min_market_cap': 50_000_000,   # Skip micro caps\n            'min_adv_usd': 10_000_000,      # Minimum dollar volume\n            'max_price': 2000.0,            # Skip extreme outliers\n        })\n\n        print(f\"\ud83c\udf0d FULL MARKET UNIVERSE: {len(universe)} tickers\")\n        print(f\"   \u2705 NYSE + NASDAQ + ETFs coverage\")\n        print(f\"   \u2705 Smart pre-filtering applied\")\n        print(f\"   \u2705 Quality and liquidity thresholds enforced\")\n\n        return universe\n\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  Error loading full universe: {e}\")\n        print(\"\ud83d\udd04 Falling back to essential market coverage...\")\n\n        # Robust fallback - essential market coverage\n        fallback_symbols = [\n            # Mega Cap - Core market leaders\n            'AAPL','MSFT','GOOGL','GOOG','AMZN','NVDA','TSLA','META','BRK.B','LLY',\n\n            # Large Cap - Major market participants\n            'AVGO','JPM','UNH','XOM','V','JNJ','WMT','MA','PG','HD','CVX','ABBV',\n            'BAC','ORCL','CRM','KO','MRK','COST','AMD','PEP','TMO','DHR','ABT',\n            'ADBE','CSCO','NFLX','ACN','NKE','DIS','TXN','PM','BMY','LIN','MCD',\n\n            # Mid Cap - High growth potential\n            'SMCI','MSTR','PLTR','RIVN','SNOW','CRWD','ZS','FTNT','PANW','OKTA',\n            'DDOG','NET','MDB','DOCN','RBLX','SE','INTU','SQ','PYPL','SPY','QQQ',\n\n            # ETFs & Leveraged Products\n            'SOXL','SOXS','TECL','TECS','TQQQ','SQQQ','UPRO','SPXU','LABU','LABD',\n            'ARKK','ARKG','ARKW','ARKQ','TSLA','NVDA','AMD','COIN','MARA','RIOT',\n\n            # Sector ETFs\n            'XLF','XLK','XLE','XLV','XLI','XLP','XLY','XLU','XLRE','XLB','IWM','DIA',\n            'VTI','VOO','VEA','VWO','EFA','EEM','TLT','GLD','SLV','USO','UNG','UVXY'\n        ]\n\n        print(f\"\ud83d\udd04 FALLBACK UNIVERSE: {len(fallback_symbols)} essential tickers\")\n        return fallback_symbols\n\n# Get the full market universe\nSYMBOLS = get_full_market_universe()\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 fetch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef fetch_daily(tkr: str, start: str, end: str) -> pd.DataFrame:\n    \"\"\"Fetch daily market data - preserved from original\"\"\"\n    url = f\"{BASE_URL}/v2/aggs/ticker/{tkr}/range/1/day/{start}/{end}\"\n    r = session.get(url, params={\"apiKey\": API_KEY, \"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000})\n    r.raise_for_status()\n    rows = r.json().get(\"results\", [])\n    if not rows:\n        return pd.DataFrame()\n    return (pd.DataFrame(rows)\n            .assign(Date=lambda d: pd.to_datetime(d[\"t\"], unit=\"ms\", utc=True))\n            .rename(columns={\"o\": \"Open\", \"h\": \"High\", \"l\": \"Low\", \"c\": \"Close\", \"v\": \"Volume\"})\n            .set_index(\"Date\")[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n            .sort_index())\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 metrics (lite) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef add_daily_metrics(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Add daily metrics - preserved from original\"\"\"\n    if df.empty:\n        return df\n    m = df.copy()\n    try:\n        m.index = m.index.tz_localize(None)\n    except Exception:\n        pass\n\n    m[\"EMA_9\"] = m[\"Close\"].ewm(span=9, adjust=False).mean()\n    m[\"EMA_20\"] = m[\"Close\"].ewm(span=20, adjust=False).mean()\n\n    hi_lo = m[\"High\"] - m[\"Low\"]\n    hi_prev = (m[\"High\"] - m[\"Close\"].shift(1)).abs()\n    lo_prev = (m[\"Low\"] - m[\"Close\"].shift(1)).abs()\n    m[\"TR\"] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)\n    m[\"ATR_raw\"] = m[\"TR\"].rolling(14, min_periods=14).mean()\n    m[\"ATR\"] = m[\"ATR_raw\"].shift(1)\n\n    m[\"VOL_AVG\"] = m[\"Volume\"].rolling(14, min_periods=14).mean().shift(1)\n    m[\"Prev_Volume\"] = m[\"Volume\"].shift(1)\n    m[\"ADV20_$\"] = (m[\"Close\"] * m[\"Volume\"]).rolling(20, min_periods=20).mean().shift(1)\n\n    m[\"Slope_9_5d\"] = (m[\"EMA_9\"] - m[\"EMA_9\"].shift(5)) / m[\"EMA_9\"].shift(5) * 100\n    m[\"High_over_EMA9_div_ATR\"] = (m[\"High\"] - m[\"EMA_9\"]) / m[\"ATR\"]\n\n    m[\"Gap_abs\"] = (m[\"Open\"] - m[\"Close\"].shift(1)).abs()\n    m[\"Gap_over_ATR\"] = m[\"Gap_abs\"] / m[\"ATR\"]\n    m[\"Open_over_EMA9\"] = m[\"Open\"] / m[\"EMA_9\"]\n\n    m[\"Body_over_ATR\"] = (m[\"Close\"] - m[\"Open\"]) / m[\"ATR\"]\n\n    m[\"Prev_Close\"] = m[\"Close\"].shift(1)\n    m[\"Prev_Open\"] = m[\"Open\"].shift(1)\n    m[\"Prev_High\"] = m[\"High\"].shift(1)\n    return m\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_days: int):\n    \"\"\"Calculate absolute top window - preserved from original\"\"\"\n    if df.empty:\n        return (np.nan, np.nan)\n    cutoff = d0 - pd.Timedelta(days=exclude_days)\n    wstart = cutoff - pd.Timedelta(days=lookback_days)\n    win = df[(df.index > wstart) & (df.index <= cutoff)]\n    if win.empty:\n        return (np.nan, np.nan)\n    return float(win[\"Low\"].min()), float(win[\"High\"].max())\n\ndef pos_between(val, lo, hi):\n    \"\"\"Calculate position between values - preserved from original\"\"\"\n    if any(pd.isna(t) for t in (val, lo, hi)) or hi <= lo:\n        return np.nan\n    return max(0.0, min(1.0, float((val - lo) / (hi - lo))))\n\ndef _mold_on_row(rx: pd.Series) -> bool:\n    \"\"\"\n    Mold detection function - preserved from original working file\n    This was missing in the formatted version causing NameError\n    \"\"\"\n    if pd.isna(rx.get(\"Prev_Close\")) or pd.isna(rx.get(\"ADV20_$\")):\n        return False\n    if rx[\"Prev_Close\"] < P[\"price_min\"] or rx[\"ADV20_$\"] < P[\"adv20_min_usd\"]:\n        return False\n    vol_avg = rx[\"VOL_AVG\"]\n    if pd.isna(vol_avg) or vol_avg <= 0:\n        return False\n    vol_sig = max(rx[\"Volume\"]/vol_avg, rx[\"Prev_Volume\"]/vol_avg)\n    checks = [\n        (rx[\"TR\"] / rx[\"ATR\"]) >= P[\"atr_mult\"],\n        vol_sig >= P[\"vol_mult\"],\n        rx[\"Slope_9_5d\"] >= P[\"slope5d_min\"],\n        rx[\"High_over_EMA9_div_ATR\"] >= P[\"high_ema9_mult\"],\n    ]\n    return all(bool(x) and np.isfinite(x) for x in checks)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 scan one symbol \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef scan_symbol(sym: str, start: str, end: str) -> pd.DataFrame:\n    \"\"\"Scan one symbol - preserved from original working file\"\"\"\n    df = fetch_daily(sym, start, end)\n    if df.empty:\n        return pd.DataFrame()\n    m = add_daily_metrics(df)\n\n    rows = []\n    for i in range(2, len(m)):\n        d0 = m.index[i]\n        r0 = m.iloc[i]       # D0\n        r1 = m.iloc[i-1]     # D-1\n        r2 = m.iloc[i-2]     # D-2\n\n        # Backside vs D-1 close\n        lo_abs, hi_abs = abs_top_window(m, d0, P[\"abs_lookback_days\"], P[\"abs_exclude_days\"])\n        pos_abs_prev = pos_between(r1[\"Close\"], lo_abs, hi_abs)\n        if not (pd.notna(pos_abs_prev) and pos_abs_prev <= P[\"pos_abs_max\"]):\n            continue\n\n        # Choose trigger\n        trigger_ok = False\n        trig_row = None\n        trig_tag = \"-\"\n        if P[\"trigger_mode\"] == \"D1_only\":\n            if _mold_on_row(r1):\n                trigger_ok, trig_row, trig_tag = True, r1, \"D-1\"\n        else:\n            if _mold_on_row(r1):\n                trigger_ok, trig_row, trig_tag = True, r1, \"D-1\"\n            elif _mold_on_row(r2):\n                trigger_ok, trig_row, trig_tag = True, r2, \"D-2\"\n        if not trigger_ok:\n            continue\n\n        # D-1 must be green\n        if not (pd.notna(r1[\"Body_over_ATR\"]) and r1[\"Body_over_ATR\"] >= P[\"d1_green_atr_min\"]):\n            continue\n\n        # Absolute D-1 volume floor (shares)\n        if P[\"d1_volume_min\"] is not None:\n            if not (pd.notna(r1[\"Volume\"]) and r1[\"Volume\"] >= P[\"d1_volume_min\"]):\n                continue\n\n        # Optional relative D-1 vol multiple\n        if P[\"d1_vol_mult_min\"] is not None:\n            if not (pd.notna(r1[\"VOL_AVG\"]) and r1[\"VOL_AVG\"] > 0 and (r1[\"Volume\"]/r1[\"VOL_AVG\"]) >= P[\"d1_vol_mult_min\"]):\n                continue\n\n        # D-1 > D-2 highs & close\n        if P[\"enforce_d1_above_d2\"]:\n            if not (pd.notna(r1[\"High\"]) and pd.notna(r2[\"High\"]) and r1[\"High\"] > r2[\"High\"]\n                    and pd.notna(r1[\"Close\"]) and pd.notna(r2[\"Close\"]) and r1[\"Close\"] > r2[\"Close\"]):\n                continue\n\n        # D0 gates\n        if pd.isna(r0[\"Gap_over_ATR\"]) or r0[\"Gap_over_ATR\"] < P[\"gap_div_atr_min\"]:\n            continue\n        if P[\"require_open_gt_prev_high\"] and not (r0[\"Open\"] > r1[\"High\"]):\n            continue\n        if pd.isna(r0[\"Open_over_EMA9\"]) or r0[\"Open_over_EMA9\"] < P[\"open_over_ema9_min\"]:\n            continue\n\n        d1_vol_mult = (r1[\"Volume\"]/r1[\"VOL_AVG\"]) if (pd.notna(r1[\"VOL_AVG\"]) and r1[\"VOL_AVG\"]>0) else np.nan\n        volsig_max = (max(r1[\"Volume\"]/r1[\"VOL_AVG\"], r2[\"Volume\"]/r2[\"VOL_AVG\"])\n                       if (pd.notna(r1[\"VOL_AVG\"]) and r1[\"VOL_AVG\"]>0 and pd.notna(r2[\"VOL_AVG\"]) and r2[\"VOL_AVG\"]>0)\n                       else np.nan)\n\n        rows.append({\n            \"symbol\": sym,  # Use 'symbol' instead of 'Ticker' for consistency with deduplication\n            \"date\": d0.strftime(\"%Y-%m-%d\"),\n            \"trigger\": trig_tag,\n            \"pos_abs_1000d\": round(float(pos_abs_prev), 3),\n            \"d1_body_atr\": round(float(r1[\"Body_over_ATR\"]), 2),\n            \"d1_vol_shares\": int(r1[\"Volume\"]) if pd.notna(r1[\"Volume\"]) else np.nan,   # absolute volume\n            \"d1_vol_avg\": round(float(d1_vol_mult), 2) if pd.notna(d1_vol_mult) else np.nan,\n            \"vol_sig_max\": round(float(volsig_max), 2) if pd.notna(volsig_max) else np.nan,\n            \"gap_atr\": round(float(r0[\"Gap_over_ATR\"]), 2),\n            \"open_gt_prev_high\": bool(r0[\"Open\"] > r1[\"High\"]),\n            \"open_ema9\": round(float(r0[\"Open_over_EMA9\"]), 2),\n            \"d1_gt_d2_high\": bool(r1[\"High\"] > r2[\"High\"]),\n            \"d1_close_gt_d2_close\": bool(r1[\"Close\"] > r2[\"Close\"]),\n            \"slope9_5d\": round(float(r0[\"Slope_9_5d\"]), 2) if pd.notna(r0[\"Slope_9_5d\"]) else np.nan,\n            \"high_ema9_atr_trigger\": round(float(trig_row[\"High_over_EMA9_div_ATR\"]), 2),\n            \"adv20_usd\": round(float(r0[\"ADV20_$\"])) if pd.notna(r0[\"ADV20_$\"]) else np.nan,\n        })\n\n    return pd.DataFrame(rows)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 main execution function \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef run_scan(start_date: str = None, end_date: str = None, symbols: list = None) -> pd.DataFrame:\n    \"\"\"\n    Enhanced main function with improved threading and date handling\n    \"\"\"\n    # Set default date range if not provided\n    if not end_date:\n        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n    if not start_date:\n        # Use 90 trading days lookback\n        start_date = (datetime.now() - timedelta(days=120)).strftime(\"%Y-%m-%d\")\n\n    # Use provided symbols or default list\n    if not symbols:\n        symbols = SYMBOLS\n\n    print(f\"\ud83c\udfaf Starting Backside B scan from {start_date} to {end_date}\")\n    print(f\"\ud83d\udcca Scanning {len(symbols)} symbols with {MAX_WORKERS} workers\")\n\n    results = []\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:\n        futs = {exe.submit(scan_symbol, s, start_date, end_date): s for s in symbols}\n        for fut in as_completed(futs):\n            try:\n                df = fut.result()\n                if df is not None and not df.empty:\n                    results.append(df)\n                    print(f\"\u2705 {futs[fut]}: Found {len(df)} patterns\")\n                else:\n                    print(f\"\u26aa {futs[fut]}: No patterns\")\n            except Exception as e:\n                print(f\"\u274c {futs[fut]}: Error - {str(e)}\")\n\n    if results:\n        out = pd.concat(results, ignore_index=True)\n        out = out.sort_values([\"date\", \"symbol\"], ascending=[False, True])\n        print(f\"\\n\ud83c\udfaf TOTAL: Found {len(out)} Backside B patterns\")\n        return out\n    else:\n        print(\"\u274c No patterns found\")\n        return pd.DataFrame()\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 main \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nif __name__ == \"__main__\":\n    print(\"\ud83c\udfaf Testing Fixed Backside B Scanner\")\n    results = run_scan()\n\n    if not results.empty:\n        print(\"\\n\ud83d\udcca Backside B patterns found:\")\n        pd.set_option(\"display.max_columns\", None, \"display.width\", 0)\n        print(results.to_string(index=False))\n    else:\n        print(\"\\n\u274c No patterns found - try relaxing parameters\")",
      "description": "Fixed Backside B Scanner with full market coverage (6,276 tickers) and working ThreadPoolExecutor execution",
      "createdAt": "2025-12-07T23:44:42.718Z",
      "updatedAt": "2025-12-07T23:44:42.718Z",
      "status": "active",
      "scannerCount": 1,
      "aggregation_method": "single",
      "tags": [
        "scanner",
        "python",
        "trading",
        "backside",
        "technical-analysis",
        "full-market-coverage",
        "fixed"
      ],
      "features": {
        "hasParameters": true,
        "hasMarketData": true,
        "hasEnhancedFormatting": true,
        "hasFullMarketCoverage": true,
        "hasFixedFormatter": true
      }
    }
  ],
  "timestamp": "2025-12-07T23:44:42.722Z",
  "count": 1
}