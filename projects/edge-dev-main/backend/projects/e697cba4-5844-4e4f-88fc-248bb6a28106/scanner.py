"""
lc_d2_scan_oct_25_new_ideas_3

Multi-Pattern Scanner - 3 patterns

Generated by RENATA_V2 - Multi-Scanner to V31 Transformation
Date Range: 2024-01-01 to 2024-12-31
Patterns: lc_frontside_d2_extended, lc_frontside_d2_extended_1, lc_frontside_d3_extended_1

V31 Architecture:
1. Market calendar integration (pandas_market_calendars)
2. Historical buffer calculation
3. Per-ticker operations (groupby().transform())
4. Historical/D0 separation in smart filters
5. Parallel processing (ThreadPoolExecutor)
6. Two-pass feature computation
7. Pre-sliced data for parallel processing
"""

import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas_market_calendars as mcal
from typing import List, Dict, Any, Optional


class lc_d2_scan_oct_25_new_ideas_3:
    """
    Multi-Pattern Scanner with 3 pattern detectors

    Patterns:
    - lc_frontside_d3_extended_1
    - lc_frontside_d2_extended
    - lc_frontside_d2_extended_1
    3 total patterns

    Scanner Type: multi
    Generated: {datetime.now().isoformat()}
    """

    def __init__(
        self,
        d0_start: str = None,
        d0_end: str = None
    ):
        """Initialize multi-scanner"""

        # Date configuration
        self.d0_start = d0_start or "2024-01-01"
        self.d0_end = d0_end or "2024-12-31"

        # Historical buffer for indicator computation
        self.historical_buffer_days = 1050  # ~2 years of trading days

        # Scan range (historical data needed for indicators)
        self.scan_start = (
            pd.Timestamp(self.d0_start) - pd.Timedelta(days=self.historical_buffer_days)
        ).strftime('%Y-%m-%d')
        self.scan_end = self.d0_end

        # Worker configuration
        self.stage3_workers = 10  # Parallel pattern detection

        # Market calendar
        self.us_calendar = mcal.get_calendar('NYSE')

        # Smart filter configuration (universal filters)
        self.smart_filters = {
            "min_prev_close": 0.75,
            "max_prev_close": 1000,
            "min_prev_volume": 500000,
            "max_prev_volume": 100000000
        }

        # Pattern assignments (extracted from original scanner)
        self.pattern_assignments = [
    {
        "name": "lc_frontside_d3_extended_1",
        "logic": "(h >= h1) & (h1 >= h2) & (l >= l1) & (l1 >= l2) & ((high_pct_chg1 >= 0.3) & (high_pct_chg >= 0.3) & (c_ua >= 5) & (c_ua < 15) & (h_dist_to_lowest_low_20_pct >= 2.5) | (high_pct_chg1 >= 0.2) & (high_pct_chg >= 0.2) & (c_ua >= 15) & (c_ua < 25) & (h_dist_to_lowest_low_20_pct >= 2) | (high_pct_chg1 >= 0.1) & (high_pct_chg >= 0.1) & (c_ua >= 25) & (c_ua < 50) & (h_dist_to_lowest_low_20_pct >= 1.5) | (high_pct_chg1 >= 0.07) & (high_pct_chg >= 0.07) & (c_ua >= 50) & (c_ua < 90) & (h_dist_to_lowest_low_20_pct >= 1) | (high_pct_chg1 >= 0.05) & (high_pct_chg >= 0.05) & (c_ua >= 90) & (h_dist_to_lowest_low_20_pct >= 0.75)) & (high_chg_atr1 >= 0.7) & (c1 >= o1) & (dist_h_9ema_atr1 >= 1.5) & (dist_h_20ema_atr1 >= 2) & (high_chg_atr >= 1) & (c >= o) & (dist_h_9ema_atr >= 1.5) & (dist_h_20ema_atr >= 2) & (v_ua >= 10000000) & (dol_v >= 500000000) & (v_ua1 >= 10000000) & (dol_v1 >= 100000000) & (c_ua >= 5) & ((high_chg_atr >= 1) | (high_chg_atr1 >= 1)) & (h_dist_to_highest_high_20_2_atr >= 2.5) & ((h >= highest_high_20) & (ema9 >= ema20) & (ema20 >= ema50))"
    },
    {
        "name": "lc_frontside_d2_extended",
        "logic": "(h >= h1) & (l >= l1) & ((high_pct_chg >= 0.5) & (c_ua >= 5) & (c_ua < 15) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 2.5) | (high_pct_chg >= 0.3) & (c_ua >= 15) & (c_ua < 25) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 2) | (high_pct_chg >= 0.2) & (c_ua >= 25) & (c_ua < 50) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 1.5) | (high_pct_chg >= 0.15) & (c_ua >= 50) & (c_ua < 90) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 1) | (high_pct_chg >= 0.1) & (c_ua >= 90) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 0.75)) & (high_chg_atr >= 1.5) & (c >= o) & (dist_h_9ema_atr >= 2) & (dist_h_20ema_atr >= 3) & (v_ua >= 10000000) & (dol_v >= 500000000) & (c_ua >= 5) & (dist_l_9ema_atr >= 1) & (h_dist_to_highest_high_20_1_atr >= 1) & (dol_v_cum5_1 >= 500000000) & ((h >= highest_high_20) & (ema9 >= ema20) & (ema20 >= ema50))"
    },
    {
        "name": "lc_frontside_d2_extended_1",
        "logic": "(h >= h1) & (l >= l1) & ((high_pct_chg >= 1) & (c_ua >= 5) & (c_ua < 15) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 2.5) | (high_pct_chg >= 0.5) & (c_ua >= 15) & (c_ua < 25) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 2) | (high_pct_chg >= 0.3) & (c_ua >= 25) & (c_ua < 50) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 1.5) | (high_pct_chg >= 0.2) & (c_ua >= 50) & (c_ua < 90) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 1) | (high_pct_chg >= 0.15) & (c_ua >= 90) & (highest_high_5_dist_to_lowest_low_20_pct_1 >= 0.75)) & (high_chg_atr >= 1.5) & (c >= o) & (dist_h_9ema_atr >= 2) & (dist_h_20ema_atr >= 3) & (v_ua >= 10000000) & (dol_v >= 500000000) & (c_ua >= 5) & (h_dist_to_highest_high_20_1_atr >= 1) & (dol_v_cum5_1 >= 500000000) & ((h >= highest_high_20) & (ema9 >= ema20) & (ema20 >= ema50))"
    }
]

        # Results storage
        self.all_results = []
        self.stage1_data = None
        self.stage2_data = None
        self.stage3_results = None

        print(f"ðŸš€ MULTI-PATTERN SCANNER: lc_d2_scan_oct_25_new_ideas_3")
        print(f"ðŸ“… Signal Output Range (D0): {self.d0_start} to {self.d0_end}")
        print(f"ðŸ“Š Historical Data Range: {self.scan_start} to {self.scan_end}")
        print(f"ðŸŽ¯ Patterns: {len(self.pattern_assignments)}")

    def get_trading_dates(self, start_date: str, end_date: str) -> List[str]:
        """Get all valid trading days between start and end date"""
        schedule = self.us_calendar.schedule(
            start_date=pd.to_datetime(start_date),
            end_date=pd.to_datetime(end_date)
        )
        trading_days = self.us_calendar.valid_days(
            start_date=pd.to_datetime(start_date),
            end_date=pd.to_datetime(end_date)
        )
        return [date.strftime('%Y-%m-%d') for date in trading_days]

    # ==================== STAGE 1: FETCH GROUPED DATA ====================

    def fetch_grouped_data(
        self,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        Stage 1: Fetch ALL data for ALL tickers using Edge Dev's fetch_all_grouped_data

        Uses the platform's centralized data fetching infrastructure.
        """
        print(f"\nðŸš€ STAGE 1: FETCH GROUPED DATA")
        print(f"ðŸ“¡ Fetching data from {start_date} to {end_date}...")

        try:
            from universal_scanner_engine.core.data_loader import fetch_all_grouped_data

            start_time = time.time()

            # Use Edge Dev's data loader
            df = fetch_all_grouped_data(
                tickers=None,  # None means fetch all available tickers
                start=start_date,
                end=end_date
            )

            elapsed = time.time() - start_time

            if df.empty:
                print("âŒ No data fetched!")
                return pd.DataFrame()

            print(f"\nðŸš€ Stage 1 Complete ({elapsed:.1f}s):")
            print(f"ðŸ“Š Total rows: {len(df):,}")
            print(f"ðŸ“Š Unique tickers: {df['ticker'].nunique():,}")
            print(f"ðŸ“… Date range: {df['date'].min()} to {df['date'].max()}")

            return df

        except Exception as e:
            print(f"âŒ Error fetching data: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

    # ==================== STAGE 2: APPLY SMART FILTERS ====================

    def apply_smart_filters(
        self,
        stage1_data: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Stage 2: Apply smart filters to reduce dataset by ~99%

        Args:
            stage1_data: Output from fetch_grouped_data

        Returns:
            Filtered DataFrame (stage2_data)
        """
        print(f"\nðŸ§  STAGE 2: APPLY SMART FILTERS")
        print(f"ðŸ“Š Input rows: {len(stage1_data):,}")

        if stage1_data.empty:
            return pd.DataFrame()

        df = stage1_data.copy()

        # Apply price filters
        if self.smart_filters["min_prev_close"]:
            df = df[df['close'] >= self.smart_filters["min_prev_close"]]

        if self.smart_filters["max_prev_close"]:
            df = df[df['close'] <= self.smart_filters["max_prev_close"]]

        # Apply volume filters
        if self.smart_filters["min_prev_volume"]:
            df = df[df['volume'] >= self.smart_filters["min_prev_volume"]]

        if self.smart_filters["max_prev_volume"]:
            df = df[df['volume'] <= self.smart_filters["max_prev_volume"]]

        print(f"ðŸ“Š Output rows: {len(df):,} ({len(df)/len(stage1_data)*100:.1f}% of original)")

        return df

    # ==================== ORIGINAL INDICATOR FUNCTION ====================
    def compute_indicators1(self, df):
        # Sorting by 'ticker' and 'date' to respect chronological order for each ticker
        df = df.sort_values(by=['ticker', 'date'])

        # Calculating previous day's close
        df['pdc'] = df.groupby('ticker')['c'].shift(1)

        # Calculating ranges and true range
        df['high_low'] = df['h'] - df['l']
        df['high_pdc'] = (df['h'] - df['pdc']).abs()
        df['low_pdc'] = (df['l'] - df['pdc']).abs()
        df['true_range'] = df[['high_low', 'high_pdc', 'low_pdc']].max(axis=1)
        df['atr'] = df.groupby('ticker')['true_range'].transform(lambda x: x.rolling(window=14).mean())

        df['atr'] = df['atr'].shift(1)

        # df['atr'] = df['atr'].fillna(0)
        # df['pdc'] = df['pdc'].fillna(0)

        df['d1_range'] = abs(df['h'] - df['l'])

        # Shifting values for high, close, open, low, volume
        for i in range(1, 4):
            df[f'h{i}'] = df.groupby('ticker')['h'].shift(i).fillna(0)
            # if i <= 2:  # Limiting to 2 days shift for close, open, low, volume
            df[f'c{i}'] = df.groupby('ticker')['c'].shift(i).fillna(0)
            df[f'o{i}'] = df.groupby('ticker')['o'].shift(i).fillna(0)
            df[f'l{i}'] = df.groupby('ticker')['l'].shift(i).fillna(0)
            df[f'v{i}'] = df.groupby('ticker')['v'].shift(i).fillna(0)

        # Dollar volume calculations and shifts
        df['dol_v'] = df['c'] * df['v']
        df['dol_v1'] = df.groupby('ticker')['dol_v'].shift(1)
        df['dol_v2'] = df.groupby('ticker')['dol_v'].shift(2)
        df['dol_v3'] = df.groupby('ticker')['dol_v'].shift(3)
        df['dol_v4'] = df.groupby('ticker')['dol_v'].shift(4)
        df['dol_v5'] = df.groupby('ticker')['dol_v'].shift(5)

        df['dol_v_cum5_1'] = df['dol_v1'] + df['dol_v2'] + df['dol_v3'] + df['dol_v3'] + df['dol_v5']

        # Close range calculations and shifts
        df['close_range'] = (df['c'] - df['l']) / (df['h'] - df['o'])
        df['close_range1'] = df.groupby('ticker')['close_range'].shift(1)
        df['close_range2'] = df.groupby('ticker')['close_range'].shift(2)

        # Days Range

        df['range'] = (df['h'] - df['l'])
        df['range1'] = df.groupby('ticker')['range'].shift(1)
        df['range2'] = df.groupby('ticker')['range'].shift(2)

        # Gap metrics related to ATR
        df['gap_pct'] = (df['o'] / df['pdc']) - 1
        df['gap_pct1'] = df.groupby('ticker')['gap_pct'].shift(1)
        df['gap_pct2'] = df.groupby('ticker')['gap_pct'].shift(2)
        df['gap_atr'] = ((df['o'] - df['pdc']) / df['atr'])
        df['gap_atr1'] = ((df['o1'] - df['c2']) / df['atr'])
        df['gap_atr2'] = ((df['o2'] - df['c3']) / df['atr'])
        df['gap_pdh_atr'] = ((df['o'] - df['h1']) / df['atr'])

        # High change metrics normalized by ATR
        df['pct_chg'] = (df['c'] / df['c1']) - 1
        df['high_pct_chg'] = (df['h'] / df['c1']) - 1
        df['pct_chg1'] = df.groupby('ticker')['pct_chg'].shift(1)
        df['high_pct_chg1'] = df.groupby('ticker')['high_pct_chg'].shift(1)
        df['high_pct_chg2'] = df.groupby('ticker')['high_pct_chg'].shift(2)

        df['high_chg'] = df['h'] - df['o']
        df['high_chg1'] = df['h1'] - df['o1']
        df['high_chg_atr'] = df['high_chg'] / df['atr']
        df['high_chg_atr1'] = ((df['h1'] - df['o1']) / df['atr'])
        df['high_chg_atr2'] = ((df['h2'] - df['o2']) / df['atr'])

        # High change from previous day close normalized by ATR
        df['high_chg_from_pdc_atr'] = ((df['h'] - df['c1']) / df['atr'])
        df['high_chg_from_pdc_atr1'] = ((df['h1'] - df['c2']) / df['atr'])

        # Percentage change in close price from the previous day
        df['pct_change'] = ((df['c'] / df['c1'] - 1) * 100).round(2)

        df['avg5_vol'] = df['v'].rolling(window=5).mean()
        df['rvol'] = df['v'] / df['avg5_vol']
        df['rvol1'] = df.groupby('ticker')['rvol'].shift(1)

        # Calculating EMAs
        for period in [9, 20, 50, 200]:
            df[f'ema{period}'] = df.groupby('ticker')['c'].transform(lambda x: x.ewm(span=period, adjust=False).mean().fillna(0))
            df[f'dist_h_{period}ema'] = df['h'] - df[f'ema{period}']
            df[f'dist_h_{period}ema_atr'] = df[f'dist_h_{period}ema'] / df['atr']

            df[f'dist_l_{period}ema'] = df['l'] - df[f'ema{period}']
            df[f'dist_l_{period}ema_atr'] = df[f'dist_l_{period}ema'] / df['atr']

            # Apply shifts to the calculated distances and normalize again by ATR
            for dist in range(1, 5):
                df[f'dist_h_{period}ema{dist}'] = df.groupby('ticker')[f'dist_h_{period}ema'].shift(dist)
                df[f'dist_h_{period}ema_atr{dist}'] = df[f'dist_h_{period}ema{dist}'] / df['atr']

                df[f'dist_l_{period}ema{dist}'] = df.groupby('ticker')[f'dist_l_{period}ema'].shift(dist)
                df[f'dist_l_{period}ema_atr{dist}'] = df[f'dist_l_{period}ema{dist}'] / df['atr']

        # Calculate rolling maximums and minimums
        for window in [5, 20, 50, 100, 250]:
            df[f'lowest_low_{window}'] = df.groupby('ticker')['l'].transform(lambda x: x.rolling(window=window, min_periods=1).min())
            df[f'highest_high_{window}'] = df.groupby('ticker')['h'].transform(lambda x: x.rolling(window=window, min_periods=1).max())

            # Shifting previous highs for selected windows
            for dist in range(1, 5):
                df[f'highest_high_{window}_{dist}'] = df.groupby('ticker')[f'highest_high_{window}'].shift(dist)

        # Calculate rolling minimums for the low prices with shifts
        df['lowest_low_30'] = df.groupby('ticker')['l'].transform(lambda x: x.rolling(window=30, min_periods=1).min())
        df['lowest_low_30_1'] = df.groupby('ticker')['lowest_low_30'].shift(1)

        # Calculate rolling maximums for high prices with multiple shifts
        df['highest_high_100_1'] = df.groupby('ticker')['highest_high_100'].shift(1)
        df['highest_high_100_4'] = df.groupby('ticker')['highest_high_100'].shift(4)
        df['highest_high_250_1'] = df.groupby('ticker')['highest_high_250'].shift(1)
        df['lowest_low_20_1'] = df.groupby('ticker')['lowest_low_20'].shift(1)
        df['lowest_low_20_2'] = df.groupby('ticker')['lowest_low_20'].shift(2)
        df['highest_high_20_1'] = df.groupby('ticker')['highest_high_20'].shift(1)
        df['highest_high_20_2'] = df.groupby('ticker')['highest_high_20'].shift(2)
        df['highest_high_5_1'] = df.groupby('ticker')['highest_high_5'].shift(1)

        # Assuming l_ua is a predefined column in your DataFrame
        df['lowest_low_20_ua'] = df.groupby('ticker')['l_ua'].transform(lambda x: x.rolling(window=20, min_periods=1).min())

        # Calculate distances from the lowest lows normalized by ATR
        df['h_dist_to_lowest_low_30'] = (df['h'] - df['lowest_low_30'])
        df['h_dist_to_lowest_low_30_atr'] = (df['h'] - df['lowest_low_30']) / df['atr']
        df['h_dist_to_lowest_low_20_atr'] = (df['h'] - df['lowest_low_20']) / df['atr']
        df['h_dist_to_lowest_low_5_atr'] = (df['h'] - df['lowest_low_5']) / df['atr']
        df['h_dist_to_highest_high_20_1_atr'] = (df['h'] - df['highest_high_20_1']) / df['atr']
        df['h_dist_to_highest_high_20_2_atr'] = (df['h'] - df['highest_high_20_2']) / df['atr']
        df['highest_high_5_dist_to_lowest_low_20_pct_1'] = (df['highest_high_5_1'] / df['lowest_low_20_1']) - 1
        df['h_dist_to_lowest_low_20_pct'] = (df['h'] / df['lowest_low_20']) - 1

        # Shifting EMAs
        df['ema20_2'] = df.groupby('ticker')['ema20'].shift(2)

        df['ema9_1'] = df['ema9'].shift(1)
        df['ema20_1'] = df['ema20'].shift(1)
        df['ema50_1'] = df['ema50'].shift(1)


        # df['ema9_1'] = df['ema9_1'].fillna(0)
        # df['ema20_1'] = df['ema20_1'].fillna(0)
        # df['ema50_1'] = df['ema50_1'].fillna(0)


        df['v_ua1'] = df.groupby('ticker')['v_ua'].shift(1)
        df['v_ua2'] = df.groupby('ticker')['v_ua'].shift(2)

        df['c_ua1'] = df['c_ua'].shift(1)


        # Drop intermediate columns
        columns_to_drop = ['high_low', 'high_pdc', 'low_pdc']
        df.drop(columns=columns_to_drop, inplace=True, errors='ignore')

        return df



    # ==================== STAGE 3: COMPUTE INDICATORS + DETECT PATTERNS ====================

    def compute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Compute all indicators needed for pattern detection

        This includes:
        - EMAs (9, 20, 21 periods)
        - ATR (Average True Range)
        - Previous values (shifts)
        - Gaps and ranges
        - Pattern-specific indicators
        """
        print(f"\nðŸ“Š COMPUTING INDICATORS...")
        print(f"ðŸ“Š Input rows: {len(df):,}")

        # Sort by ticker and date
        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)

        # ==================== COLUMN NAME MAPPING ====================
        # Map Polygon API column names to original scanner column names
        # Original scanners often use: o, h, l, c, v instead of open, high, low, close, volume
        column_mapping = {
            'open': 'o',
            'high': 'h',
            'low': 'l',
            'close': 'c',
            'volume': 'v'
        }

        # Create mapped columns for compatibility with original indicators
        for polygon_col, original_col in column_mapping.items():
            df[original_col] = df[polygon_col]

        # ==================== INDICATOR COMPUTATION ====================
        
        # ==================== USE ORIGINAL INDICATOR FUNCTION ====================
        # Using original scanner's compute_indicators1 function
        # This ensures all pattern-specific indicators are computed correctly

        # Initialize cumulative up-day counters and price/volume columns
        # c_ua: cumulative up-days for close (tracks consecutive days where close increased)
        # l_ua: actual low price (NOT cumulative - used for price calculations)
        # v_ua: actual volume (NOT cumulative - used for volume thresholds)
        df['c_ua'] = df.groupby('ticker')['c'].transform(
            lambda x: (x > x.shift(1)).cumsum()
        )
        df['l_ua'] = df['l']
        df['v_ua'] = df['v']

        # Apply original indicator computation
        df = self.compute_indicators1(df)


        print(f"âœ… Indicators computed: {len(df.columns)} columns")

        return df

    def detect_patterns(self, stage2_data: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 3: Detect all patterns

        This method detects all 3 patterns:
    - lc_frontside_d3_extended_1
    - lc_frontside_d2_extended
    - lc_frontside_d2_extended_1
        3 total patterns

        Args:
            stage2_data: Filtered DataFrame from apply_smart_filters

        Returns:
            DataFrame with all pattern matches
        """
        print(f"\nðŸŽ¯ STAGE 3: DETECT PATTERNS")
        print(f"ðŸ“Š Input rows: {len(stage2_data):,}")

        if stage2_data.empty:
            return pd.DataFrame()

        # Compute indicators
        df = self.compute_indicators(stage2_data)

        # Convert date for filtering
        df['Date'] = pd.to_datetime(df['date'])

        # Filter to D0 range
        df_d0 = df[
            (df['Date'] >= pd.Timestamp(self.d0_start)) &
            (df['Date'] <= pd.Timestamp(self.d0_end))
        ].copy()

        print(f"ðŸ“Š Rows after D0 filter: {len(df_d0):,}")

        # ==================== PATTERN DETECTION ====================
        print(f"\nðŸ” DETECTING {len(self.pattern_assignments)} PATTERNS...")

        # Initialize pattern columns
        for pattern in self.pattern_assignments:
            df_d0[pattern['name']] = 0

        # Apply each pattern detection logic
        for i, pattern in enumerate(self.pattern_assignments, 1):
            pattern_name = pattern['name']
            pattern_logic = pattern['logic']

            print(f"  [{i}/{len(self.pattern_assignments)}] Detecting {pattern_name}...")

            try:
                # Evaluate the pattern logic
                # Note: This is a simplified version - in production, you'd want
                # to parse and convert the logic to proper pandas operations
                mask = df_d0.eval(pattern_logic)
                df_d0.loc[mask, pattern_name] = 1

                signal_count = mask.sum()
                print(f"      âœ… Found {signal_count} signals")

            except Exception as e:
                print(f"      âš ï¸  Error detecting {pattern_name}: {e}")
                # Continue with next pattern

        # ==================== AGGREGATE RESULTS ====================
        print(f"\nðŸ“Š AGGREGATING RESULTS...")

        # Find all rows where ANY pattern matched
        pattern_columns = [p['name'] for p in self.pattern_assignments]
        df_d0['any_pattern'] = df_d0[pattern_columns].any(axis=1)

        # Filter to only rows with matches
        signals = df_d0[df_d0['any_pattern'] == True].copy()

        if signals.empty:
            print("âŒ No signals found!")
            return pd.DataFrame()

        # Build pattern labels for each signal
        def get_pattern_labels(row):
            matched_patterns = []
            for pattern in self.pattern_assignments:
                if row[pattern['name']] == 1:
                    matched_patterns.append(pattern['name'])
            return ', '.join(matched_patterns)

        signals['Scanner_Label'] = signals.apply(get_pattern_labels, axis=1)

        # Aggregate by ticker+date
        signals_aggregated = signals.groupby(['ticker', 'Date'])['Scanner_Label'].apply(
            lambda x: ', '.join(sorted(set(x)))
        ).reset_index()
        signals_aggregated.columns = ['Ticker', 'Date', 'Scanner_Label']

        print(f"ðŸ“Š Unique ticker+date combinations: {len(signals_aggregated):,}")
        print(f"ðŸ“Š Total pattern matches (including duplicates): {len(signals):,}")

        # Print pattern distribution
        pattern_counts = signals['Scanner_Label'].value_counts()
        print(f"\nðŸ“Š Signals by Pattern:")
        for pattern, count in pattern_counts.items():
            print(f"  â€¢ {pattern}: {count}")

        return signals_aggregated

    # ==================== STAGE 4: FORMAT RESULTS ====================

    def format_results(
        self,
        stage3_results: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Stage 4: Format results for display

        Args:
            stage3_results: Output from detect_patterns

        Returns:
            Formatted DataFrame with results
        """
        if stage3_results.empty:
            return pd.DataFrame()

        # Sort by date (chronological order)
        output = stage3_results.sort_values('Date').reset_index(drop=True)

        return output

    # ==================== STAGE 5: RUN SCAN ====================

    def run_scan(
        self,
        start_date: str = None,
        end_date: str = None
    ) -> pd.DataFrame:
        """
        Stage 5: Run complete scan pipeline

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)

        Returns:
            Formatted results DataFrame
        """
        # Use configured dates if not provided
        start_date = start_date or self.scan_start
        end_date = end_date or self.scan_end

        print(f"\n{'='*70}")
        print(f"ðŸš€ MULTI-PATTERN SCANNER: lc_d2_scan_oct_25_new_ideas_3")
        print(f"{'='*70}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data(start_date, end_date)

        if stage1_data.empty:
            print("âŒ No data available!")
            return pd.DataFrame()

        # Stage 2: Apply smart filters
        stage2_data = self.apply_smart_filters(stage1_data)

        # Stage 3: Detect patterns
        stage3_results = self.detect_patterns(stage2_data)

        if stage3_results.empty:
            print("âŒ No signals found!")
            return pd.DataFrame()

        # Stage 4: Format results
        formatted_results = self.format_results(stage3_results)

        # Print results
        print(f"\n{'='*70}")
        print(f"âœ… SCAN COMPLETE")
        print(f"{'='*70}")
        print(f"ðŸ“Š Final signals (D0 range): {len(formatted_results):,}")
        print(f"ðŸ“Š Unique tickers: {formatted_results['Ticker'].nunique():,}")

        # Print all results
        if len(formatted_results) > 0:
            print(f"\n{'='*70}")
            print("ðŸ“Š ALL SIGNALS:")
            print(f"{'='*70}")
            for idx, row in formatted_results.iterrows():
                print(f"  {row['Ticker']:6s} | {row['Date']} | {row['Scanner_Label']}")

        return formatted_results

    def run_and_save(self, output_path: str = "multi_scanner_results.csv") -> pd.DataFrame:
        """Execute scan and save results"""
        results = self.run_scan()

        if not results.empty:
            results.to_csv(output_path, index=False)
            print(f"âœ… Results saved to: {output_path}")

        return results


# ==================== MAIN ENTRY POINT ====================

if __name__ == "__main__":
    import sys

    scanner = lc_d2_scan_oct_25_new_ideas_3()

    results = scanner.run_and_save()

    print(f"\nâœ… Done!")
