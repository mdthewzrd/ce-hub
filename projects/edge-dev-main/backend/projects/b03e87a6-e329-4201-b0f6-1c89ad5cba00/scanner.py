"""
backside_para_b_copy_3

A daily scanner that identifies stocks showing a 'backside' pattern where the prior day (D-1) confirms strength relative to D-2, followed by a gap-up open on the trade day (D0) above the previous high.

Generated by RENATA_V2 - 2026-01-14 00:06:43

TRUE v31 Architecture - All 7 Core Pillars Implemented:
1. âœ… Market calendar (pandas_market_calendars)
2. âœ… Historical buffer calculation
3. âœ… Per-ticker operations (groupby().transform())
4. âœ… Historical/D0 separation in smart filters
5. âœ… Parallel processing (ThreadPoolExecutor)
6. âœ… Two-pass feature computation
7. âœ… Pre-sliced data for parallel processing
"""

# daily_para_backside_lite_scan.py
# Daily-only "A+ para, backside" scan â€” lite mold.
# Trigger: D-1 (or D-2) fits; trade day (D0) must gap & open > D-1 high.
# D-1 must take out D-2 high and close above D-2 close.
# Adds absolute D-1 volume floor: d1_volume_min.

import pandas_market_calendars as mcal
import pandas as pd, numpy as np, requests
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€
session  = requests.Session()
API_KEY  = "Fm7brz4s23eSocDErnL68cE7wspz2K1I"
BASE_URL = "https://api.polygon.io"
MAX_WORKERS = 6

PRINT_FROM = "2025-01-01"  # set None to keep all
PRINT_TO   = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ knobs â”€â”€â”€â”€â”€â”€â”€â”€â”€
P = {
    # hard liquidity / price
    "price_min"        : 8.0,
    "adv20_min_usd"    : 30_000_000,

    # backside context (absolute window)
    "abs_lookback_days": 1000,
    "abs_exclude_days" : 10,
    "pos_abs_max"      : 0.75,

    # trigger mold (evaluated on D-1 or D-2)
    "trigger_mode"     : "D1_or_D2",   # "D1_only" or "D1_or_D2"
    "atr_mult"         : .9,
    "vol_mult"         : 0.9,         # max(D-1 vol/avg, D-2 vol/avg)

    # Relative D-1 vol (optional). Set to None to disable.
    "d1_vol_mult_min"  : None,         # e.g., 1.25

    # NEW: Absolute D-1 volume floor (shares). Set None to disable.
    "d1_volume_min"    : 15_000_000,   # e.g., require â‰¥ 20M shares on D-1

    "slope5d_min"      : 3.0,
    "high_ema9_mult"   : 1.05,

    # trade-day (D0) gates
    "gap_div_atr_min"   : .75,
    "open_over_ema9_min": .9,
    "d1_green_atr_min"  : 0.30,
    "require_open_gt_prev_high": True,

    # relative requirement
    "enforce_d1_above_d2": True,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ universe â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYMBOLS = [
    'EW', 'JAMF', 'VNET', 'DYN', 'BITI', 'DOCN', 'FLNC', 'FLR', 'SHLS', 'DPRO', 'PATH', 'ARRY', 'SOXL', 'BULL', 'ZVRA', 'BKKT', 'ONTO', 'B', 'AMAT', 'CLSK', 'BIDU', 'BILI', 'TSLL', 'TSLR', 'TSLT', 'EDU', 'BABX', 'FRO', 'ESTC', 'TLRY', 'MRK', 'PDD', 'DLO', 'INTC', 'BZ', 'CRNC', 'ETHU', 'SOLT', 'XPEV', 'VSAT', 'CRVO', 'WRD', 'COUR', 'RKT', 'YETI', 'CLF', 'KSS', 'ETSY', 'YYAI', 'AI', 'BABA', 'SBSW', 'SOC', 'TAC', 'YINN', 'LI', 'REPL', 'SBET', 'USAR', 'TNXP', 'MLGO', 'TIGR', 'TREX', 'BEAM', 'SOUN', 'SNPS', 'METC', 'EL', 'CONL', 'RDDT', 'ZETA', 'ETHD', 'PCT', 'APA', 'CNQ', 'COP', 'EOG', 'EQNR', 'PR', 'USO', 'XOM', 'DV', 'SAIL', 'CRSP', 'HUT', 'IREN', 'AFRM', 'BNTX', 'GME', 'NNE', 'OKLO', 'BITO', 'BITU', 'NUKK', 'ACAD', 'AMD', 'KULR', 'NVDL', 'NVDX', 'FSLR', 'PSX', 'RUN', 'AES', 'ALGM', 'MBLY', 'MNSO', 'NXT', 'PBF', 'QUBT', 'RGTI', 'SEDG', 'ST', 'TRIP', 'VYX', 'ETH', 'ETHA', 'ETHE', 'EVH', 'FETH', 'LYFT', 'TTD', 'AS', 'WWW', 'BGC', 'VRNS', 'TEVA', 'EXAS', 'ANET', 'ATI', 'GSK', 'ODD', 'GRAL', 'CERT', 'DRIP', 'SCO', 'FAZ', 'FNGD', 'NVD', 'PLTD', 'PSQ', 'QID', 'RWM', 'SDOW', 'SDS', 'SH', 'SOXS', 'SPDN', 'SPXS', 'SPXU', 'SQQQ', 'SRTY', 'TECS', 'TZA', 'UVIX', 'UVXY', 'VIXY', 'VXX', 'YANG', 'HDB', 'TRNR', 'U', 'ZTO', 'NXPI', 'STM', 'BOIL', 'SMCI', 'SMCX', 'UNG', 'PGY', 'RXRX', 'HTHT', 'PENN', 'GRRR', 'DXC', 'FLG', 'SBUX', 'VFC', 'MT', 'ALLY', 'TEM'
    , 'QRVO', 'STX', 'GFI', 'HMY', 'ATEC', 'CTRA', 'DVN', 'GDS', 'KITT', 'LCID'
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ fetch â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_daily(tkr: str, start: str, end: str) -> pd.DataFrame:
    url = f"{BASE_URL}/v2/aggs/ticker/{tkr}/range/1/day/{start}/{end}"
    r   = session.get(url, params={"apiKey": API_KEY, "adjusted":"true", "sort":"asc", "limit":50000})
    r.raise_for_status()
    rows = r.json().get("results", [])
    if not rows: return pd.DataFrame()
    return (pd.DataFrame(rows)
            .assign(Date=lambda d: pd.to_datetime(d["t"], unit="ms", utc=True))
            .rename(columns={"o":"Open","h":"High","l":"Low","c":"Close","v":"Volume"})
            .set_index("Date")[["Open","High","Low","Close","Volume"]]
            .sort_index())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ metrics (lite) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def add_daily_metrics(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty: return df
    m = df.copy()
    try: m.index = m.index.tz_localize(None)
    except Exception: pass

    m["EMA_9"]  = m["Close"].ewm(span=9 , adjust=False).mean()
    m["EMA_20"] = m["Close"].ewm(span=20, adjust=False).mean()

    hi_lo   = m["High"] - m["Low"]
    hi_prev = (m["High"] - m["Close"].shift(1)).abs()
    lo_prev = (m["Low"]  - m["Close"].shift(1)).abs()
    m["TR"]      = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
    m["ATR_raw"] = m["TR"].rolling(14, min_periods=14).mean()
    m["ATR"]     = m["ATR_raw"].shift(1)

    m["VOL_AVG"]     = m["Volume"].rolling(14, min_periods=14).mean().shift(1)
    m["Prev_Volume"] = m["Volume"].shift(1)
    m["ADV20_$"]     = (m["Close"] * m["Volume"]).rolling(20, min_periods=20).mean().shift(1)

    m["Slope_9_5d"]  = (m["EMA_9"] - m["EMA_9"].shift(5)) / m["EMA_9"].shift(5) * 100
    m["High_over_EMA9_div_ATR"] = (m["High"] - m["EMA_9"]) / m["ATR"]

    m["Gap_abs"]       = (m["Open"] - m["Close"].shift(1)).abs()
    m["Gap_over_ATR"]  = m["Gap_abs"] / m["ATR"]
    m["Open_over_EMA9"]= m["Open"] / m["EMA_9"]

    m["Body_over_ATR"] = (m["Close"] - m["Open"]) / m["ATR"]

    m["Prev_Close"] = m["Close"].shift(1)
    m["Prev_Open"]  = m["Open"].shift(1)
    m["Prev_High"]  = m["High"].shift(1)
    return m

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def abs_top_window(df: pd.DataFrame, d0: pd.Timestamp, lookback_days: int, exclude_days: int):
    """Calculate absolute top window (v31 version uses df['date'] and snake_case)"""
    if df.empty: return (np.nan, np.nan)
    cutoff = d0 - pd.Timedelta(days=exclude_days)
    wstart = cutoff - pd.Timedelta(days=lookback_days)
    win = df[(df['date'] > wstart) & (df['date'] <= cutoff)]
    if win.empty: return (np.nan, np.nan)
    return float(win['low'].min()), float(win['high'].max())
def pos_between(val, lo, hi):
    if any(pd.isna(t) for t in (val, lo, hi)) or hi <= lo: return np.nan
    return max(0.0, min(1.0, float((val - lo) / (hi - lo))))

def _mold_on_row(rx: pd.Series, params: dict) -> bool:
    """
    Check if a row meets the trigger mold criteria

    Args:
        rx: Row to check
        params: Parameter dictionary (P dict)

    Returns:
        True if row meets all criteria
    """
    if pd.isna(rx.get("prev_close")) or pd.isna(rx.get("adv20_$")):
        return False
    if rx["prev_close"] < params["price_min"] or rx["adv20_$"] < params["adv20_min_usd"]:
        return False
    vol_avg = rx["vol_avg"]
    if pd.isna(vol_avg) or vol_avg <= 0: return False
    vol_sig = max(rx["volume"]/vol_avg, rx["prev_volume"]/vol_avg)
    checks = [
        (rx["tr"] / rx["atr"]) >= params["atr_mult"],
        vol_sig                 >= params["vol_mult"],
        rx["slope_9_5d"]        >= params["slope5d_min"],
        rx["high_over_ema9_div_atr"] >= params["high_ema9_mult"],
    ]
    return all(bool(x) and np.isfinite(x) for x in checks)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€ scan one symbol â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRUE v31 Scanner Class â”€â”€â”€â”€â”€â”€â”€â”€â”€
class backside_para_b_copy_3:
    """
    Daily Para Backside Lite

    A daily scanner that identifies stocks showing a 'backside' pattern where the prior day (D-1) confirms strength relative to D-2, followed by a gap-up open on the trade day (D0) above the previous high.

    Scanner Type: Standalone Scanner converted to TRUE v31 Architecture
    Generated: 2026-01-14 00:06:43

    TRUE v31 Architecture - All 7 Core Pillars Implemented
    """

    def __init__(self, api_key: str, d0_start: str, d0_end: str):
        """
        Initialize scanner with proper v31 configuration

        âœ… PILLAR 2: Calculate historical buffer for ABS window
        """
        self.api_key = api_key
        self.base_url = "https://api.polygon.io"

        # âœ… CRITICAL: Calculate historical buffer
        # Lookback = abs_lookback_days + 50 for safety margin
        lookback_buffer = 1050  # From detected params

        # Calculate scan_start with historical buffer
        scan_start_dt = pd.to_datetime(d0_start) - pd.Timedelta(days=lookback_buffer)
        self.scan_start = scan_start_dt.strftime('%Y-%m-%d')
        self.d0_start_user = d0_start
        self.d0_end_user = d0_end

        # âœ… Workers for parallel processing
        self.stage1_workers = 5
        self.stage3_workers = 10

        # âœ… Session pooling for API efficiency
        import requests
        self.session = requests.Session()
        from requests.adapters import HTTPAdapter
        self.session.mount('https://', HTTPAdapter(
            pool_connections=100,
            pool_maxsize=100
        ))

        # âœ… Parameters
        self.params = self._extract_parameters()

        # âœ… Scanner name for logging
        self.scanner_name = self.__class__.__name__

        print(f"ğŸ“Š Scanner initialized: {self.scanner_name}")
        print(f"   Historical buffer: {lookback_buffer} days")
        print(f"   Scan range: {self.scan_start} to {self.d0_end_user}")
        print(f"   D0 output range: {self.d0_start_user} to {self.d0_end_user}")

    def _extract_parameters(self) -> dict:
        """Extract parameters from P dict or return defaults"""
        # Try to extract from global scope
        try:
            import sys
            frame = sys._getframe(1)
            if 'P' in frame.f_locals:
                return frame.f_locals['P']
        except:
            pass

        # Default parameters with common backside B values
        # âœ… CRITICAL FIX: vol_mult changed from 2.0 to 0.9 to match working scanner
        # âœ… CRITICAL FIX: Added trigger_mode and d1_vol_mult_min (were missing)
        return {
            "price_min": 8.0,
            "adv20_min_usd": 30_000_000,
            "abs_lookback_days": 1000,
            "abs_exclude_days": 10,
            "pos_abs_max": 0.75,
            "atr_mult": 0.9,
            "vol_mult": 0.9,  # âœ… FIXED: Changed from 2.0 to 0.9
            "slope5d_min": 3.0,
            "high_ema9_mult": 1.05,
            "gap_div_atr_min": 0.75,
            "open_over_ema9_min": 0.9,
            "d1_green_atr_min": 0.30,
            "require_open_gt_prev_high": True,
            "enforce_d1_above_d2": True,
            "d1_volume_min": 15_000_000,
            "trigger_mode": "D1_or_D2",  # âœ… FIXED: Added missing parameter
            "d1_vol_mult_min": None,  # âœ… FIXED: Added missing parameter
        }

    def run_scan(self):
        """
        ğŸš€ Main execution: 5-stage v31 pipeline

        Returns:
            List of signal dictionaries
        """
        print(f"\n======================================================================")
        print(f"ğŸš€ RUNNING TRUE v31 SCAN: {self.scanner_name}")
        print(f"======================================================================")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data()
        if stage1_data is None or stage1_data.empty:
            print("\nâŒ Scan failed: No data loaded")
            return []

        # Stage 2a: Compute simple features
        stage2a_data = self.compute_simple_features(stage1_data)

        # Stage 2b: Apply smart filters (with historical/D0 separation)
        stage2b_data = self.apply_smart_filters(stage2a_data)

        # Stage 3a: Compute full features
        stage3a_data = self.compute_full_features(stage2b_data)

        # Stage 3b: Detect patterns (with pre-sliced parallel processing)
        stage3_results = self.detect_patterns(stage3a_data)

        print(f"\nâœ… SCAN COMPLETE: {len(stage3_results)} signals detected")
        return stage3_results

    def fetch_grouped_data(self):
        """
        âœ… PILLAR 1: Market calendar integration
        âœ… PILLAR 5: Parallel processing

        Stage 1: Fetch ALL tickers for ALL dates using grouped endpoint
        """
        import pandas_market_calendars as mcal
        from concurrent.futures import ThreadPoolExecutor, as_completed

        # âœ… PILLAR 1: Use market calendar (NOT weekday checks)
        nyse = mcal.get_calendar('NYSE')
        trading_dates = nyse.schedule(
            start_date=self.scan_start,
            end_date=self.d0_end_user
        ).index.strftime('%Y-%m-%d').tolist()

        print(f"  ğŸ“… Fetching {len(trading_dates)} trading days from {self.scan_start} to {self.d0_end_user}")
        print(f"  ğŸŒ Using grouped endpoint (1 call per day)")
        print(f"  âš™ï¸  Parallel workers: {self.stage1_workers}")

        all_data = []

        # âœ… PILLAR 5: Parallel fetching
        with ThreadPoolExecutor(max_workers=self.stage1_workers) as executor:
            future_to_date = {
                executor.submit(self._fetch_grouped_day, date_str): date_str
                for date_str in trading_dates
            }

            for future in as_completed(future_to_date):
                date_str = future_to_date[future]
                try:
                    data = future.result()
                    if data is not None and not data.empty:
                        all_data.append(data)
                        print(f"    âœ… {date_str}: {len(data)} records")
                except Exception as e:
                    print(f"    âš ï¸  {date_str}: {e}")

        if all_data:
            result = pd.concat(all_data, ignore_index=True)
            print(f"  âœ… Stage 1 complete: {len(result)} total records")
            return result
        else:
            print(f"  âš ï¸  Stage 1: No data retrieved")
            return pd.DataFrame()

    def _fetch_grouped_day(self, date_str: str):
        """
        Fetch ALL tickers for ONE day using grouped endpoint
        """
        url = f"{self.base_url}/v2/aggs/grouped/locale/us/market/stocks/{date_str}"
        response = self.session.get(url, params={'apiKey': self.api_key, 'adjust': 'true'})

        if response.status_code != 200:
            print(f"    âš ï¸  API error {response.status_code} for {date_str}")
            return None

        data = response.json()
        if "results" not in data or not data["results"]:
            return None

        # Convert to DataFrame
        all_data = []
        for result in data["results"]:
            all_data.append({
                "ticker": result.get("T"),
                "date": date_str,
                "open": result.get("o"),
                "high": result.get("h"),
                "low": result.get("l"),
                "close": result.get("c"),
                "volume": result.get("v"),
            })

        return pd.DataFrame(all_data)

    def compute_simple_features(self, df: pd.DataFrame):
        """
        âœ… PILLAR 3: Per-ticker operations
        âœ… PILLAR 6: Two-pass feature computation (simple first)

        Stage 2a: Compute SIMPLE features for efficient filtering

        Only computes features needed for filtering:
        - prev_close
        - adv20_usd (with per-ticker groupby)
        - price_range
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 2a: Computing simple features for {len(df)} rows")

        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'])

        # Previous close
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)

        # âœ… PILLAR 3: Per-ticker operations for ADV20
        df['adv20_usd'] = (df['close'] * df['volume']).groupby(df['ticker']).transform(
            lambda x: x.rolling(window=20, min_periods=20).mean()
        )

        # Price range
        df['price_range'] = df['high'] - df['low']

        print(f"    âœ… Simple features computed")
        return df

    def apply_smart_filters(self, df: pd.DataFrame):
        """
        âœ… PILLAR 4: Separate historical from D0 data

        Stage 2b: Smart filters with HISTORICAL DATA PRESERVATION

        CRITICAL: Only filter D0 output range, preserve all historical data
        for ABS window calculations.
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 2b input: {len(df)} rows")

        # âœ… PILLAR 4: Split historical from D0
        df_historical = df[~df['date'].between(self.d0_start_user, self.d0_end_user)].copy()
        df_output_range = df[df['date'].between(self.d0_start_user, self.d0_end_user)].copy()

        print(f"    ğŸ“Š Historical: {len(df_historical)} rows")
        print(f"    ğŸ“Š D0 range: {len(df_output_range)} rows")

        # âœ… CRITICAL: Filter ONLY D0 range
        df_output_filtered = df_output_range.copy()

        # Price filter
        if 'price_min' in self.params:
            min_price = self.params['price_min']
            df_output_filtered = df_output_filtered[
                (df_output_filtered['close'] >= min_price) &
                (df_output_filtered['open'] >= min_price)
            ]

        # Volume filter
        if 'adv20_min_usd' in self.params:
            min_adv = self.params['adv20_min_usd']
            df_output_filtered = df_output_filtered[
                df_output_filtered['adv20_usd'] >= min_adv
            ]

        print(f"    ğŸ“Š After filters: {len(df_output_filtered)} rows")

        # âœ… CRITICAL: COMBINE historical + filtered D0
        df_combined = pd.concat([df_historical, df_output_filtered], ignore_index=True)

        print(f"  âœ… Stage 2b complete: {len(df_combined)} rows (historical preserved)")
        return df_combined

    def compute_full_features(self, df: pd.DataFrame):
        """
        âœ… PILLAR 3: Per-ticker operations
        âœ… PILLAR 6: Two-pass feature computation (full features after filter)

        Stage 3a: Compute ALL technical indicators

        Computes expensive features only on data that passed filters.
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 3a: Computing full features for {len(df)} rows")

        result_dfs = []

        for ticker, group in df.groupby('ticker'):
            group = group.sort_values('date').copy()

            # EMA
            group['ema_9'] = group['close'].ewm(span=9, adjust=False).mean()
            group['ema_20'] = group['close'].ewm(span=20, adjust=False).mean()

            # ATR
            hi_lo = group['high'] - group['low']
            hi_prev = (group['high'] - group['close'].shift(1)).abs()
            lo_prev = (group['low'] - group['close'].shift(1)).abs()
            group['tr'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
            group['atr_raw'] = group['tr'].rolling(14, min_periods=14).mean()
            group['atr'] = group['atr_raw'].shift(1)

            # âœ… FIX #9: Add missing volume metrics for _mold_on_row
            group['vol_avg'] = group['volume'].rolling(14, min_periods=14).mean().shift(1)
            group['prev_volume'] = group['volume'].shift(1)
            group['adv20_$'] = (group['close'] * group['volume']).rolling(20, min_periods=20).mean().shift(1)

            # âœ… FIX #9: Add slope calculation
            group['slope_9_5d'] = (group['ema_9'] - group['ema_9'].shift(5)) / group['ema_9'].shift(5) * 100

            # âœ… FIX #9: Add high over EMA9 div ATR (needed by _mold_on_row)
            group['high_over_ema9_div_atr'] = (group['high'] - group['ema_9']) / group['atr']

            # âœ… FIX #9: Add gap metrics
            group['gap_abs'] = (group['open'] - group['close'].shift(1)).abs()
            group['gap_over_atr'] = group['gap_abs'] / group['atr']
            group['open_over_ema9'] = group['open'] / group['ema_9']

            # âœ… FIX #9: Add body over ATR
            group['body_over_atr'] = (group['close'] - group['open']) / group['atr']

            # âœ… FIX #9: Add previous values
            group['prev_close'] = group['close'].shift(1)
            group['prev_open'] = group['open'].shift(1)
            group['prev_high'] = group['high'].shift(1)

            result_dfs.append(group)

        result = pd.concat(result_dfs, ignore_index=True)
        print(f"    âœ… Full features computed")
        return result

    def detect_patterns(self, df: pd.DataFrame):
        """
        âœ… PILLAR 7: Pre-sliced data for parallel processing
        âœ… PILLAR 5: Parallel ticker processing

        Stage 3b: Pattern detection with parallel processing
        """
        if df.empty:
            return []

        print(f"  ğŸ¯ Stage 3b: Detecting patterns in {len(df)} rows")

        # Get D0 range
        d0_start_dt = pd.to_datetime(self.d0_start_user)
        d0_end_dt = pd.to_datetime(self.d0_end_user)

        # âœ… PILLAR 7: Pre-slice ticker data BEFORE parallel processing
        ticker_data_list = []
        for ticker, ticker_df in df.groupby('ticker'):
            ticker_data_list.append((ticker, ticker_df.copy(), d0_start_dt, d0_end_dt))

        all_results = []

        # âœ… PILLAR 5: Parallel processing with pre-sliced data
        from concurrent.futures import ThreadPoolExecutor, as_completed

        with ThreadPoolExecutor(max_workers=self.stage3_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_ticker_optimized_pre_sliced, ticker_data): ticker_data[0]
                for ticker_data in ticker_data_list
            }

            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                        print(f"    âœ… {ticker}: {len(results)} signals")
                except Exception as e:
                    print(f"    âš ï¸  {ticker}: {e}")

        print(f"  âœ… Stage 3b complete: {len(all_results)} total signals")
        return all_results

    def _process_ticker_optimized_pre_sliced(self, ticker_data: tuple):
        """
        âœ… PILLAR 7: Process pre-sliced ticker data
        âœ… FIX #10: Use snake_case throughout (matching compute_full_features)
        âœ… PILLAR 4: Early D0 filtering

        Process pre-sliced ticker data with early D0 range filtering.
        Uses data directly from compute_full_features (no renaming, no recomputation).
        """
        ticker, ticker_df, d0_start_dt, d0_end_dt = ticker_data

        # âœ… Minimum data check
        if len(ticker_df) < 100:
            return []

        # âœ… FIX #10: Sort and prepare data (keep snake_case column names)
        # All features are already computed by compute_full_features()
        ticker_df = ticker_df.sort_values('date').reset_index(drop=True)

        # âœ… FIX #10: Convert date to datetime for comparison
        ticker_df['date'] = pd.to_datetime(ticker_df['date'])

        # Get parameters
        try:
            P_local = self.params
        except:
            P_local = {
                "price_min": 8.0,
                "adv20_min_usd": 30_000_000,
                "abs_lookback_days": 1000,
                "abs_exclude_days": 10,
                "pos_abs_max": 0.75,
                "atr_mult": 0.9,
                "vol_mult": 0.9,  # âœ… FIX 15: Changed from 2.0 to 0.9 to match working scanner
                "slope5d_min": 3.0,
                "high_ema9_mult": 1.05,
                "gap_div_atr_min": 0.75,
                "open_over_ema9_min": 0.9,
                "d1_green_atr_min": 0.30,
                "require_open_gt_prev_high": True,
                "enforce_d1_above_d2": True,
                "d1_volume_min": 15_000_000,
                "trigger_mode": "D1_or_D2",  # âœ… FIX 15: Added missing trigger_mode param
                "d1_vol_mult_min": None,  # âœ… FIX 15: Added missing d1_vol_mult_min param
            }

        # âœ… Initialize results list (detection loop appends to this)
        all_rows = []

        # âœ… FIX #10: Detection loop with early D0 filtering
        # Use ticker_df directly with snake_case column names (all features computed)
        for i in range(2, len(ticker_df)):
            d0 = ticker_df.iloc[i]['date']

            # âœ… PILLAR 4: EARLY FILTER - Skip if not in D0 range
            if d0 < d0_start_dt or d0 > d0_end_dt:
                continue

            r0 = ticker_df.iloc[i]
            r1 = ticker_df.iloc[i-1]
            r2 = ticker_df.iloc[i-2]

            # Original detection logic from scan_symbol
            # This uses the extracted detection loop BODY (content inside the for loop)
            # The extracted body should NOT include its own for loop statement
            try:
                    # Backside vs D-1 close
                    lo_abs, hi_abs = abs_top_window(ticker_df, d0, P_local["abs_lookback_days"], P_local["abs_exclude_days"])
                    pos_abs_prev = pos_between(r1["close"], lo_abs, hi_abs)
                    if not (pd.notna(pos_abs_prev) and pos_abs_prev <= P_local["pos_abs_max"]):
                        continue

                    # Choose trigger
                    trigger_ok = False; trig_row = None; trig_tag = "-"
                    if P_local["trigger_mode"] == "D1_only":
                        if _mold_on_row(r1, P_local): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
                    else:
                        if _mold_on_row(r1, P_local): trigger_ok, trig_row, trig_tag = True, r1, "D-1"
                        elif _mold_on_row(r2, P_local): trigger_ok, trig_row, trig_tag = True, r2, "D-2"
                    if not trigger_ok:
                        continue

                    # D-1 must be green
                    if not (pd.notna(r1["body_over_atr"]) and r1["body_over_atr"] >= P_local["d1_green_atr_min"]):
                        continue

                    # Absolute D-1 volume floor (shares)
                    if P_local["d1_volume_min"] is not None:
                        if not (pd.notna(r1["volume"]) and r1["volume"] >= P_local["d1_volume_min"]):
                            continue

                    # Optional relative D-1 vol multiple
                    if P_local["d1_vol_mult_min"] is not None:
                        if not (pd.notna(r1["vol_avg"]) and r1["vol_avg"] > 0 and (r1["volume"]/r1["vol_avg"]) >= P_local["d1_vol_mult_min"]):
                            continue

                    # D-1 > D-2 highs & close
                    if P_local["enforce_d1_above_d2"]:
                        if not (pd.notna(r1["high"]) and pd.notna(r2["high"]) and r1["high"] > r2["high"]
                                and pd.notna(r1["close"]) and pd.notna(r2["close"]) and r1["close"] > r2["close"]):
                            continue

                    # D0 gates
                    if pd.isna(r0["gap_over_atr"]) or r0["gap_over_atr"] < P_local["gap_div_atr_min"]:
                        continue
                    if P_local["require_open_gt_prev_high"] and not (r0["open"] > r1["prev_high"]):
                        continue
                    if pd.isna(r0["open_over_ema9"]) or r0["open_over_ema9"] < P_local["open_over_ema9_min"]:
                        continue

                    d1_vol_mult = (r1["volume"]/r1["vol_avg"]) if (pd.notna(r1["vol_avg"]) and r1["vol_avg"]>0) else np.nan
                    volsig_max  = (max(r1["volume"]/r1["vol_avg"], r2["volume"]/r2["vol_avg"])
                                   if (pd.notna(r1["vol_avg"]) and r1["vol_avg"]>0 and pd.notna(r2["vol_avg"]) and r2["vol_avg"]>0)
                                   else np.nan)

                    all_rows.append({
                        "Ticker": ticker,
                        "Date": d0.strftime("%Y-%m-%d"),
                        "Trigger": trig_tag,
                        "PosAbs_1000d": round(float(pos_abs_prev), 3),
                        "D1_Body/atr": round(float(r1["body_over_atr"]), 2),
                        "D1Vol(shares)": int(r1["volume"]) if pd.notna(r1["volume"]) else np.nan,   # absolute volume
                        "D1Vol/Avg": round(float(d1_vol_mult), 2) if pd.notna(d1_vol_mult) else np.nan,
                        "VolSig(max D-1,D-2)/Avg": round(float(volsig_max), 2) if pd.notna(volsig_max) else np.nan,
                        "Gap/atr": round(float(r0["gap_over_atr"]), 2),
                        "open>PrevHigh": bool(r0["open"] > r1["prev_high"]),
                        "open/EMA9": round(float(r0["open_over_ema9"]), 2),
                        "D1>H(D-2)": bool(r1["high"] > r2["high"]),
                        "D1Close>D2Close": bool(r1["close"] > r2["close"]),
                        "Slope9_5d": round(float(r0["slope_9_5d"]), 2) if pd.notna(r0["slope_9_5d"]) else np.nan,
                        "high-EMA9/atr(trigger)": round(float(trig_row["high_over_ema9_div_atr"]), 2),
                        "adv20_$": round(float(r0["adv20_$"])) if pd.notna(r0["adv20_$"]) else np.nan,
                    })
            except:
                pass

        return all_rows

    def format_results(self, results: list) -> pd.DataFrame:
        """
        Format detection results for output

        Args:
            results: List of detection result dictionaries

        Returns:
            DataFrame with formatted results
        """
        if not results:
            return pd.DataFrame()

        df = pd.DataFrame(results)

        # Reorder columns for better readability
        column_order = ['ticker', 'date', 'close', 'gap', 'gap/atr', 'atr', 'adv20_usd']
        column_order = [col for col in column_order if col in df.columns]

        # Add any additional columns that aren't in the standard order
        additional_cols = [col for col in df.columns if col not in column_order]
        column_order.extend(additional_cols)

        return df[column_order]



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Module-level functions for backend integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_scan(d0_start=None, d0_end=None):
    # Module-level function for backend integration
    # Returns simple ticker+date results
    import os

    scanner = backside_para_b_copy_3(
        api_key=os.getenv('POLYGON_API_KEY', 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'),
        d0_start=d0_start or "2024-01-01",
        d0_end=d0_end or "2024-12-31"
    )

    results = scanner.run_scan()

    # Return simple results: just ticker and date
    simple_results = []
    for r in results:
        ticker_val = r.get('Ticker', r.get('ticker', 'N/A'))
        date_val = r.get('Date', r.get('date', 'N/A'))
        simple_results.append({'ticker': ticker_val, 'date': date_val})

    return simple_results


def main():
    # Main function for backend integration
    # Returns simple ticker+date results
    return run_scan()


if __name__ == "__main__":
    import os

    # Example usage
    scanner = backside_para_b_copy_3(
        api_key=os.getenv('POLYGON_API_KEY', 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'),
        d0_start="2024-01-01",
        d0_end="2024-12-31"
    )

    results = scanner.run_scan()

    if results:
        print(f"\nResults ({len(results)} signals):")
        for r in results:
            ticker_val = r.get('Ticker', r.get('ticker', 'N/A'))
            date_val = r.get('Date', r.get('date', 'N/A'))
            print(f"{ticker_val} {date_val}")
    else:
        print("\nNo results found.")
