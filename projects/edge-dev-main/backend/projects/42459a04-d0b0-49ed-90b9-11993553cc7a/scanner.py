"""
half_A_scan_copy

Multi-Pattern Scanner - 0 patterns

Generated by RENATA_V2 - Multi-Scanner to V31 Transformation
Date Range: 2024-01-01 to 2024-12-31
Patterns: close_d3, close_d4

V31 Architecture:
1. Market calendar integration (pandas_market_calendars)
2. Historical buffer calculation
3. Per-ticker operations (groupby().transform())
4. Historical/D0 separation in smart filters
5. Parallel processing (ThreadPoolExecutor)
6. Two-pass feature computation
7. Pre-sliced data for parallel processing
"""

import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas_market_calendars as mcal
from typing import List, Dict, Any, Optional


class half_A_scan_copy:
    """
    Multi-Pattern Scanner with 0 pattern detectors

    Patterns:

    0 total patterns

    Scanner Type: multi
    Generated: {datetime.now().isoformat()}
    """

    def __init__(
        self,
        d0_start: str = None,
        d0_end: str = None
    ):
        """Initialize multi-scanner"""

        # Date configuration
        self.d0_start = d0_start or "2024-01-01"
        self.d0_end = d0_end or "2024-12-31"

        # Historical buffer for indicator computation
        self.historical_buffer_days = 1050  # ~2 years of trading days

        # Scan range (historical data needed for indicators)
        self.scan_start = (
            pd.Timestamp(self.d0_start) - pd.Timedelta(days=self.historical_buffer_days)
        ).strftime('%Y-%m-%d')
        self.scan_end = self.d0_end

        # Worker configuration
        self.stage3_workers = 10  # Parallel pattern detection

        # Market calendar
        self.us_calendar = mcal.get_calendar('NYSE')

        # Smart filter configuration (universal filters)
        self.smart_filters = {
            "min_prev_close": 0.75,
            "max_prev_close": 1000,
            "min_prev_volume": 500000,
            "max_prev_volume": 100000000
        }

        # Pattern assignments (extracted from original scanner)
        self.pattern_assignments = []

        # Results storage
        self.all_results = []
        self.stage1_data = None
        self.stage2_data = None
        self.stage3_results = None

        print(f"ðŸš€ MULTI-PATTERN SCANNER: half_A_scan_copy")
        print(f"ðŸ“… Signal Output Range (D0): {self.d0_start} to {self.d0_end}")
        print(f"ðŸ“Š Historical Data Range: {self.scan_start} to {self.scan_end}")
        print(f"ðŸŽ¯ Patterns: {len(self.pattern_assignments)}")

    def get_trading_dates(self, start_date: str, end_date: str) -> List[str]:
        """Get all valid trading days between start and end date"""
        schedule = self.us_calendar.schedule(
            start_date=pd.to_datetime(start_date),
            end_date=pd.to_datetime(end_date)
        )
        trading_days = self.us_calendar.valid_days(
            start_date=pd.to_datetime(start_date),
            end_date=pd.to_datetime(end_date)
        )
        return [date.strftime('%Y-%m-%d') for date in trading_days]

    # ==================== STAGE 1: FETCH GROUPED DATA ====================

    def fetch_grouped_data(
        self,
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        Stage 1: Fetch ALL data for ALL tickers using Edge Dev's fetch_all_grouped_data

        Uses the platform's centralized data fetching infrastructure.
        """
        print(f"\nðŸš€ STAGE 1: FETCH GROUPED DATA")
        print(f"ðŸ“¡ Fetching data from {start_date} to {end_date}...")

        try:
            from universal_scanner_engine.core.data_loader import fetch_all_grouped_data

            start_time = time.time()

            # Use Edge Dev's data loader
            df = fetch_all_grouped_data(
                tickers=None,  # None means fetch all available tickers
                start=start_date,
                end=end_date
            )

            elapsed = time.time() - start_time

            if df.empty:
                print("âŒ No data fetched!")
                return pd.DataFrame()

            print(f"\nðŸš€ Stage 1 Complete ({elapsed:.1f}s):")
            print(f"ðŸ“Š Total rows: {len(df):,}")
            print(f"ðŸ“Š Unique tickers: {df['ticker'].nunique():,}")
            print(f"ðŸ“… Date range: {df['date'].min()} to {df['date'].max()}")

            return df

        except Exception as e:
            print(f"âŒ Error fetching data: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

    # ==================== STAGE 2: APPLY SMART FILTERS ====================

    def apply_smart_filters(
        self,
        stage1_data: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Stage 2: Apply smart filters to reduce dataset by ~99%

        Args:
            stage1_data: Output from fetch_grouped_data

        Returns:
            Filtered DataFrame (stage2_data)
        """
        print(f"\nðŸ§  STAGE 2: APPLY SMART FILTERS")
        print(f"ðŸ“Š Input rows: {len(stage1_data):,}")

        if stage1_data.empty:
            return pd.DataFrame()

        df = stage1_data.copy()

        # Apply price filters
        if self.smart_filters["min_prev_close"]:
            df = df[df['close'] >= self.smart_filters["min_prev_close"]]

        if self.smart_filters["max_prev_close"]:
            df = df[df['close'] <= self.smart_filters["max_prev_close"]]

        # Apply volume filters
        if self.smart_filters["min_prev_volume"]:
            df = df[df['volume'] >= self.smart_filters["min_prev_volume"]]

        if self.smart_filters["max_prev_volume"]:
            df = df[df['volume'] <= self.smart_filters["max_prev_volume"]]

        print(f"ðŸ“Š Output rows: {len(df):,} ({len(df)/len(stage1_data)*100:.1f}% of original)")

        return df

    # ==================== ORIGINAL INDICATOR FUNCTION ====================


    # ==================== STAGE 3: COMPUTE INDICATORS + DETECT PATTERNS ====================

    def compute_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Compute all indicators needed for pattern detection

        This includes:
        - EMAs (9, 20, 21 periods)
        - ATR (Average True Range)
        - Previous values (shifts)
        - Gaps and ranges
        - Pattern-specific indicators
        """
        print(f"\nðŸ“Š COMPUTING INDICATORS...")
        print(f"ðŸ“Š Input rows: {len(df):,}")

        # Sort by ticker and date
        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)

        # ==================== COLUMN NAME MAPPING ====================
        # Map Polygon API column names to original scanner column names
        # Original scanners often use: o, h, l, c, v instead of open, high, low, close, volume
        column_mapping = {
            'open': 'o',
            'high': 'h',
            'low': 'l',
            'close': 'c',
            'volume': 'v'
        }

        # Create mapped columns for compatibility with original indicators
        for polygon_col, original_col in column_mapping.items():
            df[original_col] = df[polygon_col]

        # ==================== INDICATOR COMPUTATION ====================
        
        # ==================== GENERIC INDICATORS ====================
        # Using generic indicators (original indicator function not available)

        # EMAs
        df['ema_9'] = df.groupby('ticker')['close'].transform(
            lambda x: x.ewm(span=9, adjust=False).mean()
        )
        df['ema_20'] = df.groupby('ticker')['close'].transform(
            lambda x: x.ewm(span=20, adjust=False).mean()
        )
        df['ema_21'] = df.groupby('ticker')['close'].transform(
            lambda x: x.ewm(span=21, adjust=False).mean()
        )

        # True Range and ATR
        df['tr'] = np.maximum(
            df['high'] - df['low'],
            np.maximum(
                abs(df['high'] - df['close'].shift(1)),
                abs(df['low'] - df['close'].shift(1))
            )
        )
        df['atr'] = df.groupby('ticker')['tr'].transform(
            lambda x: x.ewm(span=14, adjust=False).mean()
        )
        df['atr_9'] = df.groupby('ticker')['tr'].transform(
            lambda x: x.ewm(span=9, adjust=False).mean()
        )
        df['atr_21'] = df.groupby('ticker')['tr'].transform(
            lambda x: x.ewm(span=21, adjust=False).mean()
        )

        # Previous values
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)
        df['prev_open'] = df.groupby('ticker')['open'].shift(1)
        df['prev_high'] = df.groupby('ticker')['high'].shift(1)
        df['prev_low'] = df.groupby('ticker')['low'].shift(1)
        df['prev_volume'] = df.groupby('ticker')['volume'].shift(1)

        # Shifted values (D-2, D-3, etc.)
        for i in range(2, 6):
            df[f'close_{{i}}'] = df.groupby('ticker')['close'].shift(i)
            df[f'high_{{i}}'] = df.groupby('ticker')['high'].shift(i)
            df[f'low_{{i}}'] = df.groupby('ticker')['low'].shift(i)
            df[f'open_{{i}}'] = df.groupby('ticker')['open'].shift(i)
            df[f'volume_{{i}}'] = df.groupby('ticker')['volume'].shift(i)

        # Gaps and ranges
        df['gap'] = (df['open'] / df['prev_close']) - 1
        df['range'] = df['high'] - df['low']
        df['prev_range'] = df.groupby('ticker')['range'].shift(1)


        print(f"âœ… Indicators computed: {len(df.columns)} columns")

        return df

    def detect_patterns(self, stage2_data: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 3: Detect all patterns

        This method detects all 0 patterns:

        0 total patterns

        Args:
            stage2_data: Filtered DataFrame from apply_smart_filters

        Returns:
            DataFrame with all pattern matches
        """
        print(f"\nðŸŽ¯ STAGE 3: DETECT PATTERNS")
        print(f"ðŸ“Š Input rows: {len(stage2_data):,}")

        if stage2_data.empty:
            return pd.DataFrame()

        # Compute indicators
        df = self.compute_indicators(stage2_data)

        # Convert date for filtering
        df['Date'] = pd.to_datetime(df['date'])

        # Filter to D0 range
        df_d0 = df[
            (df['Date'] >= pd.Timestamp(self.d0_start)) &
            (df['Date'] <= pd.Timestamp(self.d0_end))
        ].copy()

        print(f"ðŸ“Š Rows after D0 filter: {len(df_d0):,}")

        # ==================== PATTERN DETECTION ====================
        print(f"\nðŸ” DETECTING {len(self.pattern_assignments)} PATTERNS...")

        # Initialize pattern columns
        for pattern in self.pattern_assignments:
            df_d0[pattern['name']] = 0

        # Apply each pattern detection logic
        for i, pattern in enumerate(self.pattern_assignments, 1):
            pattern_name = pattern['name']
            pattern_logic = pattern['logic']

            print(f"  [{i}/{len(self.pattern_assignments)}] Detecting {pattern_name}...")

            try:
                # Evaluate the pattern logic
                # Note: This is a simplified version - in production, you'd want
                # to parse and convert the logic to proper pandas operations
                mask = df_d0.eval(pattern_logic)
                df_d0.loc[mask, pattern_name] = 1

                signal_count = mask.sum()
                print(f"      âœ… Found {signal_count} signals")

            except Exception as e:
                print(f"      âš ï¸  Error detecting {pattern_name}: {e}")
                # Continue with next pattern

        # ==================== AGGREGATE RESULTS ====================
        print(f"\nðŸ“Š AGGREGATING RESULTS...")

        # Find all rows where ANY pattern matched
        pattern_columns = [p['name'] for p in self.pattern_assignments]
        df_d0['any_pattern'] = df_d0[pattern_columns].any(axis=1)

        # Filter to only rows with matches
        signals = df_d0[df_d0['any_pattern'] == True].copy()

        if signals.empty:
            print("âŒ No signals found!")
            return pd.DataFrame()

        # Build pattern labels for each signal
        def get_pattern_labels(row):
            matched_patterns = []
            for pattern in self.pattern_assignments:
                if row[pattern['name']] == 1:
                    matched_patterns.append(pattern['name'])
            return ', '.join(matched_patterns)

        signals['Scanner_Label'] = signals.apply(get_pattern_labels, axis=1)

        # Aggregate by ticker+date
        signals_aggregated = signals.groupby(['ticker', 'Date'])['Scanner_Label'].apply(
            lambda x: ', '.join(sorted(set(x)))
        ).reset_index()
        signals_aggregated.columns = ['Ticker', 'Date', 'Scanner_Label']

        print(f"ðŸ“Š Unique ticker+date combinations: {len(signals_aggregated):,}")
        print(f"ðŸ“Š Total pattern matches (including duplicates): {len(signals):,}")

        # Print pattern distribution
        pattern_counts = signals['Scanner_Label'].value_counts()
        print(f"\nðŸ“Š Signals by Pattern:")
        for pattern, count in pattern_counts.items():
            print(f"  â€¢ {pattern}: {count}")

        return signals_aggregated

    # ==================== STAGE 4: FORMAT RESULTS ====================

    def format_results(
        self,
        stage3_results: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Stage 4: Format results for display

        Args:
            stage3_results: Output from detect_patterns

        Returns:
            Formatted DataFrame with results
        """
        if stage3_results.empty:
            return pd.DataFrame()

        # Sort by date (chronological order)
        output = stage3_results.sort_values('Date').reset_index(drop=True)

        return output

    # ==================== STAGE 5: RUN SCAN ====================

    def run_scan(
        self,
        start_date: str = None,
        end_date: str = None
    ) -> pd.DataFrame:
        """
        Stage 5: Run complete scan pipeline

        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)

        Returns:
            Formatted results DataFrame
        """
        # Use configured dates if not provided
        start_date = start_date or self.scan_start
        end_date = end_date or self.scan_end

        print(f"\n{'='*70}")
        print(f"ðŸš€ MULTI-PATTERN SCANNER: half_A_scan_copy")
        print(f"{'='*70}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data(start_date, end_date)

        if stage1_data.empty:
            print("âŒ No data available!")
            return pd.DataFrame()

        # Stage 2: Apply smart filters
        stage2_data = self.apply_smart_filters(stage1_data)

        # Stage 3: Detect patterns
        stage3_results = self.detect_patterns(stage2_data)

        if stage3_results.empty:
            print("âŒ No signals found!")
            return pd.DataFrame()

        # Stage 4: Format results
        formatted_results = self.format_results(stage3_results)

        # Print results
        print(f"\n{'='*70}")
        print(f"âœ… SCAN COMPLETE")
        print(f"{'='*70}")
        print(f"ðŸ“Š Final signals (D0 range): {len(formatted_results):,}")
        print(f"ðŸ“Š Unique tickers: {formatted_results['Ticker'].nunique():,}")

        # Print all results
        if len(formatted_results) > 0:
            print(f"\n{'='*70}")
            print("ðŸ“Š ALL SIGNALS:")
            print(f"{'='*70}")
            for idx, row in formatted_results.iterrows():
                print(f"  {row['Ticker']:6s} | {row['Date']} | {row['Scanner_Label']}")

        return formatted_results

    def run_and_save(self, output_path: str = "multi_scanner_results.csv") -> pd.DataFrame:
        """Execute scan and save results"""
        results = self.run_scan()

        if not results.empty:
            results.to_csv(output_path, index=False)
            print(f"âœ… Results saved to: {output_path}")

        return results


# ==================== MAIN ENTRY POINT ====================

if __name__ == "__main__":
    import sys

    scanner = half_A_scan_copy()

    results = scanner.run_and_save()

    print(f"\nâœ… Done!")
