"""
half_A_scan

The code provided is incomplete and does not contain the full logic for any specific trading strategy. It includes data fetching and some technical indicator computations, but the core strategy logic, entry/exit conditions, and pattern detection are missing.

Generated by RENATA_V2 - 2026-01-20 19:21:57

TRUE v31 Architecture - All 7 Core Pillars Implemented:
1. âœ… Market calendar (pandas_market_calendars)
2. âœ… Historical buffer calculation
3. âœ… Per-ticker operations (groupby().transform())
4. âœ… Historical/D0 separation in smart filters
5. âœ… Parallel processing (ThreadPoolExecutor)
6. âœ… Two-pass feature computation
7. âœ… Pre-sliced data for parallel processing
"""

# scan_daily.py  â”€â”€ optimized with session reuse & parallel execution
# ---------------------------------------------------------------------------
import pandas as pd
import numpy as np
import requests
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
session  = requests.Session()
API_KEY  = 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'
BASE_URL = 'https://api.polygon.io'

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Fetching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def fetch_aggregates(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
    """Download daily bars from Polygon.io and return a clean DataFrame."""
    url  = f"{BASE_URL}/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}"
    resp = session.get(url, params={'apiKey': API_KEY})
    resp.raise_for_status()
    data = resp.json().get('results', [])
    if not data:
        return pd.DataFrame()

    df = pd.DataFrame(data)
    df['Date'] = pd.to_datetime(df['t'], unit='ms')
    df.rename(columns={'o':'Open','h':'High','l':'Low','c':'Close','v':'Volume'}, inplace=True)
    df.set_index('Date', inplace=True)
    return df[['Open','High','Low','Close','Volume']]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metric Computations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def compute_emas(df: pd.DataFrame, spans=(9, 20)) -> pd.DataFrame:
    for span in spans:
        df[f'EMA_{span}'] = df['Close'].ewm(span=span, adjust=False).mean()
    return df

def compute_atr(df: pd.DataFrame, period: int = 30) -> pd.DataFrame:
    hi_lo   = df['High'] - df['Low']
    hi_prev = (df['High'] - df['Close'].shift(1)).abs()
    lo_prev = (df['Low']  - df['Close'].shift(1)).abs()
    df['TR'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
    df['ATR_raw']        = df['TR'].rolling(window=period, min_periods=period).mean()
    df['ATR']            = df['ATR_raw'].shift(1)
    df['ATR_Pct_Change'] = df['ATR_raw'].pct_change().shift(1) * 100
    return df

def compute_volume(df: pd.DataFrame, period: int = 30) -> pd.DataFrame:
    df['VOL_AVG_raw'] = df['Volume'].rolling(window=period, min_periods=period).mean()
    df['VOL_AVG']     = df['VOL_AVG_raw'].shift(1)
    df['Prev_Volume'] = df['Volume'].shift(1)
    return df

def compute_slopes(df: pd.DataFrame, span: int, windows=(3, 5, 15)) -> pd.DataFrame:
    for w in windows:
        df[f'Slope_{span}_{w}d'] = (
            (df[f'EMA_{span}'] - df[f'EMA_{span}'].shift(w)) / df[f'EMA_{span}'].shift(w)
        ) * 100
    return df

def compute_custom_50d_slope(df: pd.DataFrame, span: int = 9,
                             start_shift: int = 4, end_shift: int = 50) -> pd.DataFrame:
    """Slope from day-4 back to day-50 (positive â‡’ up-trend)."""
    col = f'Slope_{span}_{start_shift}to{end_shift}d'
    df[col] = (
        (df[f'EMA_{span}'].shift(start_shift) - df[f'EMA_{span}'].shift(end_shift))
        / df[f'EMA_{span}'].shift(end_shift)
    ) * 100
    return df

def compute_gap(df: pd.DataFrame) -> pd.DataFrame:
    df['Gap']          = (df['Open'] - df['Close'].shift(1)).abs()
    df['Gap_over_ATR'] = df['Gap'] / df['ATR']
    return df

def compute_div_ema_atr(df: pd.DataFrame, spans=(9, 20)) -> pd.DataFrame:
    for span in spans:
        df[f'High_over_EMA{span}_div_ATR'] = (df['High'] - df[f'EMA_{span}']) / df['ATR']
    return df

def compute_pct_changes(df: pd.DataFrame) -> pd.DataFrame:
    low7  = df['Low'].rolling(window=7 , min_periods=7 ).min()
    low14 = df['Low'].rolling(window=14, min_periods=14).min()
    df['Pct_7d_low_div_ATR']  = ((df['Close'] - low7 ) / low7 ) / df['ATR'] * 100
    df['Pct_14d_low_div_ATR'] = ((df['Close'] - low14) / low14) / df['ATR'] * 100
    return df

def compute_range_position(df: pd.DataFrame) -> pd.DataFrame:
    df['Upper_70_Range'] = (df['Close'] - df['Low']) / (df['High'] - df['Low']) * 100
    return df

def compute_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
    df = (df.pipe(compute_emas)
            .pipe(compute_atr)
            .pipe(compute_volume)
            .pipe(compute_slopes, span=9)
            .pipe(compute_custom_50d_slope, span=9, start_shift=4, end_shift=50)
            .pipe(compute_gap)
            .pipe(compute_div_ema_atr)
            .pipe(compute_pct_changes)
            .pipe(compute_range_position))

    # multi-day references
    df['Prev_Close'] = df['Close'].shift(1)
    df['Prev_Open']  = df['Open'].shift(1)
    df['Prev_High']  = df['High'].shift(1)
    df['Close_D3']   = df['Close'].shift(3)
    df['Close_D4']   = df['Close'].shift(4)

    # previous-day % gain
    df['Prev_Gain_Pct'] = (df['Prev_Close'] - df['Prev_Open']) / df['Prev_Open'] * 100

    # 1-, 2-, 3-day moves vs ATR
    df['Pct_1d']         = df['Close'].pct_change() * 100
    df['Pct_1d_div_ATR'] = df['Pct_1d'] / df['ATR']
    df['Move2d_div_ATR'] = (df['Prev_Close'] - df['Close_D3']) / df['ATR']
    df['Move3d_div_ATR'] = (df['Prev_Close'] - df['Close_D4']) / df['ATR']

    # misc ratios
    df['Range_over_ATR']  = df['TR'] / df['ATR']
    df['Vol_over_AVG']    = df['Volume'] / df['VOL_AVG']
    df['Close_over_EMA9'] = df['Close'] / df['EMA_9']
    df['Open_over_EMA9']  = df['Open']  / df['EMA_9']
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scan Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def scan_daily_para(df: pd.DataFrame, params: dict | None = None) -> pd.DataFrame:
    defaults = {
        'atr_mult'              : 4,
        'vol_mult'              : 2,
        'slope3d_min'           : 10,
        'slope5d_min'           : 20,
        'slope15d_min'          : 40,
        'slope50d_min'        : 60,  # optional long-trend filter
        'high_ema9_mult'        : 4,
        'high_ema20_mult'       : 5,
        'pct7d_low_div_atr_min' : 0.5,
        'pct14d_low_div_atr_min': 1.5,
        'gap_div_atr_min'       : 0.5,
        'open_over_ema9_min'    : 1.25,
        'atr_pct_change_min'    : 5,
        'prev_close_min'        : 15.0,
        'prev_gain_pct_min'     : 0.25,  # â† new trigger threshold
        'pct2d_div_atr_min'     : 2,
        'pct3d_div_atr_min'     : 3,
    }
    if params:
        defaults.update(params)
    d = defaults

    df_m = compute_all_metrics(df.copy())

    cond = (
        (df_m['TR']            / df_m['ATR']        >= d['atr_mult']) &
        (df_m['Volume']        / df_m['VOL_AVG']    >= d['vol_mult']) &
        (df_m['Prev_Volume']   / df_m['VOL_AVG']    >= d['vol_mult']) &
        (df_m['Slope_9_3d']    >= d['slope3d_min']) &
        (df_m['Slope_9_5d']    >= d['slope5d_min']) &
        (df_m['Slope_9_15d']   >= d['slope15d_min']) &
        (df_m['High_over_EMA9_div_ATR']  >= d['high_ema9_mult']) &
        (df_m['High_over_EMA20_div_ATR'] >= d['high_ema20_mult']) &
        (df_m['Pct_7d_low_div_ATR']      >= d['pct7d_low_div_atr_min']) &
        (df_m['Pct_14d_low_div_ATR']     >= d['pct14d_low_div_atr_min']) &
        (df_m['Gap_over_ATR']            >= d['gap_div_atr_min']) &
        (df_m['Open'] / df_m['EMA_9']    >= d['open_over_ema9_min']) &
        (df_m['ATR_Pct_Change']          >= d['atr_pct_change_min']) &
        (df_m['Prev_Close']              >  d['prev_close_min']) &
        (df_m['Move2d_div_ATR']          >= d['pct2d_div_atr_min']) &
        (df_m['Move3d_div_ATR']          >= d['pct3d_div_atr_min']) &
        # new trigger rule: previous-day gain â‰¥ threshold
        (df_m['Prev_Gain_Pct']           >= d['prev_gain_pct_min']) &
        # gap-up rule
        (df_m['Open'] > df_m['Prev_High'])
        # optional long-trend filter
        #& (df_m['Slope_9_4to50d'] >= d['slope50d_min'])
    )
    return df_m.loc[cond]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker & Parallel Scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def fetch_and_scan(symbol: str, start_date: str, end_date: str, params: dict) -> list[tuple[str, str]]:
    df = fetch_aggregates(symbol, start_date, end_date)
    if df.empty:
        return []
    hits = scan_daily_para(df, params)
    return [(symbol, d.strftime('%Y-%m-%d')) for d in hits.index]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
if __name__ == '__main__':
    custom_params = {
        'atr_mult'              : 4,
        'vol_mult'              : 2.0,
        'slope3d_min'           : 10,
        'slope5d_min'           : 20,
        'slope15d_min'          : 50,
        'high_ema9_mult'        : 4,
        'high_ema20_mult'       : 5,
        'pct7d_low_div_atr_min' : 0.5,
        'pct14d_low_div_atr_min': 1.5,
        'gap_div_atr_min'       : 0.5,
        'open_over_ema9_min'    : 1.0,
        'atr_pct_change_min'    : 5,
        'prev_close_min'        : 10.0,
        'prev_gain_pct_min'     : 0.25,   # adjust if needed
        'pct2d_div_atr_min'     : 2,
        'pct3d_div_atr_min'     : 3,
        'slope50d_min'        : 60,
    }

    symbols = ['BLBX', 'CRML', 'UAMY', 'SPRB', 'APLM', 'FIG', 'BMNR', 'VOR', 'QNTM', 'SBET', 'PLRZ', 'SMCL', 'SMCX', 'GRAL', 'AIFF', 'CRNC', 'KITT', 'NITO', 'NVNI', 'KULR', 'NUKK', 'QUBT', 'TNXP', 'UMAC', 'MSTR', 'MSTX', 'LMND', 'BKKT', 'BTCT', 'CLDI', 'DXYZ', 'DJT', 'PHUN', 'GNPX', 'JDZG', 'CHAU', 'TIGR', 'BILI', 'DUO', 'UVIX', 'UVXY', 'SERV', 'NNE', 'LPA', 'GME', 'ADXN', 'JANX', 'LUNR', 'SMCI', 'HOLO']

    start_date = '2024-01-01'
    end_date   = datetime.today().strftime('%Y-%m-%d')

    with ThreadPoolExecutor(max_workers=5) as exe:
        futures = {
            exe.submit(fetch_and_scan, s, start_date, end_date, custom_params): s
            for s in symbols
        }
        for fut in as_completed(futures):
            for sym, hit_date in fut.result():
                print(f"{sym} {hit_date}")








# â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRUE v31 Scanner Class â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os

class half_A_scan:
    """
    Incomplete Strategy

    The code provided is incomplete and does not contain the full logic for any specific trading strategy. It includes data fetching and some technical indicator computations, but the core strategy logic, entry/exit conditions, and pattern detection are missing.

    Scanner Type: Generic Scanner converted to TRUE v31 Architecture
    Generated: 2026-01-20 19:21:57

    TRUE v31 Architecture - All 7 Core Pillars Implemented
    """

    def __init__(self, d0_start: str = None, d0_end: str = None):
        """
        Initialize scanner with proper v31 configuration

        âœ… PILLAR 2: Calculate historical buffer for indicators
        """
        # Date range configuration
        self.d0_start_user = d0_start or "2024-01-01"
        self.d0_end_user = d0_end or "2024-12-31"

        # âœ… CRITICAL: Calculate historical buffer
        lookback_buffer = 780  # From detected params with safety margin

        # Calculate scan_start with historical buffer
        scan_start_dt = pd.to_datetime(self.d0_start_user) - pd.Timedelta(days=lookback_buffer)
        self.scan_start = scan_start_dt.strftime('%Y-%m-%d')

        # âœ… Workers for parallel processing
        self.stage3_workers = 10

        # âœ… Parameters extracted from original code
        self.params = self._extract_parameters()

        # âœ… Scanner name for logging
        self.scanner_name = self.__class__.__name__

        print(f"ğŸ“Š Scanner initialized: {self.scanner_name}")
        print(f"   Historical buffer: {lookback_buffer} days")
        print(f"   Scan range: {self.scan_start} to {self.d0_end_user}")
        print(f"   D0 output range: {self.d0_start_user} to {self.d0_end_user}")

    def _extract_parameters(self) -> dict:
        """Extract parameters from original code or return defaults"""
        # Try to extract from global scope
        try:
            import sys
            frame = sys._getframe(1)
            if 'P' in frame.f_locals:
                return frame.f_locals['P']
        except:
            pass

        # Default parameters from detected params
        return {'apiKey': 'APIKEY', 'atr_mult': 4, 'vol_mult': 2.0, 'slope3d_min': 10, 'slope5d_min': 20, 'slope15d_min': 50, 'slope50d_min': 60, 'high_ema9_mult': 4, 'high_ema20_mult': 5, 'pct7d_low_div_atr_min': 0.5, 'pct14d_low_div_atr_min': 1.5, 'gap_div_atr_min': 0.5, 'open_over_ema9_min': 1.0, 'atr_pct_change_min': 5, 'prev_close_min': 10.0, 'prev_gain_pct_min': 0.25, 'pct2d_div_atr_min': 2, 'pct3d_div_atr_min': 3}

    def run_scan(self):
        """
        ğŸš€ Main execution: 5-stage v31 pipeline

        Returns:
            List of signal dictionaries
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ RUNNING TRUE v31 SCAN: {self.scanner_name}")
        print(f"{'='*70}")

        # Stage 1: Fetch grouped data
        stage1_data = self.fetch_grouped_data()
        if stage1_data is None or stage1_data.empty:
            print("\nâŒ Scan failed: No data loaded")
            return []

        # Stage 2a: Compute simple features
        stage2a_data = self.compute_simple_features(stage1_data)

        # Stage 2b: Apply smart filters (with historical/D0 separation)
        stage2b_data = self.apply_smart_filters(stage2a_data)

        # Stage 3a: Compute full features
        stage3a_data = self.compute_full_features(stage2b_data)

        # Stage 3b: Detect patterns (with pre-sliced parallel processing)
        stage3_results = self.detect_patterns(stage3a_data)

        print(f"\nâœ… SCAN COMPLETE: {len(stage3_results)} signals detected")
        return stage3_results

    def fetch_grouped_data(self):
        """
        âœ… PILLAR 1: Market calendar integration
        âœ… PILLAR 5: Parallel processing

        Stage 1: Fetch ALL tickers for ALL dates using Polygon's grouped API (direct approach)
        """
        import requests
        from datetime import timedelta

        API_KEY = os.getenv("POLYGON_API_KEY", "Fm7brz4s23eSocDErnL68cE7wspz2K1I")
        BASE_URL = "https://api.polygon.io"

        print(f"  ğŸ“¡ Fetching data from {self.scan_start} to {self.d0_end_user} using Polygon grouped API...")

        all_data = []
        current_date = pd.to_datetime(self.scan_start).date()
        end = pd.to_datetime(self.d0_end_user).date()

        # Fetch data for each trading day
        while current_date <= end:
            # Skip weekends
            if current_date.weekday() < 5:  # Monday-Friday
                date_str = current_date.strftime("%Y-%m-%d")
                url = BASE_URL + "/v2/aggs/grouped/locale/us/market/stocks/" + date_str
                params = {
                    "adjusted": "true",
                    "apiKey": API_KEY
                }

                try:
                    response = requests.get(url, params=params)
                    response.raise_for_status()
                    data = response.json()

                    if 'results' in data and data['results']:
                        df_daily = pd.DataFrame(data['results'])
                        df_daily['date'] = pd.to_datetime(df_daily['t'], unit='ms').dt.date
                        df_daily.rename(columns={'T': 'ticker', 'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}, inplace=True)
                        all_data.append(df_daily[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']])

                except Exception as e:
                    print(f"    âš ï¸  Error fetching data for {date_str}: {e}")

            current_date += timedelta(days=1)

        if not all_data:
            print(f"  âŒ No data fetched!")
            return pd.DataFrame()

        # Combine all daily data
        df = pd.concat(all_data, ignore_index=True)

        if df.empty:
            print(f"  âŒ No data fetched!")
            return pd.DataFrame()

        print(f"  âœ… Stage 1 complete: {len(df)} total records")
        print(f"     ğŸ“Š Unique tickers: {df['ticker'].nunique()}")
        print(f"     ğŸ“… Date range: {df['date'].min()} to {df['date'].max()}")
        return df

    def compute_simple_features(self, df: pd.DataFrame):
        """
        âœ… PILLAR 3: Per-ticker operations
        âœ… PILLAR 6: Two-pass feature computation (simple first)

        Stage 2a: Compute SIMPLE features for efficient filtering

        Only computes features needed for filtering:
        - prev_close
        - adv20_usd (with per-ticker groupby)
        - price_range
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 2a: Computing simple features for {len(df)} rows")

        df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'])

        # Previous close
        df['prev_close'] = df.groupby('ticker')['close'].shift(1)

        # âœ… PILLAR 3: Per-ticker operations for ADV20
        df['adv20_usd'] = (df['close'] * df['volume']).groupby(df['ticker']).transform(
            lambda x: x.rolling(window=20, min_periods=20).mean()
        )

        # Price range
        df['price_range'] = df['high'] - df['low']

        print(f"    âœ… Simple features computed")
        return df

    def apply_smart_filters(self, df: pd.DataFrame):
        """
        âœ… PILLAR 4: Separate historical from D0 data

        Stage 2b: Smart filters with HISTORICAL DATA PRESERVATION

        CRITICAL: Only filter D0 output range, preserve all historical data
        for indicator calculations.
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 2b input: {len(df)} rows")

        # âœ… PILLAR 4: Split historical from D0
        df_historical = df[~df['date'].between(self.d0_start_user, self.d0_end_user)].copy()
        df_output_range = df[df['date'].between(self.d0_start_user, self.d0_end_user)].copy()

        print(f"    ğŸ“Š Historical: {len(df_historical)} rows")
        print(f"    ğŸ“Š D0 range: {len(df_output_range)} rows")

        # âœ… CRITICAL: Filter ONLY D0 range using extracted parameters
        df_output_filtered = df_output_range.copy()

        # Price filter
        if 'price_min' in self.params:
            min_price = self.params['price_min']
            df_output_filtered = df_output_filtered[
                (df_output_filtered['close'] >= min_price) &
                (df_output_filtered['open'] >= min_price)
            ]
            print(f"    ğŸ“Š After price filter (>= ${min_price}): {len(df_output_filtered)} rows")

        # Volume filter
        if 'adv20_min_usd' in self.params:
            min_adv = self.params['adv20_min_usd']
            df_output_filtered = df_output_filtered[
                df_output_filtered['adv20_usd'] >= min_adv
            ]
            print(f"    ğŸ“Š After volume filter (>= ${min_adv:,}): {len(df_output_filtered)} rows")

        print(f"  âœ… Stage 2b complete: {len(df_historical) + len(df_output_filtered)} rows (historical preserved)")

        # âœ… CRITICAL: COMBINE historical + filtered D0
        df_combined = pd.concat([df_historical, df_output_filtered], ignore_index=True)
        return df_combined

    def compute_full_features(self, df: pd.DataFrame):
        """
        âœ… PILLAR 3: Per-ticker operations
        âœ… PILLAR 6: Two-pass feature computation (full features after filter)

        Stage 3a: Compute ALL technical indicators

        Computes expensive features only on data that passed filters.
        """
        if df.empty:
            return df

        print(f"  ğŸ“Š Stage 3a: Computing full features for {len(df)} rows")

        result_dfs = []

        for ticker, group in df.groupby('ticker'):
            group = group.sort_values('date').copy()

            # EMA
            group['ema_9'] = group['close'].ewm(span=9, adjust=False).mean()
            group['ema_20'] = group['close'].ewm(span=20, adjust=False).mean()

            # ATR
            hi_lo = group['high'] - group['low']
            hi_prev = (group['high'] - group['close'].shift(1)).abs()
            lo_prev = (group['low'] - group['close'].shift(1)).abs()
            group['tr'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
            group['atr'] = group['tr'].rolling(14, min_periods=14).mean().shift(1)

            # Additional common indicators
            group['vol_avg'] = group['volume'].rolling(14, min_periods=14).mean().shift(1)
            group['prev_volume'] = group['volume'].shift(1)

            # RSI
            delta = group['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=14).mean()
            rs = gain / loss
            group['rsi'] = 100 - (100 / (1 + rs))

            result_dfs.append(group)

        result = pd.concat(result_dfs, ignore_index=True)
        print(f"    âœ… Full features computed")
        return result

    def detect_patterns(self, df: pd.DataFrame):
        """
        âœ… PILLAR 7: Pre-sliced data for parallel processing
        âœ… PILLAR 5: Parallel ticker processing

        Stage 3b: Pattern detection with parallel processing
        """
        if df.empty:
            return []

        print(f"  ğŸ¯ Stage 3b: Detecting patterns in {len(df)} rows")

        # Get D0 range
        d0_start_dt = pd.to_datetime(self.d0_start_user)
        d0_end_dt = pd.to_datetime(self.d0_end_user)

        # âœ… PILLAR 7: Pre-slice ticker data BEFORE parallel processing
        ticker_data_list = []
        for ticker, ticker_df in df.groupby('ticker'):
            ticker_data_list.append((ticker, ticker_df.copy(), d0_start_dt, d0_end_dt))

        all_results = []

        # âœ… PILLAR 5: Parallel processing with pre-sliced data
        from concurrent.futures import ThreadPoolExecutor, as_completed

        with ThreadPoolExecutor(max_workers=self.stage3_workers) as executor:
            future_to_ticker = {
                executor.submit(self._process_ticker, ticker_data): ticker_data[0]
                for ticker_data in ticker_data_list
            }

            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    results = future.result()
                    if results:
                        all_results.extend(results)
                        print(f"    âœ… {ticker}: {len(results)} signals")
                except Exception as e:
                    print(f"    âš ï¸  {ticker}: {e}")

        print(f"  âœ… Stage 3b complete: {len(all_results)} total signals")
        return all_results

    def _process_ticker(self, ticker_data: tuple):
        """
        âœ… PILLAR 7: Process pre-sliced ticker data
        âœ… PILLAR 4: Early D0 filtering

        Process ticker data with original detection logic
        """
        ticker, ticker_df, d0_start_dt, d0_end_dt = ticker_data

        # âœ… Minimum data check
        if len(ticker_df) < 100:
            return []

        # âœ… Sort and prepare data
        ticker_df = ticker_df.sort_values('date').reset_index(drop=True)
        ticker_df['date'] = pd.to_datetime(ticker_df['date'])

        # âœ… Initialize results list
        all_rows = []

        # âœ… Apply original detection logic with D0 filtering
        # The original code can use all the computed features
        try:
            # scan_daily.py  â”€â”€ optimized with session reuse & parallel execution
            # ---------------------------------------------------------------------------
            import pandas as pd
            import numpy as np
            import requests
            from datetime import datetime
            from concurrent.futures import ThreadPoolExecutor, as_completed

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
            session  = requests.Session()
            API_KEY  = 'Fm7brz4s23eSocDErnL68cE7wspz2K1I'
            BASE_URL = 'https://api.polygon.io'

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Fetching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
            def fetch_aggregates(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
                """Download daily bars from Polygon.io and return a clean DataFrame."""
                url  = f"{BASE_URL}/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}"
                resp = session.get(url, params={'apiKey': API_KEY})
                resp.raise_for_status()
                data = resp.json().get('results', [])
                if not data:
                    return pd.DataFrame()

                df = pd.DataFrame(data)
                df['Date'] = pd.to_datetime(df['t'], unit='ms')
                df.rename(columns={'o':'Open','h':'High','l':'Low','c':'Close','v':'Volume'}, inplace=True)
                df.set_index('Date', inplace=True)
                return df[['Open','High','Low','Close','Volume']]

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metric Computations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
            def compute_emas(df: pd.DataFrame, spans=(9, 20)) -> pd.DataFrame:
                for span in spans:
                    df[f'EMA_{span}'] = df['Close'].ewm(span=span, adjust=False).mean()
                return df

            def compute_atr(df: pd.DataFrame, period: int = 30) -> pd.DataFrame:
                hi_lo   = df['High'] - df['Low']
                hi_prev = (df['High'] - df['Close'].shift(1)).abs()
                lo_prev = (df['Low']  - df['Close'].shift(1)).abs()
                df['TR'] = pd.concat([hi_lo, hi_prev, lo_prev], axis=1).max(axis=1)
                df['ATR_raw']        = df['TR'].rolling(window=period, min_periods=period).mean()
                df['ATR']            = df['ATR_raw'].shift(1)
                df['ATR_Pct_Change'] = df['ATR_raw'].pct_change().shift(1) * 100
                return df

            def compute_volume(df: pd.DataFrame, period: int = 30) -> pd.DataFrame:
                df['VOL_AVG_raw'] = df['Volume'].rolling(window=period, min_periods=period).mean()
                df['VOL_AVG']     = df['VOL_AVG_raw'].shift(1)
                df['Prev_Volume'] = df['Volume'].shift(1)
                return df

            def compute_slopes(df: pd.DataFrame, span: int, windows=(3, 5, 15)) -> pd.DataFrame:
                for w in windows:
                    df[f'Slope_{span}_{w}d'] = (
                        (df[f'EMA_{span}'] - df[f'EMA_{span}'].shift(w)) / df[f'EMA_{span}'].shift(w)
                    ) * 100
                return df

            def compute_custom_50d_slope(df: pd.DataFrame, span: int = 9,
                                         start_shift: int = 4, end_shift: int = 50) -> pd.DataFrame:
                """Slope from day-4 back to day-50 (positive â‡’ up-trend)."""
                col = f'Slope_{span}_{start_shift}to{end_shift}d'
                df[col] = (
                    (df[f'EMA_{span}'].shift(start_shift) - df[f'EMA_{span}'].shift(end_shift))
                    / df[f'EMA_{span}'].shift(end_shift)
                ) * 100
                return df

            def compute_gap(df: pd.DataFrame) -> pd.DataFrame:
                df['Gap']          = (df['Open'] - df['Close'].shift(1)).abs()
                df['Gap_over_ATR'] = df['Gap'] / df['ATR']
                return df

            def compute_div_ema_atr(df: pd.DataFrame, spans=(9, 20)) -> pd.DataFrame:
                for span in spans:
                    df[f'High_over_EMA{span}_div_ATR'] = (df['High'] - df[f'EMA_{span}']) / df['ATR']
                return df

            def compute_pct_changes(df: pd.DataFrame) -> pd.DataFrame:
                low7  = df['Low'].rolling(window=7 , min_periods=7 ).min()
                low14 = df['Low'].rolling(window=14, min_periods=14).min()
                df['Pct_7d_low_div_ATR']  = ((df['Close'] - low7 ) / low7 ) / df['ATR'] * 100
                df['Pct_14d_low_div_ATR'] = ((df['Close'] - low14) / low14) / df['ATR'] * 100
                return df

            def compute_range_position(df: pd.DataFrame) -> pd.DataFrame:
                df['Upper_70_Range'] = (df['Close'] - df['Low']) / (df['High'] - df['Low']) * 100
                return df

            def compute_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
                df = (df.pipe(compute_emas)
                        .pipe(compute_atr)
                        .pipe(compute_volume)
                        .pipe(compute_slopes, span=9)
                        .pipe(compute_custom_50d_slope, span=9, start_shift=4, end_shift=50)
                        .pipe(compute_gap)
                        .pipe(compute_div_ema_atr)
                        .pipe(compute_pct_changes)
                        .pipe(compute_range_position))

                # multi-day references
                df['Prev_Close'] = df['Close'].shift(1)
                df['Prev_Open']  = df['Open'].shift(1)
                df['Prev_High']  = df['High'].shift(1)
                df['Close_D3']   = df['Close'].shift(3)
                df['Close_D4']   = df['Close'].shift(4)

                # previous-day % gain
                df['Prev_Gain_Pct'] = (df['Prev_Close'] - df['Prev_Open']) / df['Prev_Open'] * 100

                # 1-, 2-, 3-day moves vs ATR
                df['Pct_1d']         = df['Close'].pct_change() * 100
                df['Pct_1d_div_ATR'] = df['Pct_1d'] / df['ATR']
                df['Move2d_div_ATR'] = (df['Prev_Close'] - df['Close_D3']) / df['ATR']
                df['Move3d_div_ATR'] = (df['Prev_Close'] - df['Close_D4']) / df['ATR']

                # misc ratios
                df['Range_over_ATR']  = df['TR'] / df['ATR']
                df['Vol_over_AVG']    = df['Volume'] / df['VOL_AVG']
                df['Close_over_EMA9'] = df['Close'] / df['EMA_9']
                df['Open_over_EMA9']  = df['Open']  / df['EMA_9']
                return df

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scan Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
            def scan_daily_para(df: pd.DataFrame, params: dict | None = None) -> pd.DataFrame:
                defaults = {
                    'atr_mult'              : 4,
                    'vol_mult'              : 2,
                    'slope3d_min'           : 10,
                    'slope5d_min'           : 20,
                    'slope15d_min'          : 40,
                    'slope50d_min'        : 60,  # optional long-trend filter
                    'high_ema9_mult'        : 4,
                    'high_ema20_mult'       : 5,
                    'pct7d_low_div_atr_min' : 0.5,
                    'pct14d_low_div_atr_min': 1.5,
                    'gap_div_atr_min'       : 0.5,
                    'open_over_ema9_min'    : 1.25,
                    'atr_pct_change_min'    : 5,
                    'prev_close_min'        : 15.0,
                    'prev_gain_pct_min'     : 0.25,  # â† new trigger threshold
                    'pct2d_div_atr_min'     : 2,
                    'pct3d_div_atr_min'     : 3,
                }
                if params:
                    defaults.update(params)
                d = defaults

                df_m = compute_all_metrics(df.copy())

                cond = (
                    (df_m['TR']            / df_m['ATR']        >= d['atr_mult']) &
                    (df_m['Volume']        / df_m['VOL_AVG']    >= d['vol_mult']) &
                    (df_m['Prev_Volume']   / df_m['VOL_AVG']    >= d['vol_mult']) &
                    (df_m['Slope_9_3d']    >= d['slope3d_min']) &
                    (df_m['Slope_9_5d']    >= d['slope5d_min']) &
                    (df_m['Slope_9_15d']   >= d['slope15d_min']) &
                    (df_m['High_over_EMA9_div_ATR']  >= d['high_ema9_mult']) &
                    (df_m['High_over_EMA20_div_ATR'] >= d['high_ema20_mult']) &
                    (df_m['Pct_7d_low_div_ATR']      >= d['pct7d_low_div_atr_min']) &
                    (df_m['Pct_14d_low_div_ATR']     >= d['pct14d_low_div_atr_min']) &
                    (df_m['Gap_over_ATR']            >= d['gap_div_atr_min']) &
                    (df_m['Open'] / df_m['EMA_9']    >= d['open_over_ema9_min']) &
                    (df_m['ATR_Pct_Change']          >= d['atr_pct_change_min']) &
                    (df_m['Prev_Close']              >  d['prev_close_min']) &
                    (df_m['Move2d_div_ATR']          >= d['pct2d_div_atr_min']) &
                    (df_m['Move3d_div_ATR']          >= d['pct3d_div_atr_min']) &
                    # new trigger rule: previous-day gain â‰¥ threshold
                    (df_m['Prev_Gain_Pct']           >= d['prev_gain_pct_min']) &
                    # gap-up rule
                    (df_m['Open'] > df_m['Prev_High'])
                    # optional long-trend filter
                    #& (df_m['Slope_9_4to50d'] >= d['slope50d_min'])
                )
                return df_m.loc[cond]

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker & Parallel Scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
            def fetch_and_scan(symbol: str, start_date: str, end_date: str, params: dict) -> list[tuple[str, str]]:
                df = fetch_aggregates(symbol, start_date, end_date)
                if df.empty:
                    return []
                hits = scan_daily_para(df, params)
                return [(symbol, d.strftime('%Y-%m-%d')) for d in hits.index]

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
            if __name__ == '__main__':
                custom_params = {
                    'atr_mult'              : 4,
                    'vol_mult'              : 2.0,
                    'slope3d_min'           : 10,
                    'slope5d_min'           : 20,
                    'slope15d_min'          : 50,
                    'high_ema9_mult'        : 4,
                    'high_ema20_mult'       : 5,
                    'pct7d_low_div_atr_min' : 0.5,
                    'pct14d_low_div_atr_min': 1.5,
                    'gap_div_atr_min'       : 0.5,
                    'open_over_ema9_min'    : 1.0,
                    'atr_pct_change_min'    : 5,
                    'prev_close_min'        : 10.0,
                    'prev_gain_pct_min'     : 0.25,   # adjust if needed
                    'pct2d_div_atr_min'     : 2,
                    'pct3d_div_atr_min'     : 3,
                    'slope50d_min'        : 60,
                }

                symbols = ['BLBX', 'CRML', 'UAMY', 'SPRB', 'APLM', 'FIG', 'BMNR', 'VOR', 'QNTM', 'SBET', 'PLRZ', 'SMCL', 'SMCX', 'GRAL', 'AIFF', 'CRNC', 'KITT', 'NITO', 'NVNI', 'KULR', 'NUKK', 'QUBT', 'TNXP', 'UMAC', 'MSTR', 'MSTX', 'LMND', 'BKKT', 'BTCT', 'CLDI', 'DXYZ', 'DJT', 'PHUN', 'GNPX', 'JDZG', 'CHAU', 'TIGR', 'BILI', 'DUO', 'UVIX', 'UVXY', 'SERV', 'NNE', 'LPA', 'GME', 'ADXN', 'JANX', 'LUNR', 'SMCI', 'HOLO']

                start_date = '2024-01-01'
                end_date   = datetime.today().strftime('%Y-%m-%d')

                with ThreadPoolExecutor(max_workers=5) as exe:
                    futures = {
                        exe.submit(fetch_and_scan, s, start_date, end_date, custom_params): s
                        for s in symbols
                    }
                    for fut in as_completed(futures):
                        for sym, hit_date in fut.result():
                            print(f"{sym} {hit_date}")






        except Exception as e:
            print(f"    âš ï¸  {ticker}: Error in detection logic: {e}")

        return all_rows

    def format_results(self, all_results: list) -> pd.DataFrame:
        """
        Format results for display

        Args:
            all_results: List of signal dictionaries from detect_patterns

        Returns:
            DataFrame with formatted results (Ticker, Date, Scanner_Label)
        """
        if not all_results:
            return pd.DataFrame()

        # Convert to DataFrame
        df = pd.DataFrame(all_results)

        # Ensure required columns exist
        if 'Ticker' not in df.columns or 'Date' not in df.columns:
            print("  âš ï¸  Results missing required columns")
            return pd.DataFrame()

        # Group by ticker and date, aggregate labels
        if 'Scanner_Label' not in df.columns:
            df['Scanner_Label'] = 'Signal'

        # Aggregate by ticker+date
        aggregated = df.groupby(['Ticker', 'Date'])['Scanner_Label'].apply(
            lambda x: ', '.join(sorted(set(x)))
        ).reset_index()

        # Sort by date
        aggregated = aggregated.sort_values('Date').reset_index(drop=True)

        print(f"  âœ… Formatted {len(aggregated)} unique signals")
        return aggregated


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Entry Point
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import sys

    # Parse command line arguments
    d0_start = sys.argv[1] if len(sys.argv) > 1 else "2024-01-01"
    d0_end = sys.argv[2] if len(sys.argv) > 2 else "2024-12-31"

    print(f"\nğŸš€ Starting {scanner_name} Scanner")
    print(f"   D0 Range: {d0_start} to {d0_end}")
    print(f"\n{'='*70}")

    # Create scanner instance and run
    scanner = {scanner_name}(d0_start=d0_start, d0_end=d0_end)
    results = scanner.run_scan()

    # Display results
    if results:
        print(f"\n{'='*70}")
        print(f"ğŸ¯ SCAN RESULTS: {len(results)} signals detected")
        print(f"{'='*70}")

        # Convert to DataFrame for display
        import pandas as pd
        df = pd.DataFrame(results)

        # Show key columns
        if 'Ticker' in df.columns and 'Date' in df.columns:
            for _, row in df.head(20).iterrows():
                print(f"  {row['Ticker']:6s} | {row['Date']}")

            if len(df) > 20:
                print(f"  ... and {len(df) - 20} more signals")
    else:
        print(f"\nâŒ No signals detected")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Module-level functions for backend integration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
